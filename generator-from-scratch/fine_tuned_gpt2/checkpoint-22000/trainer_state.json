{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9428485436243856,
  "eval_steps": 500,
  "global_step": 22000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013376584289201752,
      "grad_norm": 6.36680269241333,
      "learning_rate": 4.997770345596432e-05,
      "loss": 2.5186,
      "step": 10
    },
    {
      "epoch": 0.0026753168578403505,
      "grad_norm": 2.977656841278076,
      "learning_rate": 4.995540691192866e-05,
      "loss": 1.0316,
      "step": 20
    },
    {
      "epoch": 0.004012975286760525,
      "grad_norm": 2.637655019760132,
      "learning_rate": 4.993311036789298e-05,
      "loss": 1.0743,
      "step": 30
    },
    {
      "epoch": 0.005350633715680701,
      "grad_norm": 2.719341516494751,
      "learning_rate": 4.9910813823857305e-05,
      "loss": 0.9805,
      "step": 40
    },
    {
      "epoch": 0.006688292144600876,
      "grad_norm": 2.279762029647827,
      "learning_rate": 4.9888517279821625e-05,
      "loss": 0.984,
      "step": 50
    },
    {
      "epoch": 0.00802595057352105,
      "grad_norm": 2.2542366981506348,
      "learning_rate": 4.986622073578596e-05,
      "loss": 0.9967,
      "step": 60
    },
    {
      "epoch": 0.009363609002441226,
      "grad_norm": 2.558260440826416,
      "learning_rate": 4.984392419175028e-05,
      "loss": 0.9693,
      "step": 70
    },
    {
      "epoch": 0.010701267431361402,
      "grad_norm": 2.2161612510681152,
      "learning_rate": 4.982162764771461e-05,
      "loss": 0.9828,
      "step": 80
    },
    {
      "epoch": 0.012038925860281578,
      "grad_norm": 2.603858470916748,
      "learning_rate": 4.9799331103678934e-05,
      "loss": 0.9781,
      "step": 90
    },
    {
      "epoch": 0.013376584289201751,
      "grad_norm": 2.1721224784851074,
      "learning_rate": 4.977703455964326e-05,
      "loss": 0.9519,
      "step": 100
    },
    {
      "epoch": 0.014714242718121927,
      "grad_norm": 2.043520927429199,
      "learning_rate": 4.975473801560758e-05,
      "loss": 0.9889,
      "step": 110
    },
    {
      "epoch": 0.0160519011470421,
      "grad_norm": 2.082314968109131,
      "learning_rate": 4.973244147157191e-05,
      "loss": 0.9489,
      "step": 120
    },
    {
      "epoch": 0.017389559575962277,
      "grad_norm": 1.8789315223693848,
      "learning_rate": 4.9710144927536237e-05,
      "loss": 0.9763,
      "step": 130
    },
    {
      "epoch": 0.018727218004882452,
      "grad_norm": 2.0161361694335938,
      "learning_rate": 4.968784838350056e-05,
      "loss": 0.9713,
      "step": 140
    },
    {
      "epoch": 0.020064876433802628,
      "grad_norm": 1.7054659128189087,
      "learning_rate": 4.9665551839464884e-05,
      "loss": 0.9668,
      "step": 150
    },
    {
      "epoch": 0.021402534862722804,
      "grad_norm": 1.7536137104034424,
      "learning_rate": 4.964325529542921e-05,
      "loss": 1.0071,
      "step": 160
    },
    {
      "epoch": 0.02274019329164298,
      "grad_norm": 1.5267645120620728,
      "learning_rate": 4.962095875139354e-05,
      "loss": 0.9586,
      "step": 170
    },
    {
      "epoch": 0.024077851720563155,
      "grad_norm": 1.4281247854232788,
      "learning_rate": 4.959866220735786e-05,
      "loss": 0.9466,
      "step": 180
    },
    {
      "epoch": 0.02541551014948333,
      "grad_norm": 1.446656346321106,
      "learning_rate": 4.957636566332219e-05,
      "loss": 0.9707,
      "step": 190
    },
    {
      "epoch": 0.026753168578403503,
      "grad_norm": 1.267879843711853,
      "learning_rate": 4.9554069119286514e-05,
      "loss": 0.9687,
      "step": 200
    },
    {
      "epoch": 0.02809082700732368,
      "grad_norm": 1.4153298139572144,
      "learning_rate": 4.953177257525084e-05,
      "loss": 0.9536,
      "step": 210
    },
    {
      "epoch": 0.029428485436243854,
      "grad_norm": 1.4200292825698853,
      "learning_rate": 4.950947603121516e-05,
      "loss": 0.9432,
      "step": 220
    },
    {
      "epoch": 0.03076614386516403,
      "grad_norm": 1.2349863052368164,
      "learning_rate": 4.948717948717949e-05,
      "loss": 0.9509,
      "step": 230
    },
    {
      "epoch": 0.0321038022940842,
      "grad_norm": 1.3537747859954834,
      "learning_rate": 4.9464882943143816e-05,
      "loss": 0.975,
      "step": 240
    },
    {
      "epoch": 0.03344146072300438,
      "grad_norm": 1.352338194847107,
      "learning_rate": 4.9442586399108143e-05,
      "loss": 0.9607,
      "step": 250
    },
    {
      "epoch": 0.034779119151924553,
      "grad_norm": 1.2404392957687378,
      "learning_rate": 4.9420289855072464e-05,
      "loss": 0.9458,
      "step": 260
    },
    {
      "epoch": 0.03611677758084473,
      "grad_norm": 1.1283255815505981,
      "learning_rate": 4.939799331103679e-05,
      "loss": 0.9492,
      "step": 270
    },
    {
      "epoch": 0.037454436009764905,
      "grad_norm": 1.2315316200256348,
      "learning_rate": 4.937569676700112e-05,
      "loss": 0.9521,
      "step": 280
    },
    {
      "epoch": 0.03879209443868508,
      "grad_norm": 1.1005083322525024,
      "learning_rate": 4.935340022296544e-05,
      "loss": 0.9744,
      "step": 290
    },
    {
      "epoch": 0.040129752867605256,
      "grad_norm": 1.231564998626709,
      "learning_rate": 4.9331103678929766e-05,
      "loss": 0.9857,
      "step": 300
    },
    {
      "epoch": 0.04146741129652543,
      "grad_norm": 1.2566090822219849,
      "learning_rate": 4.9308807134894094e-05,
      "loss": 0.9485,
      "step": 310
    },
    {
      "epoch": 0.04280506972544561,
      "grad_norm": 1.0947328805923462,
      "learning_rate": 4.928651059085842e-05,
      "loss": 0.9608,
      "step": 320
    },
    {
      "epoch": 0.04414272815436578,
      "grad_norm": 0.8481764197349548,
      "learning_rate": 4.926421404682274e-05,
      "loss": 0.9363,
      "step": 330
    },
    {
      "epoch": 0.04548038658328596,
      "grad_norm": 0.9137707352638245,
      "learning_rate": 4.924191750278707e-05,
      "loss": 0.9629,
      "step": 340
    },
    {
      "epoch": 0.046818045012206135,
      "grad_norm": 0.8504415154457092,
      "learning_rate": 4.9219620958751396e-05,
      "loss": 0.9583,
      "step": 350
    },
    {
      "epoch": 0.04815570344112631,
      "grad_norm": 0.9782169461250305,
      "learning_rate": 4.919732441471572e-05,
      "loss": 0.949,
      "step": 360
    },
    {
      "epoch": 0.049493361870046486,
      "grad_norm": 0.9385510087013245,
      "learning_rate": 4.9175027870680044e-05,
      "loss": 0.938,
      "step": 370
    },
    {
      "epoch": 0.05083102029896666,
      "grad_norm": 0.904502809047699,
      "learning_rate": 4.915273132664438e-05,
      "loss": 0.9608,
      "step": 380
    },
    {
      "epoch": 0.05216867872788684,
      "grad_norm": 0.8523162603378296,
      "learning_rate": 4.91304347826087e-05,
      "loss": 0.979,
      "step": 390
    },
    {
      "epoch": 0.053506337156807006,
      "grad_norm": 0.9006927609443665,
      "learning_rate": 4.9108138238573025e-05,
      "loss": 0.9365,
      "step": 400
    },
    {
      "epoch": 0.05484399558572718,
      "grad_norm": 0.9971051216125488,
      "learning_rate": 4.9085841694537346e-05,
      "loss": 0.9488,
      "step": 410
    },
    {
      "epoch": 0.05618165401464736,
      "grad_norm": 0.9925211071968079,
      "learning_rate": 4.906354515050167e-05,
      "loss": 0.9515,
      "step": 420
    },
    {
      "epoch": 0.05751931244356753,
      "grad_norm": 0.8851551413536072,
      "learning_rate": 4.9041248606466e-05,
      "loss": 0.9281,
      "step": 430
    },
    {
      "epoch": 0.05885697087248771,
      "grad_norm": 0.9056417346000671,
      "learning_rate": 4.901895206243032e-05,
      "loss": 0.9408,
      "step": 440
    },
    {
      "epoch": 0.060194629301407884,
      "grad_norm": 0.9984219670295715,
      "learning_rate": 4.8996655518394655e-05,
      "loss": 0.9431,
      "step": 450
    },
    {
      "epoch": 0.06153228773032806,
      "grad_norm": 0.848658561706543,
      "learning_rate": 4.8974358974358975e-05,
      "loss": 0.9534,
      "step": 460
    },
    {
      "epoch": 0.06286994615924824,
      "grad_norm": 0.8515983819961548,
      "learning_rate": 4.89520624303233e-05,
      "loss": 0.9452,
      "step": 470
    },
    {
      "epoch": 0.0642076045881684,
      "grad_norm": 0.926074206829071,
      "learning_rate": 4.892976588628762e-05,
      "loss": 0.9201,
      "step": 480
    },
    {
      "epoch": 0.06554526301708859,
      "grad_norm": 0.9351306557655334,
      "learning_rate": 4.890746934225196e-05,
      "loss": 0.9638,
      "step": 490
    },
    {
      "epoch": 0.06688292144600876,
      "grad_norm": 0.8311704993247986,
      "learning_rate": 4.888517279821628e-05,
      "loss": 0.9181,
      "step": 500
    },
    {
      "epoch": 0.06822057987492894,
      "grad_norm": 0.8426694869995117,
      "learning_rate": 4.8862876254180605e-05,
      "loss": 0.9416,
      "step": 510
    },
    {
      "epoch": 0.06955823830384911,
      "grad_norm": 0.8023571372032166,
      "learning_rate": 4.884057971014493e-05,
      "loss": 0.931,
      "step": 520
    },
    {
      "epoch": 0.07089589673276929,
      "grad_norm": 0.9686022400856018,
      "learning_rate": 4.881828316610926e-05,
      "loss": 0.9233,
      "step": 530
    },
    {
      "epoch": 0.07223355516168946,
      "grad_norm": 0.9410991072654724,
      "learning_rate": 4.879598662207358e-05,
      "loss": 0.9569,
      "step": 540
    },
    {
      "epoch": 0.07357121359060964,
      "grad_norm": 0.9132514595985413,
      "learning_rate": 4.877369007803791e-05,
      "loss": 0.9115,
      "step": 550
    },
    {
      "epoch": 0.07490887201952981,
      "grad_norm": 0.871471643447876,
      "learning_rate": 4.8751393534002235e-05,
      "loss": 0.9301,
      "step": 560
    },
    {
      "epoch": 0.07624653044844999,
      "grad_norm": 0.9739051461219788,
      "learning_rate": 4.8729096989966555e-05,
      "loss": 0.9682,
      "step": 570
    },
    {
      "epoch": 0.07758418887737016,
      "grad_norm": 0.916255533695221,
      "learning_rate": 4.870680044593088e-05,
      "loss": 0.9276,
      "step": 580
    },
    {
      "epoch": 0.07892184730629034,
      "grad_norm": 0.9056844711303711,
      "learning_rate": 4.868450390189521e-05,
      "loss": 0.9506,
      "step": 590
    },
    {
      "epoch": 0.08025950573521051,
      "grad_norm": 0.7731453776359558,
      "learning_rate": 4.866220735785954e-05,
      "loss": 0.9503,
      "step": 600
    },
    {
      "epoch": 0.0815971641641307,
      "grad_norm": 0.826329231262207,
      "learning_rate": 4.863991081382386e-05,
      "loss": 0.9189,
      "step": 610
    },
    {
      "epoch": 0.08293482259305086,
      "grad_norm": 0.7749449610710144,
      "learning_rate": 4.8617614269788185e-05,
      "loss": 0.9347,
      "step": 620
    },
    {
      "epoch": 0.08427248102197105,
      "grad_norm": 0.8818713426589966,
      "learning_rate": 4.859531772575251e-05,
      "loss": 0.9665,
      "step": 630
    },
    {
      "epoch": 0.08561013945089122,
      "grad_norm": 0.8338486552238464,
      "learning_rate": 4.857302118171684e-05,
      "loss": 0.9548,
      "step": 640
    },
    {
      "epoch": 0.08694779787981138,
      "grad_norm": 1.0628488063812256,
      "learning_rate": 4.855072463768116e-05,
      "loss": 0.9532,
      "step": 650
    },
    {
      "epoch": 0.08828545630873157,
      "grad_norm": 0.7319684028625488,
      "learning_rate": 4.852842809364549e-05,
      "loss": 0.9631,
      "step": 660
    },
    {
      "epoch": 0.08962311473765174,
      "grad_norm": 0.8988972902297974,
      "learning_rate": 4.8506131549609814e-05,
      "loss": 0.9646,
      "step": 670
    },
    {
      "epoch": 0.09096077316657192,
      "grad_norm": 0.8432270288467407,
      "learning_rate": 4.848383500557414e-05,
      "loss": 0.9407,
      "step": 680
    },
    {
      "epoch": 0.09229843159549209,
      "grad_norm": 0.7801823616027832,
      "learning_rate": 4.846153846153846e-05,
      "loss": 0.9274,
      "step": 690
    },
    {
      "epoch": 0.09363609002441227,
      "grad_norm": 0.8254121541976929,
      "learning_rate": 4.843924191750279e-05,
      "loss": 0.9323,
      "step": 700
    },
    {
      "epoch": 0.09497374845333244,
      "grad_norm": 0.8667145371437073,
      "learning_rate": 4.8416945373467117e-05,
      "loss": 0.9429,
      "step": 710
    },
    {
      "epoch": 0.09631140688225262,
      "grad_norm": 0.7492014169692993,
      "learning_rate": 4.839464882943144e-05,
      "loss": 0.9292,
      "step": 720
    },
    {
      "epoch": 0.09764906531117279,
      "grad_norm": 0.7984621524810791,
      "learning_rate": 4.8372352285395764e-05,
      "loss": 0.9378,
      "step": 730
    },
    {
      "epoch": 0.09898672374009297,
      "grad_norm": 0.777376115322113,
      "learning_rate": 4.835005574136009e-05,
      "loss": 0.9311,
      "step": 740
    },
    {
      "epoch": 0.10032438216901314,
      "grad_norm": 0.8671090602874756,
      "learning_rate": 4.832775919732442e-05,
      "loss": 0.9729,
      "step": 750
    },
    {
      "epoch": 0.10166204059793332,
      "grad_norm": 0.798641562461853,
      "learning_rate": 4.830546265328874e-05,
      "loss": 0.933,
      "step": 760
    },
    {
      "epoch": 0.10299969902685349,
      "grad_norm": 0.8717724084854126,
      "learning_rate": 4.8283166109253067e-05,
      "loss": 0.9392,
      "step": 770
    },
    {
      "epoch": 0.10433735745577367,
      "grad_norm": 0.8237734436988831,
      "learning_rate": 4.8260869565217394e-05,
      "loss": 0.9978,
      "step": 780
    },
    {
      "epoch": 0.10567501588469384,
      "grad_norm": 0.7720577120780945,
      "learning_rate": 4.823857302118172e-05,
      "loss": 0.9298,
      "step": 790
    },
    {
      "epoch": 0.10701267431361401,
      "grad_norm": 0.8183701038360596,
      "learning_rate": 4.821627647714604e-05,
      "loss": 0.9432,
      "step": 800
    },
    {
      "epoch": 0.1083503327425342,
      "grad_norm": 0.9079931378364563,
      "learning_rate": 4.8193979933110376e-05,
      "loss": 0.9281,
      "step": 810
    },
    {
      "epoch": 0.10968799117145436,
      "grad_norm": 0.8008492588996887,
      "learning_rate": 4.8171683389074696e-05,
      "loss": 0.9387,
      "step": 820
    },
    {
      "epoch": 0.11102564960037455,
      "grad_norm": 0.7983224391937256,
      "learning_rate": 4.8149386845039023e-05,
      "loss": 0.9915,
      "step": 830
    },
    {
      "epoch": 0.11236330802929471,
      "grad_norm": 0.8683564066886902,
      "learning_rate": 4.8127090301003344e-05,
      "loss": 0.9193,
      "step": 840
    },
    {
      "epoch": 0.1137009664582149,
      "grad_norm": 0.7505449056625366,
      "learning_rate": 4.810479375696767e-05,
      "loss": 0.9339,
      "step": 850
    },
    {
      "epoch": 0.11503862488713507,
      "grad_norm": 0.7955744862556458,
      "learning_rate": 4.8082497212932e-05,
      "loss": 0.9218,
      "step": 860
    },
    {
      "epoch": 0.11637628331605525,
      "grad_norm": 0.7837681174278259,
      "learning_rate": 4.806020066889632e-05,
      "loss": 0.9149,
      "step": 870
    },
    {
      "epoch": 0.11771394174497542,
      "grad_norm": 0.8210752010345459,
      "learning_rate": 4.803790412486065e-05,
      "loss": 0.9133,
      "step": 880
    },
    {
      "epoch": 0.1190516001738956,
      "grad_norm": 0.7718283534049988,
      "learning_rate": 4.8015607580824973e-05,
      "loss": 0.9315,
      "step": 890
    },
    {
      "epoch": 0.12038925860281577,
      "grad_norm": 0.7989736199378967,
      "learning_rate": 4.79933110367893e-05,
      "loss": 0.919,
      "step": 900
    },
    {
      "epoch": 0.12172691703173595,
      "grad_norm": 0.7611091136932373,
      "learning_rate": 4.797101449275362e-05,
      "loss": 0.9131,
      "step": 910
    },
    {
      "epoch": 0.12306457546065612,
      "grad_norm": 0.7431515455245972,
      "learning_rate": 4.7948717948717955e-05,
      "loss": 0.931,
      "step": 920
    },
    {
      "epoch": 0.1244022338895763,
      "grad_norm": 0.7779104709625244,
      "learning_rate": 4.7926421404682276e-05,
      "loss": 0.9249,
      "step": 930
    },
    {
      "epoch": 0.12573989231849647,
      "grad_norm": 0.7644228339195251,
      "learning_rate": 4.79041248606466e-05,
      "loss": 0.954,
      "step": 940
    },
    {
      "epoch": 0.12707755074741664,
      "grad_norm": 0.7603705525398254,
      "learning_rate": 4.788182831661093e-05,
      "loss": 0.9181,
      "step": 950
    },
    {
      "epoch": 0.1284152091763368,
      "grad_norm": 0.8287662863731384,
      "learning_rate": 4.785953177257526e-05,
      "loss": 0.9641,
      "step": 960
    },
    {
      "epoch": 0.129752867605257,
      "grad_norm": 0.7384454607963562,
      "learning_rate": 4.783723522853958e-05,
      "loss": 0.9436,
      "step": 970
    },
    {
      "epoch": 0.13109052603417717,
      "grad_norm": 0.7253111600875854,
      "learning_rate": 4.78149386845039e-05,
      "loss": 0.9005,
      "step": 980
    },
    {
      "epoch": 0.13242818446309734,
      "grad_norm": 0.7387629747390747,
      "learning_rate": 4.779264214046823e-05,
      "loss": 0.9578,
      "step": 990
    },
    {
      "epoch": 0.1337658428920175,
      "grad_norm": 0.9297045469284058,
      "learning_rate": 4.777034559643255e-05,
      "loss": 0.9573,
      "step": 1000
    },
    {
      "epoch": 0.1351035013209377,
      "grad_norm": 0.7047936320304871,
      "learning_rate": 4.774804905239688e-05,
      "loss": 0.942,
      "step": 1010
    },
    {
      "epoch": 0.13644115974985788,
      "grad_norm": 0.7365144491195679,
      "learning_rate": 4.772575250836121e-05,
      "loss": 0.9196,
      "step": 1020
    },
    {
      "epoch": 0.13777881817877805,
      "grad_norm": 0.7585029006004333,
      "learning_rate": 4.7703455964325535e-05,
      "loss": 0.9114,
      "step": 1030
    },
    {
      "epoch": 0.13911647660769821,
      "grad_norm": 0.803127646446228,
      "learning_rate": 4.7681159420289855e-05,
      "loss": 0.9525,
      "step": 1040
    },
    {
      "epoch": 0.1404541350366184,
      "grad_norm": 0.7344565391540527,
      "learning_rate": 4.765886287625418e-05,
      "loss": 0.9159,
      "step": 1050
    },
    {
      "epoch": 0.14179179346553858,
      "grad_norm": 0.7270247936248779,
      "learning_rate": 4.763656633221851e-05,
      "loss": 0.9392,
      "step": 1060
    },
    {
      "epoch": 0.14312945189445875,
      "grad_norm": 0.7946987748146057,
      "learning_rate": 4.761426978818284e-05,
      "loss": 0.9273,
      "step": 1070
    },
    {
      "epoch": 0.14446711032337892,
      "grad_norm": 0.7737051248550415,
      "learning_rate": 4.759197324414716e-05,
      "loss": 0.9621,
      "step": 1080
    },
    {
      "epoch": 0.1458047687522991,
      "grad_norm": 0.7937643527984619,
      "learning_rate": 4.7569676700111485e-05,
      "loss": 0.9098,
      "step": 1090
    },
    {
      "epoch": 0.14714242718121928,
      "grad_norm": 0.7651366591453552,
      "learning_rate": 4.754738015607581e-05,
      "loss": 0.93,
      "step": 1100
    },
    {
      "epoch": 0.14848008561013945,
      "grad_norm": 0.7946449518203735,
      "learning_rate": 4.752508361204014e-05,
      "loss": 0.9022,
      "step": 1110
    },
    {
      "epoch": 0.14981774403905962,
      "grad_norm": 0.7713757753372192,
      "learning_rate": 4.750278706800446e-05,
      "loss": 0.9146,
      "step": 1120
    },
    {
      "epoch": 0.1511554024679798,
      "grad_norm": 0.790418267250061,
      "learning_rate": 4.748049052396879e-05,
      "loss": 0.9438,
      "step": 1130
    },
    {
      "epoch": 0.15249306089689998,
      "grad_norm": 0.785868763923645,
      "learning_rate": 4.7458193979933115e-05,
      "loss": 0.9381,
      "step": 1140
    },
    {
      "epoch": 0.15383071932582015,
      "grad_norm": 0.7452868223190308,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 0.9419,
      "step": 1150
    },
    {
      "epoch": 0.15516837775474032,
      "grad_norm": 0.7221753001213074,
      "learning_rate": 4.741360089186176e-05,
      "loss": 0.9156,
      "step": 1160
    },
    {
      "epoch": 0.1565060361836605,
      "grad_norm": 0.7417294979095459,
      "learning_rate": 4.739130434782609e-05,
      "loss": 0.9244,
      "step": 1170
    },
    {
      "epoch": 0.1578436946125807,
      "grad_norm": 0.7405938506126404,
      "learning_rate": 4.736900780379042e-05,
      "loss": 0.9454,
      "step": 1180
    },
    {
      "epoch": 0.15918135304150086,
      "grad_norm": 0.7853581309318542,
      "learning_rate": 4.734671125975474e-05,
      "loss": 0.9271,
      "step": 1190
    },
    {
      "epoch": 0.16051901147042102,
      "grad_norm": 0.7491259574890137,
      "learning_rate": 4.7324414715719065e-05,
      "loss": 0.9751,
      "step": 1200
    },
    {
      "epoch": 0.1618566698993412,
      "grad_norm": 0.7200047969818115,
      "learning_rate": 4.730211817168339e-05,
      "loss": 0.9135,
      "step": 1210
    },
    {
      "epoch": 0.1631943283282614,
      "grad_norm": 0.7871435880661011,
      "learning_rate": 4.727982162764772e-05,
      "loss": 0.9452,
      "step": 1220
    },
    {
      "epoch": 0.16453198675718156,
      "grad_norm": 0.850436806678772,
      "learning_rate": 4.725752508361204e-05,
      "loss": 0.9592,
      "step": 1230
    },
    {
      "epoch": 0.16586964518610173,
      "grad_norm": 0.7558240294456482,
      "learning_rate": 4.7235228539576374e-05,
      "loss": 0.9212,
      "step": 1240
    },
    {
      "epoch": 0.1672073036150219,
      "grad_norm": 0.755628764629364,
      "learning_rate": 4.7212931995540694e-05,
      "loss": 0.9258,
      "step": 1250
    },
    {
      "epoch": 0.1685449620439421,
      "grad_norm": 0.7406746745109558,
      "learning_rate": 4.7190635451505015e-05,
      "loss": 0.9029,
      "step": 1260
    },
    {
      "epoch": 0.16988262047286226,
      "grad_norm": 0.7381203174591064,
      "learning_rate": 4.716833890746934e-05,
      "loss": 0.9288,
      "step": 1270
    },
    {
      "epoch": 0.17122027890178243,
      "grad_norm": 0.734977662563324,
      "learning_rate": 4.714604236343367e-05,
      "loss": 0.9146,
      "step": 1280
    },
    {
      "epoch": 0.1725579373307026,
      "grad_norm": 0.773557722568512,
      "learning_rate": 4.7123745819397996e-05,
      "loss": 0.9379,
      "step": 1290
    },
    {
      "epoch": 0.17389559575962277,
      "grad_norm": 0.7905227541923523,
      "learning_rate": 4.710144927536232e-05,
      "loss": 0.945,
      "step": 1300
    },
    {
      "epoch": 0.17523325418854296,
      "grad_norm": 0.6789513230323792,
      "learning_rate": 4.707915273132665e-05,
      "loss": 0.9173,
      "step": 1310
    },
    {
      "epoch": 0.17657091261746313,
      "grad_norm": 0.7373024225234985,
      "learning_rate": 4.705685618729097e-05,
      "loss": 0.9316,
      "step": 1320
    },
    {
      "epoch": 0.1779085710463833,
      "grad_norm": 0.7614285945892334,
      "learning_rate": 4.70345596432553e-05,
      "loss": 0.9092,
      "step": 1330
    },
    {
      "epoch": 0.17924622947530347,
      "grad_norm": 0.7283208966255188,
      "learning_rate": 4.701226309921962e-05,
      "loss": 0.9411,
      "step": 1340
    },
    {
      "epoch": 0.18058388790422367,
      "grad_norm": 0.8129592537879944,
      "learning_rate": 4.698996655518395e-05,
      "loss": 0.9062,
      "step": 1350
    },
    {
      "epoch": 0.18192154633314384,
      "grad_norm": 1.0089343786239624,
      "learning_rate": 4.6967670011148274e-05,
      "loss": 0.9135,
      "step": 1360
    },
    {
      "epoch": 0.183259204762064,
      "grad_norm": 0.7166935801506042,
      "learning_rate": 4.69453734671126e-05,
      "loss": 0.9255,
      "step": 1370
    },
    {
      "epoch": 0.18459686319098417,
      "grad_norm": 0.7292187213897705,
      "learning_rate": 4.692307692307693e-05,
      "loss": 0.9288,
      "step": 1380
    },
    {
      "epoch": 0.18593452161990437,
      "grad_norm": 0.7135089635848999,
      "learning_rate": 4.690078037904125e-05,
      "loss": 0.9283,
      "step": 1390
    },
    {
      "epoch": 0.18727218004882454,
      "grad_norm": 0.7056137323379517,
      "learning_rate": 4.6878483835005576e-05,
      "loss": 0.9292,
      "step": 1400
    },
    {
      "epoch": 0.1886098384777447,
      "grad_norm": 0.8066134452819824,
      "learning_rate": 4.6856187290969897e-05,
      "loss": 0.9332,
      "step": 1410
    },
    {
      "epoch": 0.18994749690666488,
      "grad_norm": 0.7111193537712097,
      "learning_rate": 4.683389074693423e-05,
      "loss": 0.9206,
      "step": 1420
    },
    {
      "epoch": 0.19128515533558507,
      "grad_norm": 0.7342537641525269,
      "learning_rate": 4.681159420289855e-05,
      "loss": 0.9531,
      "step": 1430
    },
    {
      "epoch": 0.19262281376450524,
      "grad_norm": 0.6958606839179993,
      "learning_rate": 4.678929765886288e-05,
      "loss": 0.9137,
      "step": 1440
    },
    {
      "epoch": 0.1939604721934254,
      "grad_norm": 0.7054579854011536,
      "learning_rate": 4.6767001114827206e-05,
      "loss": 0.9292,
      "step": 1450
    },
    {
      "epoch": 0.19529813062234558,
      "grad_norm": 0.7118688821792603,
      "learning_rate": 4.674470457079153e-05,
      "loss": 0.9235,
      "step": 1460
    },
    {
      "epoch": 0.19663578905126575,
      "grad_norm": 0.7327072024345398,
      "learning_rate": 4.6722408026755853e-05,
      "loss": 0.9056,
      "step": 1470
    },
    {
      "epoch": 0.19797344748018594,
      "grad_norm": 0.6965751051902771,
      "learning_rate": 4.670011148272018e-05,
      "loss": 0.937,
      "step": 1480
    },
    {
      "epoch": 0.1993111059091061,
      "grad_norm": 0.7037297487258911,
      "learning_rate": 4.667781493868451e-05,
      "loss": 0.9266,
      "step": 1490
    },
    {
      "epoch": 0.20064876433802628,
      "grad_norm": 0.7370728850364685,
      "learning_rate": 4.6655518394648835e-05,
      "loss": 0.9303,
      "step": 1500
    },
    {
      "epoch": 0.20198642276694645,
      "grad_norm": 0.7727379202842712,
      "learning_rate": 4.6633221850613156e-05,
      "loss": 0.93,
      "step": 1510
    },
    {
      "epoch": 0.20332408119586665,
      "grad_norm": 0.7419516444206238,
      "learning_rate": 4.661092530657748e-05,
      "loss": 0.9269,
      "step": 1520
    },
    {
      "epoch": 0.20466173962478681,
      "grad_norm": 0.7666073441505432,
      "learning_rate": 4.658862876254181e-05,
      "loss": 0.9193,
      "step": 1530
    },
    {
      "epoch": 0.20599939805370698,
      "grad_norm": 0.6566784977912903,
      "learning_rate": 4.656633221850613e-05,
      "loss": 0.9466,
      "step": 1540
    },
    {
      "epoch": 0.20733705648262715,
      "grad_norm": 0.6698518991470337,
      "learning_rate": 4.654403567447046e-05,
      "loss": 0.9261,
      "step": 1550
    },
    {
      "epoch": 0.20867471491154735,
      "grad_norm": 0.6680171489715576,
      "learning_rate": 4.6521739130434785e-05,
      "loss": 0.933,
      "step": 1560
    },
    {
      "epoch": 0.21001237334046752,
      "grad_norm": 0.739748477935791,
      "learning_rate": 4.649944258639911e-05,
      "loss": 0.9297,
      "step": 1570
    },
    {
      "epoch": 0.2113500317693877,
      "grad_norm": 0.758496880531311,
      "learning_rate": 4.647714604236343e-05,
      "loss": 0.9566,
      "step": 1580
    },
    {
      "epoch": 0.21268769019830785,
      "grad_norm": 0.8248854279518127,
      "learning_rate": 4.645484949832776e-05,
      "loss": 0.9314,
      "step": 1590
    },
    {
      "epoch": 0.21402534862722802,
      "grad_norm": 0.7967570424079895,
      "learning_rate": 4.643255295429209e-05,
      "loss": 0.9606,
      "step": 1600
    },
    {
      "epoch": 0.21536300705614822,
      "grad_norm": 0.7075257897377014,
      "learning_rate": 4.6410256410256415e-05,
      "loss": 0.917,
      "step": 1610
    },
    {
      "epoch": 0.2167006654850684,
      "grad_norm": 0.7062041163444519,
      "learning_rate": 4.6387959866220735e-05,
      "loss": 0.9198,
      "step": 1620
    },
    {
      "epoch": 0.21803832391398856,
      "grad_norm": 0.7659404873847961,
      "learning_rate": 4.636566332218506e-05,
      "loss": 0.9223,
      "step": 1630
    },
    {
      "epoch": 0.21937598234290873,
      "grad_norm": 0.742732048034668,
      "learning_rate": 4.634336677814939e-05,
      "loss": 0.9143,
      "step": 1640
    },
    {
      "epoch": 0.22071364077182892,
      "grad_norm": 0.6654959917068481,
      "learning_rate": 4.632107023411372e-05,
      "loss": 0.9324,
      "step": 1650
    },
    {
      "epoch": 0.2220512992007491,
      "grad_norm": 0.8667898774147034,
      "learning_rate": 4.629877369007804e-05,
      "loss": 0.942,
      "step": 1660
    },
    {
      "epoch": 0.22338895762966926,
      "grad_norm": 0.6436029672622681,
      "learning_rate": 4.6276477146042365e-05,
      "loss": 0.9293,
      "step": 1670
    },
    {
      "epoch": 0.22472661605858943,
      "grad_norm": 0.6536558866500854,
      "learning_rate": 4.625418060200669e-05,
      "loss": 0.9308,
      "step": 1680
    },
    {
      "epoch": 0.22606427448750963,
      "grad_norm": 0.7051981687545776,
      "learning_rate": 4.623188405797101e-05,
      "loss": 0.9279,
      "step": 1690
    },
    {
      "epoch": 0.2274019329164298,
      "grad_norm": 0.7813133001327515,
      "learning_rate": 4.620958751393534e-05,
      "loss": 0.9283,
      "step": 1700
    },
    {
      "epoch": 0.22873959134534996,
      "grad_norm": 0.6906253099441528,
      "learning_rate": 4.618729096989967e-05,
      "loss": 0.9318,
      "step": 1710
    },
    {
      "epoch": 0.23007724977427013,
      "grad_norm": 1.0497572422027588,
      "learning_rate": 4.6164994425863995e-05,
      "loss": 0.9252,
      "step": 1720
    },
    {
      "epoch": 0.23141490820319033,
      "grad_norm": 0.9774326086044312,
      "learning_rate": 4.6142697881828315e-05,
      "loss": 0.9224,
      "step": 1730
    },
    {
      "epoch": 0.2327525666321105,
      "grad_norm": 0.72371506690979,
      "learning_rate": 4.612040133779265e-05,
      "loss": 0.9396,
      "step": 1740
    },
    {
      "epoch": 0.23409022506103067,
      "grad_norm": 0.7379743456840515,
      "learning_rate": 4.609810479375697e-05,
      "loss": 0.9748,
      "step": 1750
    },
    {
      "epoch": 0.23542788348995083,
      "grad_norm": 0.6943268775939941,
      "learning_rate": 4.60758082497213e-05,
      "loss": 0.8992,
      "step": 1760
    },
    {
      "epoch": 0.236765541918871,
      "grad_norm": 0.766086757183075,
      "learning_rate": 4.605351170568562e-05,
      "loss": 0.9084,
      "step": 1770
    },
    {
      "epoch": 0.2381032003477912,
      "grad_norm": 0.7345722913742065,
      "learning_rate": 4.603121516164995e-05,
      "loss": 0.9686,
      "step": 1780
    },
    {
      "epoch": 0.23944085877671137,
      "grad_norm": 0.739203929901123,
      "learning_rate": 4.600891861761427e-05,
      "loss": 0.9094,
      "step": 1790
    },
    {
      "epoch": 0.24077851720563154,
      "grad_norm": 0.7363327741622925,
      "learning_rate": 4.59866220735786e-05,
      "loss": 0.9179,
      "step": 1800
    },
    {
      "epoch": 0.2421161756345517,
      "grad_norm": 0.7948965430259705,
      "learning_rate": 4.5964325529542926e-05,
      "loss": 0.9287,
      "step": 1810
    },
    {
      "epoch": 0.2434538340634719,
      "grad_norm": 0.693996012210846,
      "learning_rate": 4.594202898550725e-05,
      "loss": 0.9278,
      "step": 1820
    },
    {
      "epoch": 0.24479149249239207,
      "grad_norm": 0.6800012588500977,
      "learning_rate": 4.5919732441471574e-05,
      "loss": 0.9368,
      "step": 1830
    },
    {
      "epoch": 0.24612915092131224,
      "grad_norm": 0.8167952299118042,
      "learning_rate": 4.5897435897435895e-05,
      "loss": 0.9421,
      "step": 1840
    },
    {
      "epoch": 0.2474668093502324,
      "grad_norm": 0.673104465007782,
      "learning_rate": 4.587513935340023e-05,
      "loss": 0.9072,
      "step": 1850
    },
    {
      "epoch": 0.2488044677791526,
      "grad_norm": 0.7318885326385498,
      "learning_rate": 4.585284280936455e-05,
      "loss": 0.9378,
      "step": 1860
    },
    {
      "epoch": 0.25014212620807275,
      "grad_norm": 0.7903574109077454,
      "learning_rate": 4.5830546265328876e-05,
      "loss": 0.9266,
      "step": 1870
    },
    {
      "epoch": 0.25147978463699294,
      "grad_norm": 0.6913496851921082,
      "learning_rate": 4.5808249721293204e-05,
      "loss": 0.9153,
      "step": 1880
    },
    {
      "epoch": 0.25281744306591314,
      "grad_norm": 0.735567033290863,
      "learning_rate": 4.578595317725753e-05,
      "loss": 0.9325,
      "step": 1890
    },
    {
      "epoch": 0.2541551014948333,
      "grad_norm": 0.7465164065361023,
      "learning_rate": 4.576365663322185e-05,
      "loss": 0.9285,
      "step": 1900
    },
    {
      "epoch": 0.2554927599237535,
      "grad_norm": 0.6982099413871765,
      "learning_rate": 4.574136008918618e-05,
      "loss": 0.9287,
      "step": 1910
    },
    {
      "epoch": 0.2568304183526736,
      "grad_norm": 0.7666289806365967,
      "learning_rate": 4.5719063545150506e-05,
      "loss": 0.9269,
      "step": 1920
    },
    {
      "epoch": 0.2581680767815938,
      "grad_norm": 0.7170525193214417,
      "learning_rate": 4.569676700111483e-05,
      "loss": 0.932,
      "step": 1930
    },
    {
      "epoch": 0.259505735210514,
      "grad_norm": 0.7373540997505188,
      "learning_rate": 4.5674470457079154e-05,
      "loss": 0.9197,
      "step": 1940
    },
    {
      "epoch": 0.26084339363943415,
      "grad_norm": 0.6786831617355347,
      "learning_rate": 4.565217391304348e-05,
      "loss": 0.9201,
      "step": 1950
    },
    {
      "epoch": 0.26218105206835435,
      "grad_norm": 0.6725291013717651,
      "learning_rate": 4.562987736900781e-05,
      "loss": 0.9156,
      "step": 1960
    },
    {
      "epoch": 0.26351871049727454,
      "grad_norm": 0.7779619693756104,
      "learning_rate": 4.560758082497213e-05,
      "loss": 0.9132,
      "step": 1970
    },
    {
      "epoch": 0.2648563689261947,
      "grad_norm": 0.6780773997306824,
      "learning_rate": 4.5585284280936456e-05,
      "loss": 0.9211,
      "step": 1980
    },
    {
      "epoch": 0.2661940273551149,
      "grad_norm": 0.7099452614784241,
      "learning_rate": 4.556298773690078e-05,
      "loss": 0.9222,
      "step": 1990
    },
    {
      "epoch": 0.267531685784035,
      "grad_norm": 0.6996732354164124,
      "learning_rate": 4.554069119286511e-05,
      "loss": 0.9236,
      "step": 2000
    },
    {
      "epoch": 0.2688693442129552,
      "grad_norm": 0.6341898441314697,
      "learning_rate": 4.551839464882943e-05,
      "loss": 0.9123,
      "step": 2010
    },
    {
      "epoch": 0.2702070026418754,
      "grad_norm": 0.6850053668022156,
      "learning_rate": 4.549609810479376e-05,
      "loss": 0.9051,
      "step": 2020
    },
    {
      "epoch": 0.27154466107079556,
      "grad_norm": 0.6575720906257629,
      "learning_rate": 4.5473801560758086e-05,
      "loss": 0.9429,
      "step": 2030
    },
    {
      "epoch": 0.27288231949971575,
      "grad_norm": 0.7396436333656311,
      "learning_rate": 4.545150501672241e-05,
      "loss": 0.943,
      "step": 2040
    },
    {
      "epoch": 0.27421997792863595,
      "grad_norm": 0.6853494644165039,
      "learning_rate": 4.5429208472686733e-05,
      "loss": 0.9306,
      "step": 2050
    },
    {
      "epoch": 0.2755576363575561,
      "grad_norm": 0.7400047183036804,
      "learning_rate": 4.540691192865106e-05,
      "loss": 0.8916,
      "step": 2060
    },
    {
      "epoch": 0.2768952947864763,
      "grad_norm": 0.7339215874671936,
      "learning_rate": 4.538461538461539e-05,
      "loss": 0.9421,
      "step": 2070
    },
    {
      "epoch": 0.27823295321539643,
      "grad_norm": 0.6956488490104675,
      "learning_rate": 4.5362318840579715e-05,
      "loss": 0.9227,
      "step": 2080
    },
    {
      "epoch": 0.2795706116443166,
      "grad_norm": 0.7545130252838135,
      "learning_rate": 4.5340022296544036e-05,
      "loss": 0.9213,
      "step": 2090
    },
    {
      "epoch": 0.2809082700732368,
      "grad_norm": 0.683312177658081,
      "learning_rate": 4.531772575250836e-05,
      "loss": 0.9274,
      "step": 2100
    },
    {
      "epoch": 0.28224592850215696,
      "grad_norm": 0.7918410301208496,
      "learning_rate": 4.529542920847269e-05,
      "loss": 0.9388,
      "step": 2110
    },
    {
      "epoch": 0.28358358693107716,
      "grad_norm": 0.6769546270370483,
      "learning_rate": 4.527313266443701e-05,
      "loss": 0.9267,
      "step": 2120
    },
    {
      "epoch": 0.2849212453599973,
      "grad_norm": 0.6586754322052002,
      "learning_rate": 4.525083612040134e-05,
      "loss": 0.9723,
      "step": 2130
    },
    {
      "epoch": 0.2862589037889175,
      "grad_norm": 0.8051396608352661,
      "learning_rate": 4.5228539576365665e-05,
      "loss": 0.8988,
      "step": 2140
    },
    {
      "epoch": 0.2875965622178377,
      "grad_norm": 0.7413325309753418,
      "learning_rate": 4.520624303232999e-05,
      "loss": 0.9038,
      "step": 2150
    },
    {
      "epoch": 0.28893422064675783,
      "grad_norm": 0.706710934638977,
      "learning_rate": 4.518394648829431e-05,
      "loss": 0.9422,
      "step": 2160
    },
    {
      "epoch": 0.29027187907567803,
      "grad_norm": 0.7409068942070007,
      "learning_rate": 4.516164994425865e-05,
      "loss": 0.9379,
      "step": 2170
    },
    {
      "epoch": 0.2916095375045982,
      "grad_norm": 0.7208641171455383,
      "learning_rate": 4.513935340022297e-05,
      "loss": 0.9468,
      "step": 2180
    },
    {
      "epoch": 0.29294719593351837,
      "grad_norm": 0.7319011688232422,
      "learning_rate": 4.5117056856187295e-05,
      "loss": 0.9206,
      "step": 2190
    },
    {
      "epoch": 0.29428485436243856,
      "grad_norm": 0.7755290269851685,
      "learning_rate": 4.5094760312151615e-05,
      "loss": 0.9607,
      "step": 2200
    },
    {
      "epoch": 0.2956225127913587,
      "grad_norm": 0.6612139940261841,
      "learning_rate": 4.507246376811595e-05,
      "loss": 0.9088,
      "step": 2210
    },
    {
      "epoch": 0.2969601712202789,
      "grad_norm": 0.703395426273346,
      "learning_rate": 4.505016722408027e-05,
      "loss": 0.9254,
      "step": 2220
    },
    {
      "epoch": 0.2982978296491991,
      "grad_norm": 0.8138511776924133,
      "learning_rate": 4.502787068004459e-05,
      "loss": 0.9372,
      "step": 2230
    },
    {
      "epoch": 0.29963548807811924,
      "grad_norm": 0.723366379737854,
      "learning_rate": 4.5005574136008924e-05,
      "loss": 0.9225,
      "step": 2240
    },
    {
      "epoch": 0.30097314650703944,
      "grad_norm": 0.7014318704605103,
      "learning_rate": 4.4983277591973245e-05,
      "loss": 0.9251,
      "step": 2250
    },
    {
      "epoch": 0.3023108049359596,
      "grad_norm": 0.6844467520713806,
      "learning_rate": 4.496098104793757e-05,
      "loss": 0.9299,
      "step": 2260
    },
    {
      "epoch": 0.3036484633648798,
      "grad_norm": 0.6768231391906738,
      "learning_rate": 4.493868450390189e-05,
      "loss": 0.9156,
      "step": 2270
    },
    {
      "epoch": 0.30498612179379997,
      "grad_norm": 0.6991119384765625,
      "learning_rate": 4.491638795986623e-05,
      "loss": 0.94,
      "step": 2280
    },
    {
      "epoch": 0.3063237802227201,
      "grad_norm": 0.7961558699607849,
      "learning_rate": 4.489409141583055e-05,
      "loss": 0.91,
      "step": 2290
    },
    {
      "epoch": 0.3076614386516403,
      "grad_norm": 0.71505206823349,
      "learning_rate": 4.4871794871794874e-05,
      "loss": 0.923,
      "step": 2300
    },
    {
      "epoch": 0.3089990970805605,
      "grad_norm": 0.770924985408783,
      "learning_rate": 4.48494983277592e-05,
      "loss": 0.9092,
      "step": 2310
    },
    {
      "epoch": 0.31033675550948064,
      "grad_norm": 0.7330206036567688,
      "learning_rate": 4.482720178372353e-05,
      "loss": 0.9109,
      "step": 2320
    },
    {
      "epoch": 0.31167441393840084,
      "grad_norm": 0.8264217376708984,
      "learning_rate": 4.480490523968785e-05,
      "loss": 0.9198,
      "step": 2330
    },
    {
      "epoch": 0.313012072367321,
      "grad_norm": 0.6516013741493225,
      "learning_rate": 4.478260869565218e-05,
      "loss": 0.9298,
      "step": 2340
    },
    {
      "epoch": 0.3143497307962412,
      "grad_norm": 0.7064148783683777,
      "learning_rate": 4.4760312151616504e-05,
      "loss": 0.8982,
      "step": 2350
    },
    {
      "epoch": 0.3156873892251614,
      "grad_norm": 0.737553060054779,
      "learning_rate": 4.473801560758083e-05,
      "loss": 0.9464,
      "step": 2360
    },
    {
      "epoch": 0.3170250476540815,
      "grad_norm": 0.6850618124008179,
      "learning_rate": 4.471571906354515e-05,
      "loss": 0.9251,
      "step": 2370
    },
    {
      "epoch": 0.3183627060830017,
      "grad_norm": 0.7332990169525146,
      "learning_rate": 4.469342251950948e-05,
      "loss": 0.9419,
      "step": 2380
    },
    {
      "epoch": 0.3197003645119219,
      "grad_norm": 0.7573476433753967,
      "learning_rate": 4.4671125975473806e-05,
      "loss": 0.9019,
      "step": 2390
    },
    {
      "epoch": 0.32103802294084205,
      "grad_norm": 0.7181193232536316,
      "learning_rate": 4.464882943143813e-05,
      "loss": 0.92,
      "step": 2400
    },
    {
      "epoch": 0.32237568136976225,
      "grad_norm": 0.7305660247802734,
      "learning_rate": 4.4626532887402454e-05,
      "loss": 0.9501,
      "step": 2410
    },
    {
      "epoch": 0.3237133397986824,
      "grad_norm": 0.6749674677848816,
      "learning_rate": 4.460423634336678e-05,
      "loss": 0.9001,
      "step": 2420
    },
    {
      "epoch": 0.3250509982276026,
      "grad_norm": 0.7499062418937683,
      "learning_rate": 4.458193979933111e-05,
      "loss": 0.9296,
      "step": 2430
    },
    {
      "epoch": 0.3263886566565228,
      "grad_norm": 0.7387345433235168,
      "learning_rate": 4.455964325529543e-05,
      "loss": 0.9333,
      "step": 2440
    },
    {
      "epoch": 0.3277263150854429,
      "grad_norm": 0.7535333037376404,
      "learning_rate": 4.4537346711259756e-05,
      "loss": 0.9086,
      "step": 2450
    },
    {
      "epoch": 0.3290639735143631,
      "grad_norm": 0.7400828003883362,
      "learning_rate": 4.4515050167224084e-05,
      "loss": 0.906,
      "step": 2460
    },
    {
      "epoch": 0.33040163194328326,
      "grad_norm": 0.776470959186554,
      "learning_rate": 4.449275362318841e-05,
      "loss": 0.9403,
      "step": 2470
    },
    {
      "epoch": 0.33173929037220345,
      "grad_norm": 0.7071899175643921,
      "learning_rate": 4.447045707915273e-05,
      "loss": 0.9081,
      "step": 2480
    },
    {
      "epoch": 0.33307694880112365,
      "grad_norm": 0.7264969348907471,
      "learning_rate": 4.444816053511706e-05,
      "loss": 0.9419,
      "step": 2490
    },
    {
      "epoch": 0.3344146072300438,
      "grad_norm": 0.6852723360061646,
      "learning_rate": 4.4425863991081386e-05,
      "loss": 0.8828,
      "step": 2500
    },
    {
      "epoch": 0.335752265658964,
      "grad_norm": 0.7734622955322266,
      "learning_rate": 4.4403567447045706e-05,
      "loss": 0.9521,
      "step": 2510
    },
    {
      "epoch": 0.3370899240878842,
      "grad_norm": 0.6915856599807739,
      "learning_rate": 4.4381270903010034e-05,
      "loss": 0.9158,
      "step": 2520
    },
    {
      "epoch": 0.3384275825168043,
      "grad_norm": 0.6428331136703491,
      "learning_rate": 4.435897435897436e-05,
      "loss": 0.9174,
      "step": 2530
    },
    {
      "epoch": 0.3397652409457245,
      "grad_norm": 0.7315495014190674,
      "learning_rate": 4.433667781493869e-05,
      "loss": 0.9485,
      "step": 2540
    },
    {
      "epoch": 0.34110289937464466,
      "grad_norm": 0.7275439500808716,
      "learning_rate": 4.431438127090301e-05,
      "loss": 0.9339,
      "step": 2550
    },
    {
      "epoch": 0.34244055780356486,
      "grad_norm": 0.7206367254257202,
      "learning_rate": 4.4292084726867336e-05,
      "loss": 0.9151,
      "step": 2560
    },
    {
      "epoch": 0.34377821623248506,
      "grad_norm": 0.808509886264801,
      "learning_rate": 4.426978818283166e-05,
      "loss": 0.9389,
      "step": 2570
    },
    {
      "epoch": 0.3451158746614052,
      "grad_norm": 0.8358756303787231,
      "learning_rate": 4.424749163879599e-05,
      "loss": 0.9304,
      "step": 2580
    },
    {
      "epoch": 0.3464535330903254,
      "grad_norm": 0.7536786198616028,
      "learning_rate": 4.422519509476031e-05,
      "loss": 0.9461,
      "step": 2590
    },
    {
      "epoch": 0.34779119151924553,
      "grad_norm": 0.8207246661186218,
      "learning_rate": 4.4202898550724645e-05,
      "loss": 0.9069,
      "step": 2600
    },
    {
      "epoch": 0.34912884994816573,
      "grad_norm": 0.7621133327484131,
      "learning_rate": 4.4180602006688966e-05,
      "loss": 0.9249,
      "step": 2610
    },
    {
      "epoch": 0.35046650837708593,
      "grad_norm": 0.7429913878440857,
      "learning_rate": 4.415830546265329e-05,
      "loss": 0.9135,
      "step": 2620
    },
    {
      "epoch": 0.35180416680600607,
      "grad_norm": 0.7448993921279907,
      "learning_rate": 4.413600891861761e-05,
      "loss": 0.9283,
      "step": 2630
    },
    {
      "epoch": 0.35314182523492627,
      "grad_norm": 0.766309916973114,
      "learning_rate": 4.411371237458194e-05,
      "loss": 0.9451,
      "step": 2640
    },
    {
      "epoch": 0.35447948366384646,
      "grad_norm": 0.760205864906311,
      "learning_rate": 4.409141583054627e-05,
      "loss": 0.9276,
      "step": 2650
    },
    {
      "epoch": 0.3558171420927666,
      "grad_norm": 0.7085362672805786,
      "learning_rate": 4.406911928651059e-05,
      "loss": 0.8918,
      "step": 2660
    },
    {
      "epoch": 0.3571548005216868,
      "grad_norm": 0.720905065536499,
      "learning_rate": 4.404682274247492e-05,
      "loss": 0.9438,
      "step": 2670
    },
    {
      "epoch": 0.35849245895060694,
      "grad_norm": 0.6978422999382019,
      "learning_rate": 4.402452619843924e-05,
      "loss": 0.9076,
      "step": 2680
    },
    {
      "epoch": 0.35983011737952714,
      "grad_norm": 0.7447517514228821,
      "learning_rate": 4.400222965440357e-05,
      "loss": 0.8972,
      "step": 2690
    },
    {
      "epoch": 0.36116777580844733,
      "grad_norm": 0.6795587539672852,
      "learning_rate": 4.397993311036789e-05,
      "loss": 0.9212,
      "step": 2700
    },
    {
      "epoch": 0.3625054342373675,
      "grad_norm": 0.6829108595848083,
      "learning_rate": 4.3957636566332225e-05,
      "loss": 0.9175,
      "step": 2710
    },
    {
      "epoch": 0.36384309266628767,
      "grad_norm": 0.7299573421478271,
      "learning_rate": 4.3935340022296545e-05,
      "loss": 0.9361,
      "step": 2720
    },
    {
      "epoch": 0.3651807510952078,
      "grad_norm": 0.6947812438011169,
      "learning_rate": 4.391304347826087e-05,
      "loss": 0.9069,
      "step": 2730
    },
    {
      "epoch": 0.366518409524128,
      "grad_norm": 0.6397709846496582,
      "learning_rate": 4.38907469342252e-05,
      "loss": 0.9067,
      "step": 2740
    },
    {
      "epoch": 0.3678560679530482,
      "grad_norm": 0.7043381333351135,
      "learning_rate": 4.386845039018953e-05,
      "loss": 0.9319,
      "step": 2750
    },
    {
      "epoch": 0.36919372638196835,
      "grad_norm": 0.6214466691017151,
      "learning_rate": 4.384615384615385e-05,
      "loss": 0.892,
      "step": 2760
    },
    {
      "epoch": 0.37053138481088854,
      "grad_norm": 0.6803012490272522,
      "learning_rate": 4.3823857302118175e-05,
      "loss": 0.9158,
      "step": 2770
    },
    {
      "epoch": 0.37186904323980874,
      "grad_norm": 0.7464293241500854,
      "learning_rate": 4.38015607580825e-05,
      "loss": 0.9326,
      "step": 2780
    },
    {
      "epoch": 0.3732067016687289,
      "grad_norm": 0.751660943031311,
      "learning_rate": 4.377926421404682e-05,
      "loss": 0.9095,
      "step": 2790
    },
    {
      "epoch": 0.3745443600976491,
      "grad_norm": 0.7017607092857361,
      "learning_rate": 4.375696767001115e-05,
      "loss": 0.9167,
      "step": 2800
    },
    {
      "epoch": 0.3758820185265692,
      "grad_norm": 0.6740889549255371,
      "learning_rate": 4.373467112597548e-05,
      "loss": 0.9427,
      "step": 2810
    },
    {
      "epoch": 0.3772196769554894,
      "grad_norm": 0.6977880001068115,
      "learning_rate": 4.3712374581939804e-05,
      "loss": 0.9561,
      "step": 2820
    },
    {
      "epoch": 0.3785573353844096,
      "grad_norm": 0.7522315382957458,
      "learning_rate": 4.3690078037904125e-05,
      "loss": 0.9277,
      "step": 2830
    },
    {
      "epoch": 0.37989499381332975,
      "grad_norm": 0.8008530735969543,
      "learning_rate": 4.366778149386845e-05,
      "loss": 0.9491,
      "step": 2840
    },
    {
      "epoch": 0.38123265224224995,
      "grad_norm": 0.6488510966300964,
      "learning_rate": 4.364548494983278e-05,
      "loss": 0.9121,
      "step": 2850
    },
    {
      "epoch": 0.38257031067117014,
      "grad_norm": 0.6622156500816345,
      "learning_rate": 4.362318840579711e-05,
      "loss": 0.9408,
      "step": 2860
    },
    {
      "epoch": 0.3839079691000903,
      "grad_norm": 0.6905619502067566,
      "learning_rate": 4.360089186176143e-05,
      "loss": 0.9441,
      "step": 2870
    },
    {
      "epoch": 0.3852456275290105,
      "grad_norm": 0.768271267414093,
      "learning_rate": 4.3578595317725754e-05,
      "loss": 0.9296,
      "step": 2880
    },
    {
      "epoch": 0.3865832859579306,
      "grad_norm": 0.7056972980499268,
      "learning_rate": 4.355629877369008e-05,
      "loss": 0.9179,
      "step": 2890
    },
    {
      "epoch": 0.3879209443868508,
      "grad_norm": 0.714522123336792,
      "learning_rate": 4.353400222965441e-05,
      "loss": 0.9208,
      "step": 2900
    },
    {
      "epoch": 0.389258602815771,
      "grad_norm": 0.6902288198471069,
      "learning_rate": 4.351170568561873e-05,
      "loss": 0.8712,
      "step": 2910
    },
    {
      "epoch": 0.39059626124469116,
      "grad_norm": 0.7045649886131287,
      "learning_rate": 4.348940914158306e-05,
      "loss": 0.9199,
      "step": 2920
    },
    {
      "epoch": 0.39193391967361135,
      "grad_norm": 0.6869705319404602,
      "learning_rate": 4.3467112597547384e-05,
      "loss": 0.9268,
      "step": 2930
    },
    {
      "epoch": 0.3932715781025315,
      "grad_norm": 0.6798881888389587,
      "learning_rate": 4.3444816053511704e-05,
      "loss": 0.9191,
      "step": 2940
    },
    {
      "epoch": 0.3946092365314517,
      "grad_norm": 0.6788493990898132,
      "learning_rate": 4.342251950947603e-05,
      "loss": 0.8863,
      "step": 2950
    },
    {
      "epoch": 0.3959468949603719,
      "grad_norm": 0.731186032295227,
      "learning_rate": 4.340022296544036e-05,
      "loss": 0.9556,
      "step": 2960
    },
    {
      "epoch": 0.39728455338929203,
      "grad_norm": 0.7174264788627625,
      "learning_rate": 4.3377926421404686e-05,
      "loss": 0.9204,
      "step": 2970
    },
    {
      "epoch": 0.3986222118182122,
      "grad_norm": 0.7310659289360046,
      "learning_rate": 4.335562987736901e-05,
      "loss": 0.9375,
      "step": 2980
    },
    {
      "epoch": 0.3999598702471324,
      "grad_norm": 0.6396946907043457,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.9216,
      "step": 2990
    },
    {
      "epoch": 0.40129752867605256,
      "grad_norm": 0.6764256358146667,
      "learning_rate": 4.331103678929766e-05,
      "loss": 0.932,
      "step": 3000
    },
    {
      "epoch": 0.40263518710497276,
      "grad_norm": 0.6720699071884155,
      "learning_rate": 4.328874024526199e-05,
      "loss": 0.9053,
      "step": 3010
    },
    {
      "epoch": 0.4039728455338929,
      "grad_norm": 0.7778435349464417,
      "learning_rate": 4.326644370122631e-05,
      "loss": 0.8998,
      "step": 3020
    },
    {
      "epoch": 0.4053105039628131,
      "grad_norm": 0.694612443447113,
      "learning_rate": 4.324414715719064e-05,
      "loss": 0.9673,
      "step": 3030
    },
    {
      "epoch": 0.4066481623917333,
      "grad_norm": 0.657470166683197,
      "learning_rate": 4.3221850613154964e-05,
      "loss": 0.9249,
      "step": 3040
    },
    {
      "epoch": 0.40798582082065343,
      "grad_norm": 0.7241469621658325,
      "learning_rate": 4.319955406911929e-05,
      "loss": 0.9406,
      "step": 3050
    },
    {
      "epoch": 0.40932347924957363,
      "grad_norm": 0.7356973886489868,
      "learning_rate": 4.317725752508361e-05,
      "loss": 0.8988,
      "step": 3060
    },
    {
      "epoch": 0.41066113767849377,
      "grad_norm": 0.7466333508491516,
      "learning_rate": 4.315496098104794e-05,
      "loss": 0.9239,
      "step": 3070
    },
    {
      "epoch": 0.41199879610741397,
      "grad_norm": 0.7180359363555908,
      "learning_rate": 4.3132664437012266e-05,
      "loss": 0.9549,
      "step": 3080
    },
    {
      "epoch": 0.41333645453633416,
      "grad_norm": 0.675014317035675,
      "learning_rate": 4.3110367892976586e-05,
      "loss": 0.9175,
      "step": 3090
    },
    {
      "epoch": 0.4146741129652543,
      "grad_norm": 0.7119393944740295,
      "learning_rate": 4.308807134894092e-05,
      "loss": 0.9359,
      "step": 3100
    },
    {
      "epoch": 0.4160117713941745,
      "grad_norm": 0.6713575124740601,
      "learning_rate": 4.306577480490524e-05,
      "loss": 0.9379,
      "step": 3110
    },
    {
      "epoch": 0.4173494298230947,
      "grad_norm": 0.6521581411361694,
      "learning_rate": 4.304347826086957e-05,
      "loss": 0.9111,
      "step": 3120
    },
    {
      "epoch": 0.41868708825201484,
      "grad_norm": 0.7360004782676697,
      "learning_rate": 4.302118171683389e-05,
      "loss": 0.9161,
      "step": 3130
    },
    {
      "epoch": 0.42002474668093503,
      "grad_norm": 0.7228346467018127,
      "learning_rate": 4.299888517279822e-05,
      "loss": 0.9375,
      "step": 3140
    },
    {
      "epoch": 0.4213624051098552,
      "grad_norm": 0.7590742111206055,
      "learning_rate": 4.297658862876254e-05,
      "loss": 0.9312,
      "step": 3150
    },
    {
      "epoch": 0.4227000635387754,
      "grad_norm": 0.7133411169052124,
      "learning_rate": 4.295429208472687e-05,
      "loss": 0.8917,
      "step": 3160
    },
    {
      "epoch": 0.42403772196769557,
      "grad_norm": 0.6825125813484192,
      "learning_rate": 4.29319955406912e-05,
      "loss": 0.9073,
      "step": 3170
    },
    {
      "epoch": 0.4253753803966157,
      "grad_norm": 0.8893772959709167,
      "learning_rate": 4.2909698996655525e-05,
      "loss": 0.9188,
      "step": 3180
    },
    {
      "epoch": 0.4267130388255359,
      "grad_norm": 0.66822749376297,
      "learning_rate": 4.2887402452619846e-05,
      "loss": 0.9395,
      "step": 3190
    },
    {
      "epoch": 0.42805069725445605,
      "grad_norm": 0.7181247472763062,
      "learning_rate": 4.2865105908584166e-05,
      "loss": 0.9247,
      "step": 3200
    },
    {
      "epoch": 0.42938835568337624,
      "grad_norm": 0.7278648614883423,
      "learning_rate": 4.28428093645485e-05,
      "loss": 0.8971,
      "step": 3210
    },
    {
      "epoch": 0.43072601411229644,
      "grad_norm": 0.6808009147644043,
      "learning_rate": 4.282051282051282e-05,
      "loss": 0.9081,
      "step": 3220
    },
    {
      "epoch": 0.4320636725412166,
      "grad_norm": 0.7295394539833069,
      "learning_rate": 4.279821627647715e-05,
      "loss": 0.921,
      "step": 3230
    },
    {
      "epoch": 0.4334013309701368,
      "grad_norm": 0.6944918632507324,
      "learning_rate": 4.2775919732441475e-05,
      "loss": 0.9257,
      "step": 3240
    },
    {
      "epoch": 0.434738989399057,
      "grad_norm": 0.6839542984962463,
      "learning_rate": 4.27536231884058e-05,
      "loss": 0.9214,
      "step": 3250
    },
    {
      "epoch": 0.4360766478279771,
      "grad_norm": 0.6459541916847229,
      "learning_rate": 4.273132664437012e-05,
      "loss": 0.9121,
      "step": 3260
    },
    {
      "epoch": 0.4374143062568973,
      "grad_norm": 0.7060183882713318,
      "learning_rate": 4.270903010033445e-05,
      "loss": 0.9152,
      "step": 3270
    },
    {
      "epoch": 0.43875196468581745,
      "grad_norm": 0.742511510848999,
      "learning_rate": 4.268673355629878e-05,
      "loss": 0.9364,
      "step": 3280
    },
    {
      "epoch": 0.44008962311473765,
      "grad_norm": 0.6794184446334839,
      "learning_rate": 4.2664437012263105e-05,
      "loss": 0.9177,
      "step": 3290
    },
    {
      "epoch": 0.44142728154365785,
      "grad_norm": 0.6788589358329773,
      "learning_rate": 4.2642140468227425e-05,
      "loss": 0.9239,
      "step": 3300
    },
    {
      "epoch": 0.442764939972578,
      "grad_norm": 0.669551432132721,
      "learning_rate": 4.261984392419175e-05,
      "loss": 0.9111,
      "step": 3310
    },
    {
      "epoch": 0.4441025984014982,
      "grad_norm": 0.64271479845047,
      "learning_rate": 4.259754738015608e-05,
      "loss": 0.9008,
      "step": 3320
    },
    {
      "epoch": 0.4454402568304184,
      "grad_norm": 0.7411507964134216,
      "learning_rate": 4.257525083612041e-05,
      "loss": 0.9307,
      "step": 3330
    },
    {
      "epoch": 0.4467779152593385,
      "grad_norm": 0.7144839763641357,
      "learning_rate": 4.255295429208473e-05,
      "loss": 0.8802,
      "step": 3340
    },
    {
      "epoch": 0.4481155736882587,
      "grad_norm": 0.7632840275764465,
      "learning_rate": 4.2530657748049055e-05,
      "loss": 0.9193,
      "step": 3350
    },
    {
      "epoch": 0.44945323211717886,
      "grad_norm": 0.6942856907844543,
      "learning_rate": 4.250836120401338e-05,
      "loss": 0.8876,
      "step": 3360
    },
    {
      "epoch": 0.45079089054609905,
      "grad_norm": 0.6637036800384521,
      "learning_rate": 4.24860646599777e-05,
      "loss": 0.9206,
      "step": 3370
    },
    {
      "epoch": 0.45212854897501925,
      "grad_norm": 0.7504665851593018,
      "learning_rate": 4.246376811594203e-05,
      "loss": 0.9211,
      "step": 3380
    },
    {
      "epoch": 0.4534662074039394,
      "grad_norm": 0.6097080707550049,
      "learning_rate": 4.244147157190636e-05,
      "loss": 0.948,
      "step": 3390
    },
    {
      "epoch": 0.4548038658328596,
      "grad_norm": 0.6277593374252319,
      "learning_rate": 4.2419175027870684e-05,
      "loss": 0.9214,
      "step": 3400
    },
    {
      "epoch": 0.45614152426177973,
      "grad_norm": 0.6903961300849915,
      "learning_rate": 4.2396878483835005e-05,
      "loss": 0.9222,
      "step": 3410
    },
    {
      "epoch": 0.4574791826906999,
      "grad_norm": 0.6806713938713074,
      "learning_rate": 4.237458193979933e-05,
      "loss": 0.916,
      "step": 3420
    },
    {
      "epoch": 0.4588168411196201,
      "grad_norm": 0.7279143929481506,
      "learning_rate": 4.235228539576366e-05,
      "loss": 0.9166,
      "step": 3430
    },
    {
      "epoch": 0.46015449954854026,
      "grad_norm": 0.6467379331588745,
      "learning_rate": 4.232998885172799e-05,
      "loss": 0.9269,
      "step": 3440
    },
    {
      "epoch": 0.46149215797746046,
      "grad_norm": 0.6946624517440796,
      "learning_rate": 4.230769230769231e-05,
      "loss": 0.9081,
      "step": 3450
    },
    {
      "epoch": 0.46282981640638066,
      "grad_norm": 0.7165836095809937,
      "learning_rate": 4.228539576365664e-05,
      "loss": 0.896,
      "step": 3460
    },
    {
      "epoch": 0.4641674748353008,
      "grad_norm": 0.7049257755279541,
      "learning_rate": 4.226309921962096e-05,
      "loss": 0.8999,
      "step": 3470
    },
    {
      "epoch": 0.465505133264221,
      "grad_norm": 0.6338188648223877,
      "learning_rate": 4.224080267558528e-05,
      "loss": 0.9227,
      "step": 3480
    },
    {
      "epoch": 0.46684279169314113,
      "grad_norm": 0.6533752679824829,
      "learning_rate": 4.221850613154961e-05,
      "loss": 0.9158,
      "step": 3490
    },
    {
      "epoch": 0.46818045012206133,
      "grad_norm": 0.7132794857025146,
      "learning_rate": 4.219620958751394e-05,
      "loss": 0.9366,
      "step": 3500
    },
    {
      "epoch": 0.4695181085509815,
      "grad_norm": 0.7524670362472534,
      "learning_rate": 4.2173913043478264e-05,
      "loss": 0.9153,
      "step": 3510
    },
    {
      "epoch": 0.47085576697990167,
      "grad_norm": 0.6910470128059387,
      "learning_rate": 4.2151616499442584e-05,
      "loss": 0.9692,
      "step": 3520
    },
    {
      "epoch": 0.47219342540882187,
      "grad_norm": 0.6973496079444885,
      "learning_rate": 4.212931995540692e-05,
      "loss": 0.8878,
      "step": 3530
    },
    {
      "epoch": 0.473531083837742,
      "grad_norm": 0.7117053270339966,
      "learning_rate": 4.210702341137124e-05,
      "loss": 0.918,
      "step": 3540
    },
    {
      "epoch": 0.4748687422666622,
      "grad_norm": 0.6672117710113525,
      "learning_rate": 4.2084726867335566e-05,
      "loss": 0.9025,
      "step": 3550
    },
    {
      "epoch": 0.4762064006955824,
      "grad_norm": 0.6314782500267029,
      "learning_rate": 4.206243032329989e-05,
      "loss": 0.9173,
      "step": 3560
    },
    {
      "epoch": 0.47754405912450254,
      "grad_norm": 0.7037444710731506,
      "learning_rate": 4.204013377926422e-05,
      "loss": 0.8992,
      "step": 3570
    },
    {
      "epoch": 0.47888171755342274,
      "grad_norm": 0.7119105458259583,
      "learning_rate": 4.201783723522854e-05,
      "loss": 0.8969,
      "step": 3580
    },
    {
      "epoch": 0.48021937598234293,
      "grad_norm": 0.6976519227027893,
      "learning_rate": 4.199554069119287e-05,
      "loss": 0.9253,
      "step": 3590
    },
    {
      "epoch": 0.4815570344112631,
      "grad_norm": 0.637630820274353,
      "learning_rate": 4.1973244147157196e-05,
      "loss": 0.9257,
      "step": 3600
    },
    {
      "epoch": 0.48289469284018327,
      "grad_norm": 0.7054372429847717,
      "learning_rate": 4.195094760312152e-05,
      "loss": 0.8996,
      "step": 3610
    },
    {
      "epoch": 0.4842323512691034,
      "grad_norm": 0.7007036805152893,
      "learning_rate": 4.1928651059085844e-05,
      "loss": 0.9062,
      "step": 3620
    },
    {
      "epoch": 0.4855700096980236,
      "grad_norm": 0.6900847554206848,
      "learning_rate": 4.1906354515050164e-05,
      "loss": 0.9061,
      "step": 3630
    },
    {
      "epoch": 0.4869076681269438,
      "grad_norm": 0.7439354062080383,
      "learning_rate": 4.18840579710145e-05,
      "loss": 0.948,
      "step": 3640
    },
    {
      "epoch": 0.48824532655586395,
      "grad_norm": 0.7316379547119141,
      "learning_rate": 4.186176142697882e-05,
      "loss": 0.9181,
      "step": 3650
    },
    {
      "epoch": 0.48958298498478414,
      "grad_norm": 0.7238817811012268,
      "learning_rate": 4.1839464882943146e-05,
      "loss": 0.9311,
      "step": 3660
    },
    {
      "epoch": 0.4909206434137043,
      "grad_norm": 0.7047203183174133,
      "learning_rate": 4.181716833890747e-05,
      "loss": 0.9211,
      "step": 3670
    },
    {
      "epoch": 0.4922583018426245,
      "grad_norm": 0.734024703502655,
      "learning_rate": 4.17948717948718e-05,
      "loss": 0.9294,
      "step": 3680
    },
    {
      "epoch": 0.4935959602715447,
      "grad_norm": 0.6375654339790344,
      "learning_rate": 4.177257525083612e-05,
      "loss": 0.918,
      "step": 3690
    },
    {
      "epoch": 0.4949336187004648,
      "grad_norm": 0.7758488655090332,
      "learning_rate": 4.175027870680045e-05,
      "loss": 0.9096,
      "step": 3700
    },
    {
      "epoch": 0.496271277129385,
      "grad_norm": 0.6724798083305359,
      "learning_rate": 4.1727982162764775e-05,
      "loss": 0.9164,
      "step": 3710
    },
    {
      "epoch": 0.4976089355583052,
      "grad_norm": 0.6309676170349121,
      "learning_rate": 4.17056856187291e-05,
      "loss": 0.9072,
      "step": 3720
    },
    {
      "epoch": 0.49894659398722535,
      "grad_norm": 0.6852484941482544,
      "learning_rate": 4.168338907469342e-05,
      "loss": 0.9,
      "step": 3730
    },
    {
      "epoch": 0.5002842524161455,
      "grad_norm": 0.688459038734436,
      "learning_rate": 4.166109253065775e-05,
      "loss": 0.9278,
      "step": 3740
    },
    {
      "epoch": 0.5016219108450657,
      "grad_norm": 0.710491418838501,
      "learning_rate": 4.163879598662208e-05,
      "loss": 0.8883,
      "step": 3750
    },
    {
      "epoch": 0.5029595692739859,
      "grad_norm": 0.8287901878356934,
      "learning_rate": 4.16164994425864e-05,
      "loss": 0.9068,
      "step": 3760
    },
    {
      "epoch": 0.5042972277029061,
      "grad_norm": 0.6635586023330688,
      "learning_rate": 4.1594202898550726e-05,
      "loss": 0.9118,
      "step": 3770
    },
    {
      "epoch": 0.5056348861318263,
      "grad_norm": 0.6836486458778381,
      "learning_rate": 4.157190635451505e-05,
      "loss": 0.9187,
      "step": 3780
    },
    {
      "epoch": 0.5069725445607464,
      "grad_norm": 0.6855483651161194,
      "learning_rate": 4.154960981047938e-05,
      "loss": 0.9033,
      "step": 3790
    },
    {
      "epoch": 0.5083102029896666,
      "grad_norm": 0.7066940069198608,
      "learning_rate": 4.15273132664437e-05,
      "loss": 0.9528,
      "step": 3800
    },
    {
      "epoch": 0.5096478614185868,
      "grad_norm": 0.6349518895149231,
      "learning_rate": 4.150501672240803e-05,
      "loss": 0.9214,
      "step": 3810
    },
    {
      "epoch": 0.510985519847507,
      "grad_norm": 0.6912996768951416,
      "learning_rate": 4.1482720178372355e-05,
      "loss": 0.8842,
      "step": 3820
    },
    {
      "epoch": 0.5123231782764271,
      "grad_norm": 0.7150052189826965,
      "learning_rate": 4.146042363433668e-05,
      "loss": 0.9103,
      "step": 3830
    },
    {
      "epoch": 0.5136608367053472,
      "grad_norm": 0.703231692314148,
      "learning_rate": 4.1438127090301e-05,
      "loss": 0.9046,
      "step": 3840
    },
    {
      "epoch": 0.5149984951342674,
      "grad_norm": 0.6537412405014038,
      "learning_rate": 4.141583054626533e-05,
      "loss": 0.9091,
      "step": 3850
    },
    {
      "epoch": 0.5163361535631876,
      "grad_norm": 0.668007493019104,
      "learning_rate": 4.139353400222966e-05,
      "loss": 0.9169,
      "step": 3860
    },
    {
      "epoch": 0.5176738119921078,
      "grad_norm": 0.6316366195678711,
      "learning_rate": 4.1371237458193985e-05,
      "loss": 0.9101,
      "step": 3870
    },
    {
      "epoch": 0.519011470421028,
      "grad_norm": 0.7317895889282227,
      "learning_rate": 4.1348940914158305e-05,
      "loss": 0.9123,
      "step": 3880
    },
    {
      "epoch": 0.5203491288499482,
      "grad_norm": 0.6658817529678345,
      "learning_rate": 4.132664437012263e-05,
      "loss": 0.9172,
      "step": 3890
    },
    {
      "epoch": 0.5216867872788683,
      "grad_norm": 0.686253547668457,
      "learning_rate": 4.130434782608696e-05,
      "loss": 0.87,
      "step": 3900
    },
    {
      "epoch": 0.5230244457077885,
      "grad_norm": 0.8565061092376709,
      "learning_rate": 4.128205128205128e-05,
      "loss": 0.9177,
      "step": 3910
    },
    {
      "epoch": 0.5243621041367087,
      "grad_norm": 0.6403840184211731,
      "learning_rate": 4.125975473801561e-05,
      "loss": 0.9134,
      "step": 3920
    },
    {
      "epoch": 0.5256997625656289,
      "grad_norm": 0.6641960144042969,
      "learning_rate": 4.1237458193979935e-05,
      "loss": 0.9226,
      "step": 3930
    },
    {
      "epoch": 0.5270374209945491,
      "grad_norm": 0.6473814249038696,
      "learning_rate": 4.121516164994426e-05,
      "loss": 0.9262,
      "step": 3940
    },
    {
      "epoch": 0.5283750794234692,
      "grad_norm": 0.6994156837463379,
      "learning_rate": 4.119286510590858e-05,
      "loss": 0.8914,
      "step": 3950
    },
    {
      "epoch": 0.5297127378523894,
      "grad_norm": 0.6494188904762268,
      "learning_rate": 4.117056856187291e-05,
      "loss": 0.9232,
      "step": 3960
    },
    {
      "epoch": 0.5310503962813096,
      "grad_norm": 0.671118974685669,
      "learning_rate": 4.114827201783724e-05,
      "loss": 0.8857,
      "step": 3970
    },
    {
      "epoch": 0.5323880547102298,
      "grad_norm": 0.6817793250083923,
      "learning_rate": 4.1125975473801564e-05,
      "loss": 0.9051,
      "step": 3980
    },
    {
      "epoch": 0.53372571313915,
      "grad_norm": 0.6769902110099792,
      "learning_rate": 4.1103678929765885e-05,
      "loss": 0.9095,
      "step": 3990
    },
    {
      "epoch": 0.53506337156807,
      "grad_norm": 0.6037312746047974,
      "learning_rate": 4.108138238573022e-05,
      "loss": 0.9337,
      "step": 4000
    },
    {
      "epoch": 0.5364010299969902,
      "grad_norm": 0.6903241872787476,
      "learning_rate": 4.105908584169454e-05,
      "loss": 0.8916,
      "step": 4010
    },
    {
      "epoch": 0.5377386884259104,
      "grad_norm": 0.7317522764205933,
      "learning_rate": 4.1036789297658867e-05,
      "loss": 0.912,
      "step": 4020
    },
    {
      "epoch": 0.5390763468548306,
      "grad_norm": 0.7093622088432312,
      "learning_rate": 4.101449275362319e-05,
      "loss": 0.9157,
      "step": 4030
    },
    {
      "epoch": 0.5404140052837508,
      "grad_norm": 0.6457343697547913,
      "learning_rate": 4.0992196209587514e-05,
      "loss": 0.9057,
      "step": 4040
    },
    {
      "epoch": 0.5417516637126709,
      "grad_norm": 0.6917157173156738,
      "learning_rate": 4.096989966555184e-05,
      "loss": 0.9374,
      "step": 4050
    },
    {
      "epoch": 0.5430893221415911,
      "grad_norm": 0.6593833565711975,
      "learning_rate": 4.094760312151616e-05,
      "loss": 0.8982,
      "step": 4060
    },
    {
      "epoch": 0.5444269805705113,
      "grad_norm": 0.6696697473526001,
      "learning_rate": 4.0925306577480496e-05,
      "loss": 0.9094,
      "step": 4070
    },
    {
      "epoch": 0.5457646389994315,
      "grad_norm": 0.6765269637107849,
      "learning_rate": 4.090301003344482e-05,
      "loss": 0.9346,
      "step": 4080
    },
    {
      "epoch": 0.5471022974283517,
      "grad_norm": 0.6685237288475037,
      "learning_rate": 4.0880713489409144e-05,
      "loss": 0.9232,
      "step": 4090
    },
    {
      "epoch": 0.5484399558572719,
      "grad_norm": 0.693001925945282,
      "learning_rate": 4.0858416945373464e-05,
      "loss": 0.9026,
      "step": 4100
    },
    {
      "epoch": 0.549777614286192,
      "grad_norm": 0.6626124978065491,
      "learning_rate": 4.08361204013378e-05,
      "loss": 0.9126,
      "step": 4110
    },
    {
      "epoch": 0.5511152727151122,
      "grad_norm": 0.7262780070304871,
      "learning_rate": 4.081382385730212e-05,
      "loss": 0.9326,
      "step": 4120
    },
    {
      "epoch": 0.5524529311440324,
      "grad_norm": 0.6464977860450745,
      "learning_rate": 4.0791527313266446e-05,
      "loss": 0.9192,
      "step": 4130
    },
    {
      "epoch": 0.5537905895729526,
      "grad_norm": 0.6616817712783813,
      "learning_rate": 4.0769230769230773e-05,
      "loss": 0.9354,
      "step": 4140
    },
    {
      "epoch": 0.5551282480018728,
      "grad_norm": 0.6446918249130249,
      "learning_rate": 4.07469342251951e-05,
      "loss": 0.9044,
      "step": 4150
    },
    {
      "epoch": 0.5564659064307929,
      "grad_norm": 0.6652436256408691,
      "learning_rate": 4.072463768115942e-05,
      "loss": 0.9228,
      "step": 4160
    },
    {
      "epoch": 0.557803564859713,
      "grad_norm": 0.6319944262504578,
      "learning_rate": 4.070234113712374e-05,
      "loss": 0.9155,
      "step": 4170
    },
    {
      "epoch": 0.5591412232886332,
      "grad_norm": 0.6700304746627808,
      "learning_rate": 4.0680044593088076e-05,
      "loss": 0.8781,
      "step": 4180
    },
    {
      "epoch": 0.5604788817175534,
      "grad_norm": 0.6567128896713257,
      "learning_rate": 4.0657748049052396e-05,
      "loss": 0.9333,
      "step": 4190
    },
    {
      "epoch": 0.5618165401464736,
      "grad_norm": 0.9191807508468628,
      "learning_rate": 4.0635451505016724e-05,
      "loss": 0.9222,
      "step": 4200
    },
    {
      "epoch": 0.5631541985753937,
      "grad_norm": 0.6489996314048767,
      "learning_rate": 4.061315496098105e-05,
      "loss": 0.9061,
      "step": 4210
    },
    {
      "epoch": 0.5644918570043139,
      "grad_norm": 0.6357506513595581,
      "learning_rate": 4.059085841694538e-05,
      "loss": 0.9324,
      "step": 4220
    },
    {
      "epoch": 0.5658295154332341,
      "grad_norm": 0.7119045853614807,
      "learning_rate": 4.05685618729097e-05,
      "loss": 0.9122,
      "step": 4230
    },
    {
      "epoch": 0.5671671738621543,
      "grad_norm": 0.654912531375885,
      "learning_rate": 4.0546265328874026e-05,
      "loss": 0.9132,
      "step": 4240
    },
    {
      "epoch": 0.5685048322910745,
      "grad_norm": 0.6580280661582947,
      "learning_rate": 4.052396878483835e-05,
      "loss": 0.9006,
      "step": 4250
    },
    {
      "epoch": 0.5698424907199946,
      "grad_norm": 0.7446218132972717,
      "learning_rate": 4.050167224080268e-05,
      "loss": 0.9057,
      "step": 4260
    },
    {
      "epoch": 0.5711801491489148,
      "grad_norm": 0.728739857673645,
      "learning_rate": 4.0479375696767e-05,
      "loss": 0.9391,
      "step": 4270
    },
    {
      "epoch": 0.572517807577835,
      "grad_norm": 0.6382721662521362,
      "learning_rate": 4.045707915273133e-05,
      "loss": 0.9175,
      "step": 4280
    },
    {
      "epoch": 0.5738554660067552,
      "grad_norm": 0.674655020236969,
      "learning_rate": 4.0434782608695655e-05,
      "loss": 0.9241,
      "step": 4290
    },
    {
      "epoch": 0.5751931244356754,
      "grad_norm": 0.6868028044700623,
      "learning_rate": 4.041248606465998e-05,
      "loss": 0.9077,
      "step": 4300
    },
    {
      "epoch": 0.5765307828645955,
      "grad_norm": 0.7850871086120605,
      "learning_rate": 4.03901895206243e-05,
      "loss": 0.9067,
      "step": 4310
    },
    {
      "epoch": 0.5778684412935157,
      "grad_norm": 0.6893448829650879,
      "learning_rate": 4.036789297658863e-05,
      "loss": 0.8933,
      "step": 4320
    },
    {
      "epoch": 0.5792060997224359,
      "grad_norm": 0.6692866086959839,
      "learning_rate": 4.034559643255296e-05,
      "loss": 0.8895,
      "step": 4330
    },
    {
      "epoch": 0.5805437581513561,
      "grad_norm": 0.6904217600822449,
      "learning_rate": 4.032329988851728e-05,
      "loss": 0.9453,
      "step": 4340
    },
    {
      "epoch": 0.5818814165802763,
      "grad_norm": 0.6702015995979309,
      "learning_rate": 4.0301003344481605e-05,
      "loss": 0.9094,
      "step": 4350
    },
    {
      "epoch": 0.5832190750091965,
      "grad_norm": 0.6921604871749878,
      "learning_rate": 4.027870680044593e-05,
      "loss": 0.909,
      "step": 4360
    },
    {
      "epoch": 0.5845567334381165,
      "grad_norm": 0.670738697052002,
      "learning_rate": 4.025641025641026e-05,
      "loss": 0.8924,
      "step": 4370
    },
    {
      "epoch": 0.5858943918670367,
      "grad_norm": 0.7377943992614746,
      "learning_rate": 4.023411371237458e-05,
      "loss": 0.9233,
      "step": 4380
    },
    {
      "epoch": 0.5872320502959569,
      "grad_norm": 0.6923239827156067,
      "learning_rate": 4.021181716833891e-05,
      "loss": 0.8904,
      "step": 4390
    },
    {
      "epoch": 0.5885697087248771,
      "grad_norm": 0.6433722972869873,
      "learning_rate": 4.0189520624303235e-05,
      "loss": 0.9103,
      "step": 4400
    },
    {
      "epoch": 0.5899073671537973,
      "grad_norm": 0.6892331838607788,
      "learning_rate": 4.016722408026756e-05,
      "loss": 0.899,
      "step": 4410
    },
    {
      "epoch": 0.5912450255827174,
      "grad_norm": 0.6700544953346252,
      "learning_rate": 4.014492753623188e-05,
      "loss": 0.9232,
      "step": 4420
    },
    {
      "epoch": 0.5925826840116376,
      "grad_norm": 0.7042912244796753,
      "learning_rate": 4.012263099219622e-05,
      "loss": 0.9096,
      "step": 4430
    },
    {
      "epoch": 0.5939203424405578,
      "grad_norm": 0.6970276832580566,
      "learning_rate": 4.010033444816054e-05,
      "loss": 0.9039,
      "step": 4440
    },
    {
      "epoch": 0.595258000869478,
      "grad_norm": 0.6861774921417236,
      "learning_rate": 4.007803790412486e-05,
      "loss": 0.9376,
      "step": 4450
    },
    {
      "epoch": 0.5965956592983982,
      "grad_norm": 0.6317903995513916,
      "learning_rate": 4.0055741360089185e-05,
      "loss": 0.9143,
      "step": 4460
    },
    {
      "epoch": 0.5979333177273183,
      "grad_norm": 0.7054911851882935,
      "learning_rate": 4.003344481605351e-05,
      "loss": 0.8911,
      "step": 4470
    },
    {
      "epoch": 0.5992709761562385,
      "grad_norm": 0.6267910599708557,
      "learning_rate": 4.001114827201784e-05,
      "loss": 0.9212,
      "step": 4480
    },
    {
      "epoch": 0.6006086345851587,
      "grad_norm": 0.625537097454071,
      "learning_rate": 3.998885172798216e-05,
      "loss": 0.886,
      "step": 4490
    },
    {
      "epoch": 0.6019462930140789,
      "grad_norm": 0.7217710018157959,
      "learning_rate": 3.9966555183946494e-05,
      "loss": 0.9194,
      "step": 4500
    },
    {
      "epoch": 0.6032839514429991,
      "grad_norm": 0.662623405456543,
      "learning_rate": 3.9944258639910815e-05,
      "loss": 0.8882,
      "step": 4510
    },
    {
      "epoch": 0.6046216098719192,
      "grad_norm": 0.7248339653015137,
      "learning_rate": 3.992196209587514e-05,
      "loss": 0.8985,
      "step": 4520
    },
    {
      "epoch": 0.6059592683008393,
      "grad_norm": 0.6956506967544556,
      "learning_rate": 3.989966555183946e-05,
      "loss": 0.8949,
      "step": 4530
    },
    {
      "epoch": 0.6072969267297595,
      "grad_norm": 0.6612618565559387,
      "learning_rate": 3.9877369007803796e-05,
      "loss": 0.89,
      "step": 4540
    },
    {
      "epoch": 0.6086345851586797,
      "grad_norm": 0.6448971629142761,
      "learning_rate": 3.985507246376812e-05,
      "loss": 0.9217,
      "step": 4550
    },
    {
      "epoch": 0.6099722435875999,
      "grad_norm": 0.6479575634002686,
      "learning_rate": 3.9832775919732444e-05,
      "loss": 0.8886,
      "step": 4560
    },
    {
      "epoch": 0.6113099020165201,
      "grad_norm": 0.6247550249099731,
      "learning_rate": 3.981047937569677e-05,
      "loss": 0.9259,
      "step": 4570
    },
    {
      "epoch": 0.6126475604454402,
      "grad_norm": 0.7211959362030029,
      "learning_rate": 3.97881828316611e-05,
      "loss": 0.9134,
      "step": 4580
    },
    {
      "epoch": 0.6139852188743604,
      "grad_norm": 0.626655101776123,
      "learning_rate": 3.976588628762542e-05,
      "loss": 0.8989,
      "step": 4590
    },
    {
      "epoch": 0.6153228773032806,
      "grad_norm": 0.6858691573143005,
      "learning_rate": 3.974358974358974e-05,
      "loss": 0.9017,
      "step": 4600
    },
    {
      "epoch": 0.6166605357322008,
      "grad_norm": 0.6690014600753784,
      "learning_rate": 3.9721293199554074e-05,
      "loss": 0.9253,
      "step": 4610
    },
    {
      "epoch": 0.617998194161121,
      "grad_norm": 0.713955283164978,
      "learning_rate": 3.9698996655518394e-05,
      "loss": 0.9002,
      "step": 4620
    },
    {
      "epoch": 0.6193358525900411,
      "grad_norm": 0.6253163814544678,
      "learning_rate": 3.967670011148272e-05,
      "loss": 0.9349,
      "step": 4630
    },
    {
      "epoch": 0.6206735110189613,
      "grad_norm": 0.7428447604179382,
      "learning_rate": 3.965440356744705e-05,
      "loss": 0.8977,
      "step": 4640
    },
    {
      "epoch": 0.6220111694478815,
      "grad_norm": 0.6684670448303223,
      "learning_rate": 3.9632107023411376e-05,
      "loss": 0.9165,
      "step": 4650
    },
    {
      "epoch": 0.6233488278768017,
      "grad_norm": 0.616703450679779,
      "learning_rate": 3.9609810479375697e-05,
      "loss": 0.9096,
      "step": 4660
    },
    {
      "epoch": 0.6246864863057219,
      "grad_norm": 0.6283993124961853,
      "learning_rate": 3.9587513935340024e-05,
      "loss": 0.8938,
      "step": 4670
    },
    {
      "epoch": 0.626024144734642,
      "grad_norm": 0.7047260999679565,
      "learning_rate": 3.956521739130435e-05,
      "loss": 0.9219,
      "step": 4680
    },
    {
      "epoch": 0.6273618031635622,
      "grad_norm": 0.7035847902297974,
      "learning_rate": 3.954292084726868e-05,
      "loss": 0.9199,
      "step": 4690
    },
    {
      "epoch": 0.6286994615924824,
      "grad_norm": 0.6715951561927795,
      "learning_rate": 3.9520624303233e-05,
      "loss": 0.9129,
      "step": 4700
    },
    {
      "epoch": 0.6300371200214026,
      "grad_norm": 0.6670560240745544,
      "learning_rate": 3.9498327759197326e-05,
      "loss": 0.9006,
      "step": 4710
    },
    {
      "epoch": 0.6313747784503227,
      "grad_norm": 0.6535661220550537,
      "learning_rate": 3.9476031215161653e-05,
      "loss": 0.8959,
      "step": 4720
    },
    {
      "epoch": 0.6327124368792428,
      "grad_norm": 0.6203116178512573,
      "learning_rate": 3.9453734671125974e-05,
      "loss": 0.8982,
      "step": 4730
    },
    {
      "epoch": 0.634050095308163,
      "grad_norm": 0.635985791683197,
      "learning_rate": 3.94314381270903e-05,
      "loss": 0.899,
      "step": 4740
    },
    {
      "epoch": 0.6353877537370832,
      "grad_norm": 0.5757884383201599,
      "learning_rate": 3.940914158305463e-05,
      "loss": 0.9021,
      "step": 4750
    },
    {
      "epoch": 0.6367254121660034,
      "grad_norm": 0.6958448886871338,
      "learning_rate": 3.9386845039018956e-05,
      "loss": 0.9169,
      "step": 4760
    },
    {
      "epoch": 0.6380630705949236,
      "grad_norm": 0.6415709853172302,
      "learning_rate": 3.9364548494983276e-05,
      "loss": 0.9065,
      "step": 4770
    },
    {
      "epoch": 0.6394007290238438,
      "grad_norm": 0.7154775261878967,
      "learning_rate": 3.9342251950947603e-05,
      "loss": 0.9273,
      "step": 4780
    },
    {
      "epoch": 0.6407383874527639,
      "grad_norm": 0.7338489294052124,
      "learning_rate": 3.931995540691193e-05,
      "loss": 0.9275,
      "step": 4790
    },
    {
      "epoch": 0.6420760458816841,
      "grad_norm": 0.7286524176597595,
      "learning_rate": 3.929765886287626e-05,
      "loss": 0.9173,
      "step": 4800
    },
    {
      "epoch": 0.6434137043106043,
      "grad_norm": 0.7027987241744995,
      "learning_rate": 3.927536231884058e-05,
      "loss": 0.9271,
      "step": 4810
    },
    {
      "epoch": 0.6447513627395245,
      "grad_norm": 0.6168084144592285,
      "learning_rate": 3.9253065774804906e-05,
      "loss": 0.8945,
      "step": 4820
    },
    {
      "epoch": 0.6460890211684447,
      "grad_norm": 0.6476319432258606,
      "learning_rate": 3.923076923076923e-05,
      "loss": 0.9224,
      "step": 4830
    },
    {
      "epoch": 0.6474266795973648,
      "grad_norm": 0.6518853902816772,
      "learning_rate": 3.920847268673356e-05,
      "loss": 0.9039,
      "step": 4840
    },
    {
      "epoch": 0.648764338026285,
      "grad_norm": 0.6475685238838196,
      "learning_rate": 3.918617614269788e-05,
      "loss": 0.8979,
      "step": 4850
    },
    {
      "epoch": 0.6501019964552052,
      "grad_norm": 0.6593098640441895,
      "learning_rate": 3.916387959866221e-05,
      "loss": 0.9264,
      "step": 4860
    },
    {
      "epoch": 0.6514396548841254,
      "grad_norm": 0.7363295555114746,
      "learning_rate": 3.9141583054626535e-05,
      "loss": 0.9112,
      "step": 4870
    },
    {
      "epoch": 0.6527773133130456,
      "grad_norm": 0.7064644694328308,
      "learning_rate": 3.9119286510590856e-05,
      "loss": 0.9147,
      "step": 4880
    },
    {
      "epoch": 0.6541149717419656,
      "grad_norm": 0.630223274230957,
      "learning_rate": 3.909698996655518e-05,
      "loss": 0.8895,
      "step": 4890
    },
    {
      "epoch": 0.6554526301708858,
      "grad_norm": 0.7054390907287598,
      "learning_rate": 3.907469342251951e-05,
      "loss": 0.9103,
      "step": 4900
    },
    {
      "epoch": 0.656790288599806,
      "grad_norm": 0.6785403490066528,
      "learning_rate": 3.905239687848384e-05,
      "loss": 0.9062,
      "step": 4910
    },
    {
      "epoch": 0.6581279470287262,
      "grad_norm": 0.6198427677154541,
      "learning_rate": 3.903010033444816e-05,
      "loss": 0.9026,
      "step": 4920
    },
    {
      "epoch": 0.6594656054576464,
      "grad_norm": 0.6359544992446899,
      "learning_rate": 3.900780379041249e-05,
      "loss": 0.9264,
      "step": 4930
    },
    {
      "epoch": 0.6608032638865665,
      "grad_norm": 0.6371769905090332,
      "learning_rate": 3.898550724637681e-05,
      "loss": 0.8868,
      "step": 4940
    },
    {
      "epoch": 0.6621409223154867,
      "grad_norm": 0.6787527799606323,
      "learning_rate": 3.896321070234114e-05,
      "loss": 0.9199,
      "step": 4950
    },
    {
      "epoch": 0.6634785807444069,
      "grad_norm": 0.6503549218177795,
      "learning_rate": 3.894091415830546e-05,
      "loss": 0.9164,
      "step": 4960
    },
    {
      "epoch": 0.6648162391733271,
      "grad_norm": 0.672285258769989,
      "learning_rate": 3.8918617614269795e-05,
      "loss": 0.9293,
      "step": 4970
    },
    {
      "epoch": 0.6661538976022473,
      "grad_norm": 0.6838586926460266,
      "learning_rate": 3.8896321070234115e-05,
      "loss": 0.8986,
      "step": 4980
    },
    {
      "epoch": 0.6674915560311674,
      "grad_norm": 0.6317412853240967,
      "learning_rate": 3.887402452619844e-05,
      "loss": 0.9344,
      "step": 4990
    },
    {
      "epoch": 0.6688292144600876,
      "grad_norm": 0.7709919810295105,
      "learning_rate": 3.885172798216277e-05,
      "loss": 0.9276,
      "step": 5000
    },
    {
      "epoch": 0.6701668728890078,
      "grad_norm": 0.6800752282142639,
      "learning_rate": 3.882943143812709e-05,
      "loss": 0.9328,
      "step": 5010
    },
    {
      "epoch": 0.671504531317928,
      "grad_norm": 0.656464159488678,
      "learning_rate": 3.880713489409142e-05,
      "loss": 0.9005,
      "step": 5020
    },
    {
      "epoch": 0.6728421897468482,
      "grad_norm": 0.6395153403282166,
      "learning_rate": 3.878483835005574e-05,
      "loss": 0.9171,
      "step": 5030
    },
    {
      "epoch": 0.6741798481757684,
      "grad_norm": 0.6772337555885315,
      "learning_rate": 3.876254180602007e-05,
      "loss": 0.9015,
      "step": 5040
    },
    {
      "epoch": 0.6755175066046885,
      "grad_norm": 0.6009427309036255,
      "learning_rate": 3.874024526198439e-05,
      "loss": 0.8981,
      "step": 5050
    },
    {
      "epoch": 0.6768551650336087,
      "grad_norm": 0.6960785984992981,
      "learning_rate": 3.871794871794872e-05,
      "loss": 0.8976,
      "step": 5060
    },
    {
      "epoch": 0.6781928234625288,
      "grad_norm": 0.6226920485496521,
      "learning_rate": 3.869565217391305e-05,
      "loss": 0.9151,
      "step": 5070
    },
    {
      "epoch": 0.679530481891449,
      "grad_norm": 0.612459659576416,
      "learning_rate": 3.8673355629877374e-05,
      "loss": 0.9195,
      "step": 5080
    },
    {
      "epoch": 0.6808681403203692,
      "grad_norm": 0.6285151839256287,
      "learning_rate": 3.8651059085841695e-05,
      "loss": 0.9202,
      "step": 5090
    },
    {
      "epoch": 0.6822057987492893,
      "grad_norm": 0.6683539748191833,
      "learning_rate": 3.862876254180602e-05,
      "loss": 0.9182,
      "step": 5100
    },
    {
      "epoch": 0.6835434571782095,
      "grad_norm": 0.7249630689620972,
      "learning_rate": 3.860646599777035e-05,
      "loss": 0.904,
      "step": 5110
    },
    {
      "epoch": 0.6848811156071297,
      "grad_norm": 0.6540199518203735,
      "learning_rate": 3.8584169453734676e-05,
      "loss": 0.9103,
      "step": 5120
    },
    {
      "epoch": 0.6862187740360499,
      "grad_norm": 0.6382514834403992,
      "learning_rate": 3.8561872909699e-05,
      "loss": 0.9081,
      "step": 5130
    },
    {
      "epoch": 0.6875564324649701,
      "grad_norm": 0.5988933444023132,
      "learning_rate": 3.8539576365663324e-05,
      "loss": 0.9368,
      "step": 5140
    },
    {
      "epoch": 0.6888940908938902,
      "grad_norm": 0.6422765851020813,
      "learning_rate": 3.851727982162765e-05,
      "loss": 0.9194,
      "step": 5150
    },
    {
      "epoch": 0.6902317493228104,
      "grad_norm": 0.5835447311401367,
      "learning_rate": 3.849498327759197e-05,
      "loss": 0.931,
      "step": 5160
    },
    {
      "epoch": 0.6915694077517306,
      "grad_norm": 0.686157763004303,
      "learning_rate": 3.84726867335563e-05,
      "loss": 0.9178,
      "step": 5170
    },
    {
      "epoch": 0.6929070661806508,
      "grad_norm": 0.6781975626945496,
      "learning_rate": 3.8450390189520626e-05,
      "loss": 0.9254,
      "step": 5180
    },
    {
      "epoch": 0.694244724609571,
      "grad_norm": 0.6930925250053406,
      "learning_rate": 3.8428093645484954e-05,
      "loss": 0.8983,
      "step": 5190
    },
    {
      "epoch": 0.6955823830384911,
      "grad_norm": 0.6619436144828796,
      "learning_rate": 3.8405797101449274e-05,
      "loss": 0.8963,
      "step": 5200
    },
    {
      "epoch": 0.6969200414674113,
      "grad_norm": 0.6507082581520081,
      "learning_rate": 3.83835005574136e-05,
      "loss": 0.8911,
      "step": 5210
    },
    {
      "epoch": 0.6982576998963315,
      "grad_norm": 0.6434870958328247,
      "learning_rate": 3.836120401337793e-05,
      "loss": 0.9003,
      "step": 5220
    },
    {
      "epoch": 0.6995953583252517,
      "grad_norm": 0.6620796918869019,
      "learning_rate": 3.8338907469342256e-05,
      "loss": 0.9396,
      "step": 5230
    },
    {
      "epoch": 0.7009330167541719,
      "grad_norm": 0.612608015537262,
      "learning_rate": 3.8316610925306577e-05,
      "loss": 0.9004,
      "step": 5240
    },
    {
      "epoch": 0.702270675183092,
      "grad_norm": 0.6270439028739929,
      "learning_rate": 3.8294314381270904e-05,
      "loss": 0.8717,
      "step": 5250
    },
    {
      "epoch": 0.7036083336120121,
      "grad_norm": 0.629567563533783,
      "learning_rate": 3.827201783723523e-05,
      "loss": 0.8858,
      "step": 5260
    },
    {
      "epoch": 0.7049459920409323,
      "grad_norm": 0.6527901291847229,
      "learning_rate": 3.824972129319956e-05,
      "loss": 0.8754,
      "step": 5270
    },
    {
      "epoch": 0.7062836504698525,
      "grad_norm": 0.7070116996765137,
      "learning_rate": 3.822742474916388e-05,
      "loss": 0.9263,
      "step": 5280
    },
    {
      "epoch": 0.7076213088987727,
      "grad_norm": 0.6328373551368713,
      "learning_rate": 3.8205128205128206e-05,
      "loss": 0.9176,
      "step": 5290
    },
    {
      "epoch": 0.7089589673276929,
      "grad_norm": 0.6306329369544983,
      "learning_rate": 3.8182831661092533e-05,
      "loss": 0.9415,
      "step": 5300
    },
    {
      "epoch": 0.710296625756613,
      "grad_norm": 0.6214905381202698,
      "learning_rate": 3.8160535117056854e-05,
      "loss": 0.9093,
      "step": 5310
    },
    {
      "epoch": 0.7116342841855332,
      "grad_norm": 0.6755281686782837,
      "learning_rate": 3.813823857302118e-05,
      "loss": 0.8986,
      "step": 5320
    },
    {
      "epoch": 0.7129719426144534,
      "grad_norm": 0.6485979557037354,
      "learning_rate": 3.811594202898551e-05,
      "loss": 0.9122,
      "step": 5330
    },
    {
      "epoch": 0.7143096010433736,
      "grad_norm": 0.7262750864028931,
      "learning_rate": 3.8093645484949836e-05,
      "loss": 0.9466,
      "step": 5340
    },
    {
      "epoch": 0.7156472594722938,
      "grad_norm": 0.6483548879623413,
      "learning_rate": 3.8071348940914156e-05,
      "loss": 0.8994,
      "step": 5350
    },
    {
      "epoch": 0.7169849179012139,
      "grad_norm": 0.6043847799301147,
      "learning_rate": 3.804905239687849e-05,
      "loss": 0.9072,
      "step": 5360
    },
    {
      "epoch": 0.7183225763301341,
      "grad_norm": 0.675695538520813,
      "learning_rate": 3.802675585284281e-05,
      "loss": 0.9182,
      "step": 5370
    },
    {
      "epoch": 0.7196602347590543,
      "grad_norm": 0.6678072810173035,
      "learning_rate": 3.800445930880714e-05,
      "loss": 0.9363,
      "step": 5380
    },
    {
      "epoch": 0.7209978931879745,
      "grad_norm": 0.7015389204025269,
      "learning_rate": 3.798216276477146e-05,
      "loss": 0.9443,
      "step": 5390
    },
    {
      "epoch": 0.7223355516168947,
      "grad_norm": 0.6127780079841614,
      "learning_rate": 3.795986622073579e-05,
      "loss": 0.9248,
      "step": 5400
    },
    {
      "epoch": 0.7236732100458148,
      "grad_norm": 0.6483640074729919,
      "learning_rate": 3.793756967670011e-05,
      "loss": 0.9132,
      "step": 5410
    },
    {
      "epoch": 0.725010868474735,
      "grad_norm": 0.5997834801673889,
      "learning_rate": 3.7915273132664434e-05,
      "loss": 0.9044,
      "step": 5420
    },
    {
      "epoch": 0.7263485269036551,
      "grad_norm": 0.5938931703567505,
      "learning_rate": 3.789297658862877e-05,
      "loss": 0.9214,
      "step": 5430
    },
    {
      "epoch": 0.7276861853325753,
      "grad_norm": 0.6312838792800903,
      "learning_rate": 3.787068004459309e-05,
      "loss": 0.8693,
      "step": 5440
    },
    {
      "epoch": 0.7290238437614955,
      "grad_norm": 0.5815171003341675,
      "learning_rate": 3.7848383500557415e-05,
      "loss": 0.8998,
      "step": 5450
    },
    {
      "epoch": 0.7303615021904156,
      "grad_norm": 0.6739098429679871,
      "learning_rate": 3.7826086956521736e-05,
      "loss": 0.9025,
      "step": 5460
    },
    {
      "epoch": 0.7316991606193358,
      "grad_norm": 0.6791756749153137,
      "learning_rate": 3.780379041248607e-05,
      "loss": 0.9117,
      "step": 5470
    },
    {
      "epoch": 0.733036819048256,
      "grad_norm": 0.6285737752914429,
      "learning_rate": 3.778149386845039e-05,
      "loss": 0.9271,
      "step": 5480
    },
    {
      "epoch": 0.7343744774771762,
      "grad_norm": 0.717247486114502,
      "learning_rate": 3.775919732441472e-05,
      "loss": 0.9258,
      "step": 5490
    },
    {
      "epoch": 0.7357121359060964,
      "grad_norm": 0.6628655791282654,
      "learning_rate": 3.7736900780379045e-05,
      "loss": 0.9264,
      "step": 5500
    },
    {
      "epoch": 0.7370497943350166,
      "grad_norm": 0.6276170015335083,
      "learning_rate": 3.771460423634337e-05,
      "loss": 0.9133,
      "step": 5510
    },
    {
      "epoch": 0.7383874527639367,
      "grad_norm": 0.6360505223274231,
      "learning_rate": 3.769230769230769e-05,
      "loss": 0.8926,
      "step": 5520
    },
    {
      "epoch": 0.7397251111928569,
      "grad_norm": 0.5976069569587708,
      "learning_rate": 3.767001114827202e-05,
      "loss": 0.9165,
      "step": 5530
    },
    {
      "epoch": 0.7410627696217771,
      "grad_norm": 0.6341137290000916,
      "learning_rate": 3.764771460423635e-05,
      "loss": 0.8933,
      "step": 5540
    },
    {
      "epoch": 0.7424004280506973,
      "grad_norm": 0.6469510197639465,
      "learning_rate": 3.7625418060200674e-05,
      "loss": 0.9008,
      "step": 5550
    },
    {
      "epoch": 0.7437380864796175,
      "grad_norm": 0.6236628293991089,
      "learning_rate": 3.7603121516164995e-05,
      "loss": 0.9387,
      "step": 5560
    },
    {
      "epoch": 0.7450757449085376,
      "grad_norm": 0.6379913091659546,
      "learning_rate": 3.758082497212932e-05,
      "loss": 0.8788,
      "step": 5570
    },
    {
      "epoch": 0.7464134033374578,
      "grad_norm": 0.6068527102470398,
      "learning_rate": 3.755852842809365e-05,
      "loss": 0.9014,
      "step": 5580
    },
    {
      "epoch": 0.747751061766378,
      "grad_norm": 0.661860466003418,
      "learning_rate": 3.753623188405797e-05,
      "loss": 0.9202,
      "step": 5590
    },
    {
      "epoch": 0.7490887201952982,
      "grad_norm": 0.6266321539878845,
      "learning_rate": 3.75139353400223e-05,
      "loss": 0.9027,
      "step": 5600
    },
    {
      "epoch": 0.7504263786242183,
      "grad_norm": 0.6660729646682739,
      "learning_rate": 3.7491638795986625e-05,
      "loss": 0.9156,
      "step": 5610
    },
    {
      "epoch": 0.7517640370531384,
      "grad_norm": 0.6221464276313782,
      "learning_rate": 3.746934225195095e-05,
      "loss": 0.8983,
      "step": 5620
    },
    {
      "epoch": 0.7531016954820586,
      "grad_norm": 0.6249263286590576,
      "learning_rate": 3.744704570791527e-05,
      "loss": 0.8975,
      "step": 5630
    },
    {
      "epoch": 0.7544393539109788,
      "grad_norm": 0.6644915342330933,
      "learning_rate": 3.74247491638796e-05,
      "loss": 0.8911,
      "step": 5640
    },
    {
      "epoch": 0.755777012339899,
      "grad_norm": 0.6554909348487854,
      "learning_rate": 3.740245261984393e-05,
      "loss": 0.9005,
      "step": 5650
    },
    {
      "epoch": 0.7571146707688192,
      "grad_norm": 0.5908352136611938,
      "learning_rate": 3.7380156075808254e-05,
      "loss": 0.9191,
      "step": 5660
    },
    {
      "epoch": 0.7584523291977393,
      "grad_norm": 0.6731030941009521,
      "learning_rate": 3.7357859531772575e-05,
      "loss": 0.9146,
      "step": 5670
    },
    {
      "epoch": 0.7597899876266595,
      "grad_norm": 0.6637604236602783,
      "learning_rate": 3.73355629877369e-05,
      "loss": 0.8963,
      "step": 5680
    },
    {
      "epoch": 0.7611276460555797,
      "grad_norm": 0.6759627461433411,
      "learning_rate": 3.731326644370123e-05,
      "loss": 0.8964,
      "step": 5690
    },
    {
      "epoch": 0.7624653044844999,
      "grad_norm": 0.6923184394836426,
      "learning_rate": 3.729096989966555e-05,
      "loss": 0.944,
      "step": 5700
    },
    {
      "epoch": 0.7638029629134201,
      "grad_norm": 0.5918211936950684,
      "learning_rate": 3.726867335562988e-05,
      "loss": 0.9172,
      "step": 5710
    },
    {
      "epoch": 0.7651406213423403,
      "grad_norm": 0.6405092477798462,
      "learning_rate": 3.7246376811594204e-05,
      "loss": 0.9181,
      "step": 5720
    },
    {
      "epoch": 0.7664782797712604,
      "grad_norm": 0.6291745901107788,
      "learning_rate": 3.722408026755853e-05,
      "loss": 0.8918,
      "step": 5730
    },
    {
      "epoch": 0.7678159382001806,
      "grad_norm": 0.6601352095603943,
      "learning_rate": 3.720178372352285e-05,
      "loss": 0.9212,
      "step": 5740
    },
    {
      "epoch": 0.7691535966291008,
      "grad_norm": 0.7210485339164734,
      "learning_rate": 3.717948717948718e-05,
      "loss": 0.9039,
      "step": 5750
    },
    {
      "epoch": 0.770491255058021,
      "grad_norm": 0.6710047125816345,
      "learning_rate": 3.7157190635451506e-05,
      "loss": 0.9089,
      "step": 5760
    },
    {
      "epoch": 0.7718289134869412,
      "grad_norm": 0.6624544262886047,
      "learning_rate": 3.7134894091415834e-05,
      "loss": 0.942,
      "step": 5770
    },
    {
      "epoch": 0.7731665719158612,
      "grad_norm": 0.6330002546310425,
      "learning_rate": 3.7112597547380154e-05,
      "loss": 0.9272,
      "step": 5780
    },
    {
      "epoch": 0.7745042303447814,
      "grad_norm": 0.6512125730514526,
      "learning_rate": 3.709030100334449e-05,
      "loss": 0.9337,
      "step": 5790
    },
    {
      "epoch": 0.7758418887737016,
      "grad_norm": 0.6626552939414978,
      "learning_rate": 3.706800445930881e-05,
      "loss": 0.8885,
      "step": 5800
    },
    {
      "epoch": 0.7771795472026218,
      "grad_norm": 0.6126362681388855,
      "learning_rate": 3.7045707915273136e-05,
      "loss": 0.9108,
      "step": 5810
    },
    {
      "epoch": 0.778517205631542,
      "grad_norm": 0.5886779427528381,
      "learning_rate": 3.7023411371237457e-05,
      "loss": 0.8875,
      "step": 5820
    },
    {
      "epoch": 0.7798548640604621,
      "grad_norm": 0.6370865702629089,
      "learning_rate": 3.700111482720179e-05,
      "loss": 0.8936,
      "step": 5830
    },
    {
      "epoch": 0.7811925224893823,
      "grad_norm": 0.6488296985626221,
      "learning_rate": 3.697881828316611e-05,
      "loss": 0.9139,
      "step": 5840
    },
    {
      "epoch": 0.7825301809183025,
      "grad_norm": 0.6419482231140137,
      "learning_rate": 3.695652173913043e-05,
      "loss": 0.9077,
      "step": 5850
    },
    {
      "epoch": 0.7838678393472227,
      "grad_norm": 0.6363482475280762,
      "learning_rate": 3.6934225195094766e-05,
      "loss": 0.8933,
      "step": 5860
    },
    {
      "epoch": 0.7852054977761429,
      "grad_norm": 0.6631439924240112,
      "learning_rate": 3.6911928651059086e-05,
      "loss": 0.8914,
      "step": 5870
    },
    {
      "epoch": 0.786543156205063,
      "grad_norm": 0.7137128114700317,
      "learning_rate": 3.688963210702341e-05,
      "loss": 0.9127,
      "step": 5880
    },
    {
      "epoch": 0.7878808146339832,
      "grad_norm": 0.6592470407485962,
      "learning_rate": 3.6867335562987734e-05,
      "loss": 0.9059,
      "step": 5890
    },
    {
      "epoch": 0.7892184730629034,
      "grad_norm": 0.6844088435173035,
      "learning_rate": 3.684503901895207e-05,
      "loss": 0.9191,
      "step": 5900
    },
    {
      "epoch": 0.7905561314918236,
      "grad_norm": 0.6627894639968872,
      "learning_rate": 3.682274247491639e-05,
      "loss": 0.9204,
      "step": 5910
    },
    {
      "epoch": 0.7918937899207438,
      "grad_norm": 0.6099228262901306,
      "learning_rate": 3.6800445930880716e-05,
      "loss": 0.9113,
      "step": 5920
    },
    {
      "epoch": 0.7932314483496639,
      "grad_norm": 0.6670475602149963,
      "learning_rate": 3.677814938684504e-05,
      "loss": 0.9303,
      "step": 5930
    },
    {
      "epoch": 0.7945691067785841,
      "grad_norm": 0.673721969127655,
      "learning_rate": 3.675585284280937e-05,
      "loss": 0.8981,
      "step": 5940
    },
    {
      "epoch": 0.7959067652075043,
      "grad_norm": 0.6255562901496887,
      "learning_rate": 3.673355629877369e-05,
      "loss": 0.9118,
      "step": 5950
    },
    {
      "epoch": 0.7972444236364244,
      "grad_norm": 0.559866189956665,
      "learning_rate": 3.671125975473802e-05,
      "loss": 0.8946,
      "step": 5960
    },
    {
      "epoch": 0.7985820820653446,
      "grad_norm": 0.6588952541351318,
      "learning_rate": 3.6688963210702345e-05,
      "loss": 0.9035,
      "step": 5970
    },
    {
      "epoch": 0.7999197404942648,
      "grad_norm": 0.5988457798957825,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.9249,
      "step": 5980
    },
    {
      "epoch": 0.8012573989231849,
      "grad_norm": 0.6396999955177307,
      "learning_rate": 3.664437012263099e-05,
      "loss": 0.8982,
      "step": 5990
    },
    {
      "epoch": 0.8025950573521051,
      "grad_norm": 0.6981359720230103,
      "learning_rate": 3.662207357859532e-05,
      "loss": 0.8968,
      "step": 6000
    },
    {
      "epoch": 0.8039327157810253,
      "grad_norm": 0.6372494101524353,
      "learning_rate": 3.659977703455965e-05,
      "loss": 0.8808,
      "step": 6010
    },
    {
      "epoch": 0.8052703742099455,
      "grad_norm": 0.6859272122383118,
      "learning_rate": 3.657748049052397e-05,
      "loss": 0.9089,
      "step": 6020
    },
    {
      "epoch": 0.8066080326388657,
      "grad_norm": 0.6973025798797607,
      "learning_rate": 3.6555183946488295e-05,
      "loss": 0.9077,
      "step": 6030
    },
    {
      "epoch": 0.8079456910677858,
      "grad_norm": 0.6118180155754089,
      "learning_rate": 3.653288740245262e-05,
      "loss": 0.8912,
      "step": 6040
    },
    {
      "epoch": 0.809283349496706,
      "grad_norm": 0.6656413078308105,
      "learning_rate": 3.651059085841695e-05,
      "loss": 0.9014,
      "step": 6050
    },
    {
      "epoch": 0.8106210079256262,
      "grad_norm": 0.6776073575019836,
      "learning_rate": 3.648829431438127e-05,
      "loss": 0.9187,
      "step": 6060
    },
    {
      "epoch": 0.8119586663545464,
      "grad_norm": 0.6308229565620422,
      "learning_rate": 3.64659977703456e-05,
      "loss": 0.8903,
      "step": 6070
    },
    {
      "epoch": 0.8132963247834666,
      "grad_norm": 0.589821457862854,
      "learning_rate": 3.6443701226309925e-05,
      "loss": 0.8974,
      "step": 6080
    },
    {
      "epoch": 0.8146339832123867,
      "grad_norm": 0.638775110244751,
      "learning_rate": 3.642140468227425e-05,
      "loss": 0.922,
      "step": 6090
    },
    {
      "epoch": 0.8159716416413069,
      "grad_norm": 0.611321210861206,
      "learning_rate": 3.639910813823857e-05,
      "loss": 0.9111,
      "step": 6100
    },
    {
      "epoch": 0.8173093000702271,
      "grad_norm": 0.5931627750396729,
      "learning_rate": 3.63768115942029e-05,
      "loss": 0.9284,
      "step": 6110
    },
    {
      "epoch": 0.8186469584991473,
      "grad_norm": 0.6605625152587891,
      "learning_rate": 3.635451505016723e-05,
      "loss": 0.8989,
      "step": 6120
    },
    {
      "epoch": 0.8199846169280675,
      "grad_norm": 0.6185629963874817,
      "learning_rate": 3.633221850613155e-05,
      "loss": 0.905,
      "step": 6130
    },
    {
      "epoch": 0.8213222753569875,
      "grad_norm": 0.639694333076477,
      "learning_rate": 3.6309921962095875e-05,
      "loss": 0.9252,
      "step": 6140
    },
    {
      "epoch": 0.8226599337859077,
      "grad_norm": 0.6128517389297485,
      "learning_rate": 3.62876254180602e-05,
      "loss": 0.9143,
      "step": 6150
    },
    {
      "epoch": 0.8239975922148279,
      "grad_norm": 0.5692688226699829,
      "learning_rate": 3.626532887402453e-05,
      "loss": 0.9021,
      "step": 6160
    },
    {
      "epoch": 0.8253352506437481,
      "grad_norm": 0.7307711243629456,
      "learning_rate": 3.624303232998885e-05,
      "loss": 0.9382,
      "step": 6170
    },
    {
      "epoch": 0.8266729090726683,
      "grad_norm": 0.6571834087371826,
      "learning_rate": 3.622073578595318e-05,
      "loss": 0.9184,
      "step": 6180
    },
    {
      "epoch": 0.8280105675015885,
      "grad_norm": 0.6414992809295654,
      "learning_rate": 3.6198439241917504e-05,
      "loss": 0.9298,
      "step": 6190
    },
    {
      "epoch": 0.8293482259305086,
      "grad_norm": 0.6093255877494812,
      "learning_rate": 3.617614269788183e-05,
      "loss": 0.8983,
      "step": 6200
    },
    {
      "epoch": 0.8306858843594288,
      "grad_norm": 0.6454983353614807,
      "learning_rate": 3.615384615384615e-05,
      "loss": 0.9278,
      "step": 6210
    },
    {
      "epoch": 0.832023542788349,
      "grad_norm": 0.6708608269691467,
      "learning_rate": 3.6131549609810486e-05,
      "loss": 0.9081,
      "step": 6220
    },
    {
      "epoch": 0.8333612012172692,
      "grad_norm": 0.6107837557792664,
      "learning_rate": 3.610925306577481e-05,
      "loss": 0.8771,
      "step": 6230
    },
    {
      "epoch": 0.8346988596461894,
      "grad_norm": 0.6175365447998047,
      "learning_rate": 3.6086956521739134e-05,
      "loss": 0.8992,
      "step": 6240
    },
    {
      "epoch": 0.8360365180751095,
      "grad_norm": 0.6235330700874329,
      "learning_rate": 3.6064659977703455e-05,
      "loss": 0.9191,
      "step": 6250
    },
    {
      "epoch": 0.8373741765040297,
      "grad_norm": 0.6499614119529724,
      "learning_rate": 3.604236343366778e-05,
      "loss": 0.911,
      "step": 6260
    },
    {
      "epoch": 0.8387118349329499,
      "grad_norm": 0.6251172423362732,
      "learning_rate": 3.602006688963211e-05,
      "loss": 0.9011,
      "step": 6270
    },
    {
      "epoch": 0.8400494933618701,
      "grad_norm": 0.6203373074531555,
      "learning_rate": 3.599777034559643e-05,
      "loss": 0.8812,
      "step": 6280
    },
    {
      "epoch": 0.8413871517907903,
      "grad_norm": 0.6299372911453247,
      "learning_rate": 3.5975473801560764e-05,
      "loss": 0.922,
      "step": 6290
    },
    {
      "epoch": 0.8427248102197104,
      "grad_norm": 0.6163797974586487,
      "learning_rate": 3.5953177257525084e-05,
      "loss": 0.9079,
      "step": 6300
    },
    {
      "epoch": 0.8440624686486305,
      "grad_norm": 0.6376249194145203,
      "learning_rate": 3.593088071348941e-05,
      "loss": 0.8737,
      "step": 6310
    },
    {
      "epoch": 0.8454001270775507,
      "grad_norm": 0.5918400287628174,
      "learning_rate": 3.590858416945373e-05,
      "loss": 0.8951,
      "step": 6320
    },
    {
      "epoch": 0.8467377855064709,
      "grad_norm": 0.6856315732002258,
      "learning_rate": 3.5886287625418066e-05,
      "loss": 0.8987,
      "step": 6330
    },
    {
      "epoch": 0.8480754439353911,
      "grad_norm": 0.5786163210868835,
      "learning_rate": 3.5863991081382386e-05,
      "loss": 0.9119,
      "step": 6340
    },
    {
      "epoch": 0.8494131023643112,
      "grad_norm": 1.057845950126648,
      "learning_rate": 3.5841694537346714e-05,
      "loss": 0.8966,
      "step": 6350
    },
    {
      "epoch": 0.8507507607932314,
      "grad_norm": 0.6132193803787231,
      "learning_rate": 3.581939799331104e-05,
      "loss": 0.9136,
      "step": 6360
    },
    {
      "epoch": 0.8520884192221516,
      "grad_norm": 0.6384791731834412,
      "learning_rate": 3.579710144927537e-05,
      "loss": 0.8797,
      "step": 6370
    },
    {
      "epoch": 0.8534260776510718,
      "grad_norm": 0.6387732028961182,
      "learning_rate": 3.577480490523969e-05,
      "loss": 0.9047,
      "step": 6380
    },
    {
      "epoch": 0.854763736079992,
      "grad_norm": 0.6335698962211609,
      "learning_rate": 3.575250836120401e-05,
      "loss": 0.9339,
      "step": 6390
    },
    {
      "epoch": 0.8561013945089121,
      "grad_norm": 0.5965031385421753,
      "learning_rate": 3.573021181716834e-05,
      "loss": 0.8848,
      "step": 6400
    },
    {
      "epoch": 0.8574390529378323,
      "grad_norm": 0.7227400541305542,
      "learning_rate": 3.5707915273132664e-05,
      "loss": 0.891,
      "step": 6410
    },
    {
      "epoch": 0.8587767113667525,
      "grad_norm": 0.6333751678466797,
      "learning_rate": 3.568561872909699e-05,
      "loss": 0.9042,
      "step": 6420
    },
    {
      "epoch": 0.8601143697956727,
      "grad_norm": 0.6097814440727234,
      "learning_rate": 3.566332218506132e-05,
      "loss": 0.9284,
      "step": 6430
    },
    {
      "epoch": 0.8614520282245929,
      "grad_norm": 0.6116750240325928,
      "learning_rate": 3.5641025641025646e-05,
      "loss": 0.9221,
      "step": 6440
    },
    {
      "epoch": 0.8627896866535131,
      "grad_norm": 0.676612913608551,
      "learning_rate": 3.5618729096989966e-05,
      "loss": 0.9209,
      "step": 6450
    },
    {
      "epoch": 0.8641273450824332,
      "grad_norm": 0.620509922504425,
      "learning_rate": 3.559643255295429e-05,
      "loss": 0.9204,
      "step": 6460
    },
    {
      "epoch": 0.8654650035113534,
      "grad_norm": 0.6446654200553894,
      "learning_rate": 3.557413600891862e-05,
      "loss": 0.9127,
      "step": 6470
    },
    {
      "epoch": 0.8668026619402736,
      "grad_norm": 0.6520272493362427,
      "learning_rate": 3.555183946488295e-05,
      "loss": 0.8629,
      "step": 6480
    },
    {
      "epoch": 0.8681403203691938,
      "grad_norm": 0.6666213274002075,
      "learning_rate": 3.552954292084727e-05,
      "loss": 0.9044,
      "step": 6490
    },
    {
      "epoch": 0.869477978798114,
      "grad_norm": 0.6287572383880615,
      "learning_rate": 3.5507246376811596e-05,
      "loss": 0.8925,
      "step": 6500
    },
    {
      "epoch": 0.870815637227034,
      "grad_norm": 0.6013862490653992,
      "learning_rate": 3.548494983277592e-05,
      "loss": 0.8938,
      "step": 6510
    },
    {
      "epoch": 0.8721532956559542,
      "grad_norm": 0.6685725450515747,
      "learning_rate": 3.546265328874025e-05,
      "loss": 0.8745,
      "step": 6520
    },
    {
      "epoch": 0.8734909540848744,
      "grad_norm": 0.6440210938453674,
      "learning_rate": 3.544035674470457e-05,
      "loss": 0.8867,
      "step": 6530
    },
    {
      "epoch": 0.8748286125137946,
      "grad_norm": 0.6234233975410461,
      "learning_rate": 3.54180602006689e-05,
      "loss": 0.9155,
      "step": 6540
    },
    {
      "epoch": 0.8761662709427148,
      "grad_norm": 0.6897587776184082,
      "learning_rate": 3.5395763656633225e-05,
      "loss": 0.8737,
      "step": 6550
    },
    {
      "epoch": 0.8775039293716349,
      "grad_norm": 0.6321918368339539,
      "learning_rate": 3.5373467112597546e-05,
      "loss": 0.9032,
      "step": 6560
    },
    {
      "epoch": 0.8788415878005551,
      "grad_norm": 0.6378454566001892,
      "learning_rate": 3.535117056856187e-05,
      "loss": 0.9161,
      "step": 6570
    },
    {
      "epoch": 0.8801792462294753,
      "grad_norm": 0.6046990752220154,
      "learning_rate": 3.53288740245262e-05,
      "loss": 0.9161,
      "step": 6580
    },
    {
      "epoch": 0.8815169046583955,
      "grad_norm": 0.6482231616973877,
      "learning_rate": 3.530657748049053e-05,
      "loss": 0.8972,
      "step": 6590
    },
    {
      "epoch": 0.8828545630873157,
      "grad_norm": 0.6296001076698303,
      "learning_rate": 3.528428093645485e-05,
      "loss": 0.8884,
      "step": 6600
    },
    {
      "epoch": 0.8841922215162358,
      "grad_norm": 0.626030445098877,
      "learning_rate": 3.5261984392419175e-05,
      "loss": 0.8968,
      "step": 6610
    },
    {
      "epoch": 0.885529879945156,
      "grad_norm": 0.6223989129066467,
      "learning_rate": 3.52396878483835e-05,
      "loss": 0.9235,
      "step": 6620
    },
    {
      "epoch": 0.8868675383740762,
      "grad_norm": 0.6217398047447205,
      "learning_rate": 3.521739130434783e-05,
      "loss": 0.9166,
      "step": 6630
    },
    {
      "epoch": 0.8882051968029964,
      "grad_norm": 0.5892893075942993,
      "learning_rate": 3.519509476031215e-05,
      "loss": 0.8992,
      "step": 6640
    },
    {
      "epoch": 0.8895428552319166,
      "grad_norm": 0.6559150218963623,
      "learning_rate": 3.5172798216276484e-05,
      "loss": 0.9013,
      "step": 6650
    },
    {
      "epoch": 0.8908805136608368,
      "grad_norm": 0.7282101511955261,
      "learning_rate": 3.5150501672240805e-05,
      "loss": 0.8878,
      "step": 6660
    },
    {
      "epoch": 0.8922181720897568,
      "grad_norm": 0.6472435593605042,
      "learning_rate": 3.5128205128205125e-05,
      "loss": 0.8973,
      "step": 6670
    },
    {
      "epoch": 0.893555830518677,
      "grad_norm": 0.6166226863861084,
      "learning_rate": 3.510590858416945e-05,
      "loss": 0.9118,
      "step": 6680
    },
    {
      "epoch": 0.8948934889475972,
      "grad_norm": 0.6197119951248169,
      "learning_rate": 3.508361204013378e-05,
      "loss": 0.9265,
      "step": 6690
    },
    {
      "epoch": 0.8962311473765174,
      "grad_norm": 0.6042394042015076,
      "learning_rate": 3.506131549609811e-05,
      "loss": 0.891,
      "step": 6700
    },
    {
      "epoch": 0.8975688058054376,
      "grad_norm": 0.5974552631378174,
      "learning_rate": 3.503901895206243e-05,
      "loss": 0.9115,
      "step": 6710
    },
    {
      "epoch": 0.8989064642343577,
      "grad_norm": 0.6487249135971069,
      "learning_rate": 3.501672240802676e-05,
      "loss": 0.8688,
      "step": 6720
    },
    {
      "epoch": 0.9002441226632779,
      "grad_norm": 0.6476275324821472,
      "learning_rate": 3.499442586399108e-05,
      "loss": 0.9157,
      "step": 6730
    },
    {
      "epoch": 0.9015817810921981,
      "grad_norm": 0.6721205115318298,
      "learning_rate": 3.497212931995541e-05,
      "loss": 0.8929,
      "step": 6740
    },
    {
      "epoch": 0.9029194395211183,
      "grad_norm": 0.6644322276115417,
      "learning_rate": 3.494983277591973e-05,
      "loss": 0.9456,
      "step": 6750
    },
    {
      "epoch": 0.9042570979500385,
      "grad_norm": 0.6264389157295227,
      "learning_rate": 3.4927536231884064e-05,
      "loss": 0.8998,
      "step": 6760
    },
    {
      "epoch": 0.9055947563789586,
      "grad_norm": 0.5899625420570374,
      "learning_rate": 3.4905239687848384e-05,
      "loss": 0.9096,
      "step": 6770
    },
    {
      "epoch": 0.9069324148078788,
      "grad_norm": 0.6670804619789124,
      "learning_rate": 3.488294314381271e-05,
      "loss": 0.8885,
      "step": 6780
    },
    {
      "epoch": 0.908270073236799,
      "grad_norm": 0.6219215989112854,
      "learning_rate": 3.486064659977704e-05,
      "loss": 0.9204,
      "step": 6790
    },
    {
      "epoch": 0.9096077316657192,
      "grad_norm": 0.6331580877304077,
      "learning_rate": 3.4838350055741366e-05,
      "loss": 0.9043,
      "step": 6800
    },
    {
      "epoch": 0.9109453900946394,
      "grad_norm": 0.6299989223480225,
      "learning_rate": 3.481605351170569e-05,
      "loss": 0.8971,
      "step": 6810
    },
    {
      "epoch": 0.9122830485235595,
      "grad_norm": 0.6617931127548218,
      "learning_rate": 3.479375696767001e-05,
      "loss": 0.9253,
      "step": 6820
    },
    {
      "epoch": 0.9136207069524797,
      "grad_norm": 0.5696532726287842,
      "learning_rate": 3.477146042363434e-05,
      "loss": 0.8862,
      "step": 6830
    },
    {
      "epoch": 0.9149583653813999,
      "grad_norm": 0.6261255145072937,
      "learning_rate": 3.474916387959866e-05,
      "loss": 0.9239,
      "step": 6840
    },
    {
      "epoch": 0.91629602381032,
      "grad_norm": 0.5885829329490662,
      "learning_rate": 3.472686733556299e-05,
      "loss": 0.9225,
      "step": 6850
    },
    {
      "epoch": 0.9176336822392402,
      "grad_norm": 0.6510170698165894,
      "learning_rate": 3.4704570791527316e-05,
      "loss": 0.9151,
      "step": 6860
    },
    {
      "epoch": 0.9189713406681603,
      "grad_norm": 0.6430070400238037,
      "learning_rate": 3.4682274247491644e-05,
      "loss": 0.9171,
      "step": 6870
    },
    {
      "epoch": 0.9203089990970805,
      "grad_norm": 0.6367025375366211,
      "learning_rate": 3.4659977703455964e-05,
      "loss": 0.8821,
      "step": 6880
    },
    {
      "epoch": 0.9216466575260007,
      "grad_norm": 0.6540505290031433,
      "learning_rate": 3.463768115942029e-05,
      "loss": 0.8781,
      "step": 6890
    },
    {
      "epoch": 0.9229843159549209,
      "grad_norm": 0.5759763717651367,
      "learning_rate": 3.461538461538462e-05,
      "loss": 0.9017,
      "step": 6900
    },
    {
      "epoch": 0.9243219743838411,
      "grad_norm": 0.6133946776390076,
      "learning_rate": 3.4593088071348946e-05,
      "loss": 0.9054,
      "step": 6910
    },
    {
      "epoch": 0.9256596328127613,
      "grad_norm": 0.6317106485366821,
      "learning_rate": 3.4570791527313266e-05,
      "loss": 0.8966,
      "step": 6920
    },
    {
      "epoch": 0.9269972912416814,
      "grad_norm": 0.6066046357154846,
      "learning_rate": 3.4548494983277594e-05,
      "loss": 0.9377,
      "step": 6930
    },
    {
      "epoch": 0.9283349496706016,
      "grad_norm": 0.6300996541976929,
      "learning_rate": 3.452619843924192e-05,
      "loss": 0.9157,
      "step": 6940
    },
    {
      "epoch": 0.9296726080995218,
      "grad_norm": 0.6880373954772949,
      "learning_rate": 3.450390189520624e-05,
      "loss": 0.8927,
      "step": 6950
    },
    {
      "epoch": 0.931010266528442,
      "grad_norm": 0.6377573609352112,
      "learning_rate": 3.448160535117057e-05,
      "loss": 0.8955,
      "step": 6960
    },
    {
      "epoch": 0.9323479249573622,
      "grad_norm": 0.7483707070350647,
      "learning_rate": 3.4459308807134896e-05,
      "loss": 0.899,
      "step": 6970
    },
    {
      "epoch": 0.9336855833862823,
      "grad_norm": 0.6905088424682617,
      "learning_rate": 3.443701226309922e-05,
      "loss": 0.9302,
      "step": 6980
    },
    {
      "epoch": 0.9350232418152025,
      "grad_norm": 0.6649338603019714,
      "learning_rate": 3.4414715719063544e-05,
      "loss": 0.9476,
      "step": 6990
    },
    {
      "epoch": 0.9363609002441227,
      "grad_norm": 0.672871470451355,
      "learning_rate": 3.439241917502787e-05,
      "loss": 0.8859,
      "step": 7000
    },
    {
      "epoch": 0.9376985586730429,
      "grad_norm": 0.6177012324333191,
      "learning_rate": 3.43701226309922e-05,
      "loss": 0.8822,
      "step": 7010
    },
    {
      "epoch": 0.939036217101963,
      "grad_norm": 0.6244273781776428,
      "learning_rate": 3.4347826086956526e-05,
      "loss": 0.9028,
      "step": 7020
    },
    {
      "epoch": 0.9403738755308831,
      "grad_norm": 0.6839705109596252,
      "learning_rate": 3.4325529542920846e-05,
      "loss": 0.8999,
      "step": 7030
    },
    {
      "epoch": 0.9417115339598033,
      "grad_norm": 0.6909990906715393,
      "learning_rate": 3.430323299888517e-05,
      "loss": 0.9006,
      "step": 7040
    },
    {
      "epoch": 0.9430491923887235,
      "grad_norm": 0.6863601207733154,
      "learning_rate": 3.42809364548495e-05,
      "loss": 0.9204,
      "step": 7050
    },
    {
      "epoch": 0.9443868508176437,
      "grad_norm": 0.6475265026092529,
      "learning_rate": 3.425863991081383e-05,
      "loss": 0.8813,
      "step": 7060
    },
    {
      "epoch": 0.9457245092465639,
      "grad_norm": 0.6731613874435425,
      "learning_rate": 3.423634336677815e-05,
      "loss": 0.9121,
      "step": 7070
    },
    {
      "epoch": 0.947062167675484,
      "grad_norm": 0.6453477144241333,
      "learning_rate": 3.421404682274248e-05,
      "loss": 0.9267,
      "step": 7080
    },
    {
      "epoch": 0.9483998261044042,
      "grad_norm": 0.6557169556617737,
      "learning_rate": 3.41917502787068e-05,
      "loss": 0.8905,
      "step": 7090
    },
    {
      "epoch": 0.9497374845333244,
      "grad_norm": 0.598081648349762,
      "learning_rate": 3.416945373467112e-05,
      "loss": 0.8994,
      "step": 7100
    },
    {
      "epoch": 0.9510751429622446,
      "grad_norm": 0.7379364967346191,
      "learning_rate": 3.414715719063545e-05,
      "loss": 0.9039,
      "step": 7110
    },
    {
      "epoch": 0.9524128013911648,
      "grad_norm": 0.6155983209609985,
      "learning_rate": 3.412486064659978e-05,
      "loss": 0.9023,
      "step": 7120
    },
    {
      "epoch": 0.953750459820085,
      "grad_norm": 0.6137374639511108,
      "learning_rate": 3.4102564102564105e-05,
      "loss": 0.9276,
      "step": 7130
    },
    {
      "epoch": 0.9550881182490051,
      "grad_norm": 0.6120321154594421,
      "learning_rate": 3.4080267558528426e-05,
      "loss": 0.8573,
      "step": 7140
    },
    {
      "epoch": 0.9564257766779253,
      "grad_norm": 0.6491210460662842,
      "learning_rate": 3.405797101449276e-05,
      "loss": 0.9,
      "step": 7150
    },
    {
      "epoch": 0.9577634351068455,
      "grad_norm": 0.6080308556556702,
      "learning_rate": 3.403567447045708e-05,
      "loss": 0.9125,
      "step": 7160
    },
    {
      "epoch": 0.9591010935357657,
      "grad_norm": 0.6012215614318848,
      "learning_rate": 3.401337792642141e-05,
      "loss": 0.9158,
      "step": 7170
    },
    {
      "epoch": 0.9604387519646859,
      "grad_norm": 0.6038694381713867,
      "learning_rate": 3.399108138238573e-05,
      "loss": 0.8924,
      "step": 7180
    },
    {
      "epoch": 0.961776410393606,
      "grad_norm": 0.6013750433921814,
      "learning_rate": 3.396878483835006e-05,
      "loss": 0.91,
      "step": 7190
    },
    {
      "epoch": 0.9631140688225261,
      "grad_norm": 0.6385361552238464,
      "learning_rate": 3.394648829431438e-05,
      "loss": 0.8925,
      "step": 7200
    },
    {
      "epoch": 0.9644517272514463,
      "grad_norm": 0.5962517857551575,
      "learning_rate": 3.392419175027871e-05,
      "loss": 0.9254,
      "step": 7210
    },
    {
      "epoch": 0.9657893856803665,
      "grad_norm": 0.6036651134490967,
      "learning_rate": 3.390189520624304e-05,
      "loss": 0.8963,
      "step": 7220
    },
    {
      "epoch": 0.9671270441092867,
      "grad_norm": 0.599075973033905,
      "learning_rate": 3.387959866220736e-05,
      "loss": 0.9156,
      "step": 7230
    },
    {
      "epoch": 0.9684647025382068,
      "grad_norm": 0.6310087442398071,
      "learning_rate": 3.3857302118171685e-05,
      "loss": 0.8769,
      "step": 7240
    },
    {
      "epoch": 0.969802360967127,
      "grad_norm": 0.6396264433860779,
      "learning_rate": 3.3835005574136005e-05,
      "loss": 0.896,
      "step": 7250
    },
    {
      "epoch": 0.9711400193960472,
      "grad_norm": 0.6490859985351562,
      "learning_rate": 3.381270903010034e-05,
      "loss": 0.889,
      "step": 7260
    },
    {
      "epoch": 0.9724776778249674,
      "grad_norm": 0.664085865020752,
      "learning_rate": 3.379041248606466e-05,
      "loss": 0.9112,
      "step": 7270
    },
    {
      "epoch": 0.9738153362538876,
      "grad_norm": 0.6340976357460022,
      "learning_rate": 3.376811594202899e-05,
      "loss": 0.9132,
      "step": 7280
    },
    {
      "epoch": 0.9751529946828077,
      "grad_norm": 0.5546550154685974,
      "learning_rate": 3.3745819397993314e-05,
      "loss": 0.9125,
      "step": 7290
    },
    {
      "epoch": 0.9764906531117279,
      "grad_norm": 0.6153939962387085,
      "learning_rate": 3.372352285395764e-05,
      "loss": 0.9181,
      "step": 7300
    },
    {
      "epoch": 0.9778283115406481,
      "grad_norm": 0.6782386302947998,
      "learning_rate": 3.370122630992196e-05,
      "loss": 0.9221,
      "step": 7310
    },
    {
      "epoch": 0.9791659699695683,
      "grad_norm": 0.6018093824386597,
      "learning_rate": 3.367892976588629e-05,
      "loss": 0.8899,
      "step": 7320
    },
    {
      "epoch": 0.9805036283984885,
      "grad_norm": 0.5970489978790283,
      "learning_rate": 3.365663322185062e-05,
      "loss": 0.9044,
      "step": 7330
    },
    {
      "epoch": 0.9818412868274086,
      "grad_norm": 0.6263186931610107,
      "learning_rate": 3.3634336677814944e-05,
      "loss": 0.8929,
      "step": 7340
    },
    {
      "epoch": 0.9831789452563288,
      "grad_norm": 0.6441662311553955,
      "learning_rate": 3.3612040133779264e-05,
      "loss": 0.8967,
      "step": 7350
    },
    {
      "epoch": 0.984516603685249,
      "grad_norm": 0.6507819294929504,
      "learning_rate": 3.358974358974359e-05,
      "loss": 0.9174,
      "step": 7360
    },
    {
      "epoch": 0.9858542621141692,
      "grad_norm": 0.6301035284996033,
      "learning_rate": 3.356744704570792e-05,
      "loss": 0.9227,
      "step": 7370
    },
    {
      "epoch": 0.9871919205430894,
      "grad_norm": 0.6210976243019104,
      "learning_rate": 3.354515050167224e-05,
      "loss": 0.9053,
      "step": 7380
    },
    {
      "epoch": 0.9885295789720095,
      "grad_norm": 0.5906566381454468,
      "learning_rate": 3.352285395763657e-05,
      "loss": 0.9174,
      "step": 7390
    },
    {
      "epoch": 0.9898672374009296,
      "grad_norm": 0.6009110808372498,
      "learning_rate": 3.3500557413600894e-05,
      "loss": 0.8941,
      "step": 7400
    },
    {
      "epoch": 0.9912048958298498,
      "grad_norm": 0.7065934538841248,
      "learning_rate": 3.347826086956522e-05,
      "loss": 0.8853,
      "step": 7410
    },
    {
      "epoch": 0.99254255425877,
      "grad_norm": 0.5933826565742493,
      "learning_rate": 3.345596432552954e-05,
      "loss": 0.8951,
      "step": 7420
    },
    {
      "epoch": 0.9938802126876902,
      "grad_norm": 0.6246691346168518,
      "learning_rate": 3.343366778149387e-05,
      "loss": 0.9247,
      "step": 7430
    },
    {
      "epoch": 0.9952178711166104,
      "grad_norm": 0.5911114811897278,
      "learning_rate": 3.3411371237458196e-05,
      "loss": 0.8866,
      "step": 7440
    },
    {
      "epoch": 0.9965555295455305,
      "grad_norm": 0.5851314067840576,
      "learning_rate": 3.3389074693422524e-05,
      "loss": 0.8994,
      "step": 7450
    },
    {
      "epoch": 0.9978931879744507,
      "grad_norm": 0.6199345588684082,
      "learning_rate": 3.3366778149386844e-05,
      "loss": 0.8888,
      "step": 7460
    },
    {
      "epoch": 0.9992308464033709,
      "grad_norm": 0.6277195811271667,
      "learning_rate": 3.334448160535117e-05,
      "loss": 0.9214,
      "step": 7470
    },
    {
      "epoch": 1.000568504832291,
      "grad_norm": 0.5840317010879517,
      "learning_rate": 3.33221850613155e-05,
      "loss": 0.8971,
      "step": 7480
    },
    {
      "epoch": 1.0019061632612112,
      "grad_norm": 0.6804040670394897,
      "learning_rate": 3.3299888517279826e-05,
      "loss": 0.8499,
      "step": 7490
    },
    {
      "epoch": 1.0032438216901314,
      "grad_norm": 0.7375185489654541,
      "learning_rate": 3.3277591973244146e-05,
      "loss": 0.8544,
      "step": 7500
    },
    {
      "epoch": 1.0045814801190516,
      "grad_norm": 0.69166499376297,
      "learning_rate": 3.3255295429208474e-05,
      "loss": 0.8448,
      "step": 7510
    },
    {
      "epoch": 1.0059191385479718,
      "grad_norm": 0.6475904583930969,
      "learning_rate": 3.32329988851728e-05,
      "loss": 0.8801,
      "step": 7520
    },
    {
      "epoch": 1.007256796976892,
      "grad_norm": 0.6258841753005981,
      "learning_rate": 3.321070234113712e-05,
      "loss": 0.8731,
      "step": 7530
    },
    {
      "epoch": 1.0085944554058122,
      "grad_norm": 0.637891948223114,
      "learning_rate": 3.318840579710145e-05,
      "loss": 0.8481,
      "step": 7540
    },
    {
      "epoch": 1.0099321138347324,
      "grad_norm": 0.6535763144493103,
      "learning_rate": 3.3166109253065776e-05,
      "loss": 0.8507,
      "step": 7550
    },
    {
      "epoch": 1.0112697722636526,
      "grad_norm": 0.6617169380187988,
      "learning_rate": 3.31438127090301e-05,
      "loss": 0.8894,
      "step": 7560
    },
    {
      "epoch": 1.0126074306925728,
      "grad_norm": 0.7248775959014893,
      "learning_rate": 3.3121516164994424e-05,
      "loss": 0.8729,
      "step": 7570
    },
    {
      "epoch": 1.0139450891214927,
      "grad_norm": 0.6866939663887024,
      "learning_rate": 3.309921962095876e-05,
      "loss": 0.8634,
      "step": 7580
    },
    {
      "epoch": 1.015282747550413,
      "grad_norm": 0.6316626667976379,
      "learning_rate": 3.307692307692308e-05,
      "loss": 0.8761,
      "step": 7590
    },
    {
      "epoch": 1.0166204059793331,
      "grad_norm": 0.6369361281394958,
      "learning_rate": 3.3054626532887405e-05,
      "loss": 0.863,
      "step": 7600
    },
    {
      "epoch": 1.0179580644082533,
      "grad_norm": 0.6285863518714905,
      "learning_rate": 3.3032329988851726e-05,
      "loss": 0.8496,
      "step": 7610
    },
    {
      "epoch": 1.0192957228371735,
      "grad_norm": 0.7041731476783752,
      "learning_rate": 3.301003344481606e-05,
      "loss": 0.8642,
      "step": 7620
    },
    {
      "epoch": 1.0206333812660937,
      "grad_norm": 0.5969626903533936,
      "learning_rate": 3.298773690078038e-05,
      "loss": 0.8915,
      "step": 7630
    },
    {
      "epoch": 1.021971039695014,
      "grad_norm": 0.6665392518043518,
      "learning_rate": 3.29654403567447e-05,
      "loss": 0.869,
      "step": 7640
    },
    {
      "epoch": 1.023308698123934,
      "grad_norm": 0.6547418236732483,
      "learning_rate": 3.2943143812709035e-05,
      "loss": 0.8665,
      "step": 7650
    },
    {
      "epoch": 1.0246463565528543,
      "grad_norm": 0.7479921579360962,
      "learning_rate": 3.2920847268673356e-05,
      "loss": 0.8623,
      "step": 7660
    },
    {
      "epoch": 1.0259840149817745,
      "grad_norm": 0.68822181224823,
      "learning_rate": 3.289855072463768e-05,
      "loss": 0.8586,
      "step": 7670
    },
    {
      "epoch": 1.0273216734106945,
      "grad_norm": 0.7012093663215637,
      "learning_rate": 3.2876254180602e-05,
      "loss": 0.8722,
      "step": 7680
    },
    {
      "epoch": 1.0286593318396147,
      "grad_norm": 0.6573879718780518,
      "learning_rate": 3.285395763656634e-05,
      "loss": 0.8562,
      "step": 7690
    },
    {
      "epoch": 1.0299969902685349,
      "grad_norm": 0.6610472202301025,
      "learning_rate": 3.283166109253066e-05,
      "loss": 0.8649,
      "step": 7700
    },
    {
      "epoch": 1.031334648697455,
      "grad_norm": 0.7182320356369019,
      "learning_rate": 3.2809364548494985e-05,
      "loss": 0.8847,
      "step": 7710
    },
    {
      "epoch": 1.0326723071263753,
      "grad_norm": 0.6487782597541809,
      "learning_rate": 3.278706800445931e-05,
      "loss": 0.8724,
      "step": 7720
    },
    {
      "epoch": 1.0340099655552955,
      "grad_norm": 0.6894341111183167,
      "learning_rate": 3.276477146042364e-05,
      "loss": 0.8829,
      "step": 7730
    },
    {
      "epoch": 1.0353476239842156,
      "grad_norm": 0.6389723420143127,
      "learning_rate": 3.274247491638796e-05,
      "loss": 0.8754,
      "step": 7740
    },
    {
      "epoch": 1.0366852824131358,
      "grad_norm": 0.6365426182746887,
      "learning_rate": 3.272017837235229e-05,
      "loss": 0.8805,
      "step": 7750
    },
    {
      "epoch": 1.038022940842056,
      "grad_norm": 0.6167002320289612,
      "learning_rate": 3.2697881828316615e-05,
      "loss": 0.878,
      "step": 7760
    },
    {
      "epoch": 1.0393605992709762,
      "grad_norm": 0.6769660115242004,
      "learning_rate": 3.267558528428094e-05,
      "loss": 0.883,
      "step": 7770
    },
    {
      "epoch": 1.0406982576998964,
      "grad_norm": 0.6419140100479126,
      "learning_rate": 3.265328874024526e-05,
      "loss": 0.8834,
      "step": 7780
    },
    {
      "epoch": 1.0420359161288164,
      "grad_norm": 0.7116323113441467,
      "learning_rate": 3.263099219620959e-05,
      "loss": 0.8651,
      "step": 7790
    },
    {
      "epoch": 1.0433735745577366,
      "grad_norm": 0.681542158126831,
      "learning_rate": 3.260869565217392e-05,
      "loss": 0.8735,
      "step": 7800
    },
    {
      "epoch": 1.0447112329866568,
      "grad_norm": 0.5820342898368835,
      "learning_rate": 3.258639910813824e-05,
      "loss": 0.8829,
      "step": 7810
    },
    {
      "epoch": 1.046048891415577,
      "grad_norm": 0.6191510558128357,
      "learning_rate": 3.2564102564102565e-05,
      "loss": 0.8768,
      "step": 7820
    },
    {
      "epoch": 1.0473865498444972,
      "grad_norm": 0.6246733069419861,
      "learning_rate": 3.254180602006689e-05,
      "loss": 0.8756,
      "step": 7830
    },
    {
      "epoch": 1.0487242082734174,
      "grad_norm": 0.6625968217849731,
      "learning_rate": 3.251950947603122e-05,
      "loss": 0.8836,
      "step": 7840
    },
    {
      "epoch": 1.0500618667023376,
      "grad_norm": 0.6560101509094238,
      "learning_rate": 3.249721293199554e-05,
      "loss": 0.8742,
      "step": 7850
    },
    {
      "epoch": 1.0513995251312578,
      "grad_norm": 0.718235433101654,
      "learning_rate": 3.247491638795987e-05,
      "loss": 0.8795,
      "step": 7860
    },
    {
      "epoch": 1.052737183560178,
      "grad_norm": 0.751288115978241,
      "learning_rate": 3.2452619843924194e-05,
      "loss": 0.8832,
      "step": 7870
    },
    {
      "epoch": 1.0540748419890982,
      "grad_norm": 0.6503592729568481,
      "learning_rate": 3.243032329988852e-05,
      "loss": 0.86,
      "step": 7880
    },
    {
      "epoch": 1.0554125004180182,
      "grad_norm": 0.625339925289154,
      "learning_rate": 3.240802675585284e-05,
      "loss": 0.8708,
      "step": 7890
    },
    {
      "epoch": 1.0567501588469383,
      "grad_norm": 0.6854523420333862,
      "learning_rate": 3.238573021181717e-05,
      "loss": 0.847,
      "step": 7900
    },
    {
      "epoch": 1.0580878172758585,
      "grad_norm": 0.6521021127700806,
      "learning_rate": 3.2363433667781497e-05,
      "loss": 0.8626,
      "step": 7910
    },
    {
      "epoch": 1.0594254757047787,
      "grad_norm": 0.6389915943145752,
      "learning_rate": 3.234113712374582e-05,
      "loss": 0.8617,
      "step": 7920
    },
    {
      "epoch": 1.060763134133699,
      "grad_norm": 0.7039439082145691,
      "learning_rate": 3.2318840579710144e-05,
      "loss": 0.87,
      "step": 7930
    },
    {
      "epoch": 1.0621007925626191,
      "grad_norm": 0.6778339147567749,
      "learning_rate": 3.229654403567447e-05,
      "loss": 0.8874,
      "step": 7940
    },
    {
      "epoch": 1.0634384509915393,
      "grad_norm": 0.6557786464691162,
      "learning_rate": 3.22742474916388e-05,
      "loss": 0.8279,
      "step": 7950
    },
    {
      "epoch": 1.0647761094204595,
      "grad_norm": 0.6524437069892883,
      "learning_rate": 3.225195094760312e-05,
      "loss": 0.8551,
      "step": 7960
    },
    {
      "epoch": 1.0661137678493797,
      "grad_norm": 0.6250905394554138,
      "learning_rate": 3.222965440356745e-05,
      "loss": 0.8738,
      "step": 7970
    },
    {
      "epoch": 1.0674514262783,
      "grad_norm": 0.6362579464912415,
      "learning_rate": 3.2207357859531774e-05,
      "loss": 0.8799,
      "step": 7980
    },
    {
      "epoch": 1.0687890847072201,
      "grad_norm": 0.679483950138092,
      "learning_rate": 3.21850613154961e-05,
      "loss": 0.8942,
      "step": 7990
    },
    {
      "epoch": 1.07012674313614,
      "grad_norm": 0.7064568400382996,
      "learning_rate": 3.216276477146042e-05,
      "loss": 0.8465,
      "step": 8000
    },
    {
      "epoch": 1.0714644015650603,
      "grad_norm": 0.6190669536590576,
      "learning_rate": 3.2140468227424756e-05,
      "loss": 0.8529,
      "step": 8010
    },
    {
      "epoch": 1.0728020599939805,
      "grad_norm": 0.6657054424285889,
      "learning_rate": 3.2118171683389076e-05,
      "loss": 0.8712,
      "step": 8020
    },
    {
      "epoch": 1.0741397184229007,
      "grad_norm": 0.7157071232795715,
      "learning_rate": 3.2095875139353403e-05,
      "loss": 0.8668,
      "step": 8030
    },
    {
      "epoch": 1.0754773768518209,
      "grad_norm": 0.6211431622505188,
      "learning_rate": 3.2073578595317724e-05,
      "loss": 0.8851,
      "step": 8040
    },
    {
      "epoch": 1.076815035280741,
      "grad_norm": 0.6440749168395996,
      "learning_rate": 3.205128205128206e-05,
      "loss": 0.8509,
      "step": 8050
    },
    {
      "epoch": 1.0781526937096613,
      "grad_norm": 0.7454267740249634,
      "learning_rate": 3.202898550724638e-05,
      "loss": 0.8692,
      "step": 8060
    },
    {
      "epoch": 1.0794903521385815,
      "grad_norm": 0.657012939453125,
      "learning_rate": 3.20066889632107e-05,
      "loss": 0.853,
      "step": 8070
    },
    {
      "epoch": 1.0808280105675017,
      "grad_norm": 0.703092634677887,
      "learning_rate": 3.198439241917503e-05,
      "loss": 0.8712,
      "step": 8080
    },
    {
      "epoch": 1.0821656689964219,
      "grad_norm": 0.6607069969177246,
      "learning_rate": 3.1962095875139354e-05,
      "loss": 0.8262,
      "step": 8090
    },
    {
      "epoch": 1.0835033274253418,
      "grad_norm": 0.6443659663200378,
      "learning_rate": 3.193979933110368e-05,
      "loss": 0.8743,
      "step": 8100
    },
    {
      "epoch": 1.084840985854262,
      "grad_norm": 0.6995377540588379,
      "learning_rate": 3.1917502787068e-05,
      "loss": 0.8779,
      "step": 8110
    },
    {
      "epoch": 1.0861786442831822,
      "grad_norm": 0.6320191621780396,
      "learning_rate": 3.1895206243032335e-05,
      "loss": 0.8372,
      "step": 8120
    },
    {
      "epoch": 1.0875163027121024,
      "grad_norm": 0.6518025994300842,
      "learning_rate": 3.1872909698996656e-05,
      "loss": 0.8797,
      "step": 8130
    },
    {
      "epoch": 1.0888539611410226,
      "grad_norm": 0.6771944165229797,
      "learning_rate": 3.185061315496098e-05,
      "loss": 0.8576,
      "step": 8140
    },
    {
      "epoch": 1.0901916195699428,
      "grad_norm": 0.6775611042976379,
      "learning_rate": 3.182831661092531e-05,
      "loss": 0.8693,
      "step": 8150
    },
    {
      "epoch": 1.091529277998863,
      "grad_norm": 0.7298684120178223,
      "learning_rate": 3.180602006688964e-05,
      "loss": 0.8837,
      "step": 8160
    },
    {
      "epoch": 1.0928669364277832,
      "grad_norm": 0.6982479691505432,
      "learning_rate": 3.178372352285396e-05,
      "loss": 0.8853,
      "step": 8170
    },
    {
      "epoch": 1.0942045948567034,
      "grad_norm": 0.6939826607704163,
      "learning_rate": 3.1761426978818285e-05,
      "loss": 0.8799,
      "step": 8180
    },
    {
      "epoch": 1.0955422532856236,
      "grad_norm": 0.6270095109939575,
      "learning_rate": 3.173913043478261e-05,
      "loss": 0.8687,
      "step": 8190
    },
    {
      "epoch": 1.0968799117145438,
      "grad_norm": 0.6984580159187317,
      "learning_rate": 3.171683389074693e-05,
      "loss": 0.8628,
      "step": 8200
    },
    {
      "epoch": 1.0982175701434638,
      "grad_norm": 0.6907698512077332,
      "learning_rate": 3.169453734671126e-05,
      "loss": 0.8678,
      "step": 8210
    },
    {
      "epoch": 1.099555228572384,
      "grad_norm": 0.6703968048095703,
      "learning_rate": 3.167224080267559e-05,
      "loss": 0.8573,
      "step": 8220
    },
    {
      "epoch": 1.1008928870013042,
      "grad_norm": 0.6892665028572083,
      "learning_rate": 3.1649944258639915e-05,
      "loss": 0.8678,
      "step": 8230
    },
    {
      "epoch": 1.1022305454302244,
      "grad_norm": 0.6262248158454895,
      "learning_rate": 3.1627647714604235e-05,
      "loss": 0.8602,
      "step": 8240
    },
    {
      "epoch": 1.1035682038591446,
      "grad_norm": 0.6614171266555786,
      "learning_rate": 3.160535117056856e-05,
      "loss": 0.8514,
      "step": 8250
    },
    {
      "epoch": 1.1049058622880648,
      "grad_norm": 0.6576712727546692,
      "learning_rate": 3.158305462653289e-05,
      "loss": 0.8328,
      "step": 8260
    },
    {
      "epoch": 1.106243520716985,
      "grad_norm": 0.6924079060554504,
      "learning_rate": 3.156075808249722e-05,
      "loss": 0.867,
      "step": 8270
    },
    {
      "epoch": 1.1075811791459051,
      "grad_norm": 0.635769784450531,
      "learning_rate": 3.153846153846154e-05,
      "loss": 0.8691,
      "step": 8280
    },
    {
      "epoch": 1.1089188375748253,
      "grad_norm": 0.6816710233688354,
      "learning_rate": 3.1516164994425865e-05,
      "loss": 0.8337,
      "step": 8290
    },
    {
      "epoch": 1.1102564960037455,
      "grad_norm": 0.6777461171150208,
      "learning_rate": 3.149386845039019e-05,
      "loss": 0.8631,
      "step": 8300
    },
    {
      "epoch": 1.1115941544326655,
      "grad_norm": 0.6441739201545715,
      "learning_rate": 3.147157190635452e-05,
      "loss": 0.8533,
      "step": 8310
    },
    {
      "epoch": 1.1129318128615857,
      "grad_norm": 0.6839208602905273,
      "learning_rate": 3.144927536231884e-05,
      "loss": 0.8799,
      "step": 8320
    },
    {
      "epoch": 1.114269471290506,
      "grad_norm": 0.7078185081481934,
      "learning_rate": 3.142697881828317e-05,
      "loss": 0.8786,
      "step": 8330
    },
    {
      "epoch": 1.115607129719426,
      "grad_norm": 0.6375734806060791,
      "learning_rate": 3.1404682274247495e-05,
      "loss": 0.862,
      "step": 8340
    },
    {
      "epoch": 1.1169447881483463,
      "grad_norm": 0.671449601650238,
      "learning_rate": 3.1382385730211815e-05,
      "loss": 0.861,
      "step": 8350
    },
    {
      "epoch": 1.1182824465772665,
      "grad_norm": 0.6714947819709778,
      "learning_rate": 3.136008918617614e-05,
      "loss": 0.8539,
      "step": 8360
    },
    {
      "epoch": 1.1196201050061867,
      "grad_norm": 0.7980797290802002,
      "learning_rate": 3.133779264214047e-05,
      "loss": 0.8851,
      "step": 8370
    },
    {
      "epoch": 1.120957763435107,
      "grad_norm": 0.6663002967834473,
      "learning_rate": 3.13154960981048e-05,
      "loss": 0.8731,
      "step": 8380
    },
    {
      "epoch": 1.122295421864027,
      "grad_norm": 0.6274700164794922,
      "learning_rate": 3.129319955406912e-05,
      "loss": 0.8651,
      "step": 8390
    },
    {
      "epoch": 1.1236330802929473,
      "grad_norm": 0.6536434888839722,
      "learning_rate": 3.1270903010033445e-05,
      "loss": 0.8591,
      "step": 8400
    },
    {
      "epoch": 1.1249707387218675,
      "grad_norm": 0.6660344004631042,
      "learning_rate": 3.124860646599777e-05,
      "loss": 0.8511,
      "step": 8410
    },
    {
      "epoch": 1.1263083971507875,
      "grad_norm": 0.6524666547775269,
      "learning_rate": 3.12263099219621e-05,
      "loss": 0.8849,
      "step": 8420
    },
    {
      "epoch": 1.1276460555797077,
      "grad_norm": 0.6452275514602661,
      "learning_rate": 3.120401337792642e-05,
      "loss": 0.868,
      "step": 8430
    },
    {
      "epoch": 1.1289837140086278,
      "grad_norm": 0.7467283606529236,
      "learning_rate": 3.1181716833890754e-05,
      "loss": 0.8889,
      "step": 8440
    },
    {
      "epoch": 1.130321372437548,
      "grad_norm": 0.6848856210708618,
      "learning_rate": 3.1159420289855074e-05,
      "loss": 0.8488,
      "step": 8450
    },
    {
      "epoch": 1.1316590308664682,
      "grad_norm": 0.6839979887008667,
      "learning_rate": 3.11371237458194e-05,
      "loss": 0.8756,
      "step": 8460
    },
    {
      "epoch": 1.1329966892953884,
      "grad_norm": 0.6743227243423462,
      "learning_rate": 3.111482720178372e-05,
      "loss": 0.8638,
      "step": 8470
    },
    {
      "epoch": 1.1343343477243086,
      "grad_norm": 0.6521905064582825,
      "learning_rate": 3.109253065774805e-05,
      "loss": 0.8492,
      "step": 8480
    },
    {
      "epoch": 1.1356720061532288,
      "grad_norm": 0.638797402381897,
      "learning_rate": 3.1070234113712377e-05,
      "loss": 0.8733,
      "step": 8490
    },
    {
      "epoch": 1.137009664582149,
      "grad_norm": 0.7307306528091431,
      "learning_rate": 3.10479375696767e-05,
      "loss": 0.8526,
      "step": 8500
    },
    {
      "epoch": 1.138347323011069,
      "grad_norm": 0.6354436874389648,
      "learning_rate": 3.102564102564103e-05,
      "loss": 0.8524,
      "step": 8510
    },
    {
      "epoch": 1.1396849814399892,
      "grad_norm": 0.6697414517402649,
      "learning_rate": 3.100334448160535e-05,
      "loss": 0.9048,
      "step": 8520
    },
    {
      "epoch": 1.1410226398689094,
      "grad_norm": 0.6835455894470215,
      "learning_rate": 3.098104793756968e-05,
      "loss": 0.8792,
      "step": 8530
    },
    {
      "epoch": 1.1423602982978296,
      "grad_norm": 0.6454207897186279,
      "learning_rate": 3.0958751393534e-05,
      "loss": 0.8425,
      "step": 8540
    },
    {
      "epoch": 1.1436979567267498,
      "grad_norm": 0.6081865429878235,
      "learning_rate": 3.0936454849498333e-05,
      "loss": 0.8494,
      "step": 8550
    },
    {
      "epoch": 1.14503561515567,
      "grad_norm": 0.6419368386268616,
      "learning_rate": 3.0914158305462654e-05,
      "loss": 0.8877,
      "step": 8560
    },
    {
      "epoch": 1.1463732735845902,
      "grad_norm": 0.6640701293945312,
      "learning_rate": 3.089186176142698e-05,
      "loss": 0.8358,
      "step": 8570
    },
    {
      "epoch": 1.1477109320135104,
      "grad_norm": 0.7222452163696289,
      "learning_rate": 3.086956521739131e-05,
      "loss": 0.8617,
      "step": 8580
    },
    {
      "epoch": 1.1490485904424306,
      "grad_norm": 0.6854059100151062,
      "learning_rate": 3.0847268673355636e-05,
      "loss": 0.8669,
      "step": 8590
    },
    {
      "epoch": 1.1503862488713508,
      "grad_norm": 0.6604175567626953,
      "learning_rate": 3.0824972129319956e-05,
      "loss": 0.8768,
      "step": 8600
    },
    {
      "epoch": 1.151723907300271,
      "grad_norm": 0.6608168482780457,
      "learning_rate": 3.080267558528428e-05,
      "loss": 0.852,
      "step": 8610
    },
    {
      "epoch": 1.1530615657291912,
      "grad_norm": 0.6129019856452942,
      "learning_rate": 3.078037904124861e-05,
      "loss": 0.8977,
      "step": 8620
    },
    {
      "epoch": 1.1543992241581111,
      "grad_norm": 0.6360752582550049,
      "learning_rate": 3.075808249721293e-05,
      "loss": 0.8862,
      "step": 8630
    },
    {
      "epoch": 1.1557368825870313,
      "grad_norm": 0.6644921898841858,
      "learning_rate": 3.073578595317726e-05,
      "loss": 0.8587,
      "step": 8640
    },
    {
      "epoch": 1.1570745410159515,
      "grad_norm": 0.7244711518287659,
      "learning_rate": 3.0713489409141586e-05,
      "loss": 0.8644,
      "step": 8650
    },
    {
      "epoch": 1.1584121994448717,
      "grad_norm": 0.6236187219619751,
      "learning_rate": 3.069119286510591e-05,
      "loss": 0.8547,
      "step": 8660
    },
    {
      "epoch": 1.159749857873792,
      "grad_norm": 0.6847409009933472,
      "learning_rate": 3.0668896321070234e-05,
      "loss": 0.8662,
      "step": 8670
    },
    {
      "epoch": 1.1610875163027121,
      "grad_norm": 0.6317407488822937,
      "learning_rate": 3.064659977703456e-05,
      "loss": 0.8857,
      "step": 8680
    },
    {
      "epoch": 1.1624251747316323,
      "grad_norm": 0.7250654101371765,
      "learning_rate": 3.062430323299889e-05,
      "loss": 0.8615,
      "step": 8690
    },
    {
      "epoch": 1.1637628331605525,
      "grad_norm": 0.6131876707077026,
      "learning_rate": 3.0602006688963215e-05,
      "loss": 0.8553,
      "step": 8700
    },
    {
      "epoch": 1.1651004915894727,
      "grad_norm": 0.6595548987388611,
      "learning_rate": 3.0579710144927536e-05,
      "loss": 0.8751,
      "step": 8710
    },
    {
      "epoch": 1.1664381500183927,
      "grad_norm": 0.6082146167755127,
      "learning_rate": 3.055741360089186e-05,
      "loss": 0.8725,
      "step": 8720
    },
    {
      "epoch": 1.1677758084473129,
      "grad_norm": 0.6563588380813599,
      "learning_rate": 3.053511705685619e-05,
      "loss": 0.8673,
      "step": 8730
    },
    {
      "epoch": 1.169113466876233,
      "grad_norm": 0.6713948249816895,
      "learning_rate": 3.0512820512820518e-05,
      "loss": 0.8987,
      "step": 8740
    },
    {
      "epoch": 1.1704511253051533,
      "grad_norm": 0.6043673157691956,
      "learning_rate": 3.049052396878484e-05,
      "loss": 0.8676,
      "step": 8750
    },
    {
      "epoch": 1.1717887837340735,
      "grad_norm": 0.6770358681678772,
      "learning_rate": 3.0468227424749162e-05,
      "loss": 0.8763,
      "step": 8760
    },
    {
      "epoch": 1.1731264421629937,
      "grad_norm": 0.7052449584007263,
      "learning_rate": 3.0445930880713493e-05,
      "loss": 0.8718,
      "step": 8770
    },
    {
      "epoch": 1.1744641005919139,
      "grad_norm": 0.6731158494949341,
      "learning_rate": 3.0423634336677813e-05,
      "loss": 0.8681,
      "step": 8780
    },
    {
      "epoch": 1.175801759020834,
      "grad_norm": 0.6999037861824036,
      "learning_rate": 3.0401337792642144e-05,
      "loss": 0.8821,
      "step": 8790
    },
    {
      "epoch": 1.1771394174497543,
      "grad_norm": 0.706273078918457,
      "learning_rate": 3.0379041248606464e-05,
      "loss": 0.8823,
      "step": 8800
    },
    {
      "epoch": 1.1784770758786745,
      "grad_norm": 0.6303936839103699,
      "learning_rate": 3.0356744704570795e-05,
      "loss": 0.8426,
      "step": 8810
    },
    {
      "epoch": 1.1798147343075946,
      "grad_norm": 0.6593167185783386,
      "learning_rate": 3.033444816053512e-05,
      "loss": 0.876,
      "step": 8820
    },
    {
      "epoch": 1.1811523927365148,
      "grad_norm": 0.6470351815223694,
      "learning_rate": 3.0312151616499446e-05,
      "loss": 0.853,
      "step": 8830
    },
    {
      "epoch": 1.1824900511654348,
      "grad_norm": 0.6432070732116699,
      "learning_rate": 3.028985507246377e-05,
      "loss": 0.876,
      "step": 8840
    },
    {
      "epoch": 1.183827709594355,
      "grad_norm": 0.6500959396362305,
      "learning_rate": 3.0267558528428097e-05,
      "loss": 0.8935,
      "step": 8850
    },
    {
      "epoch": 1.1851653680232752,
      "grad_norm": 0.7126473784446716,
      "learning_rate": 3.024526198439242e-05,
      "loss": 0.8757,
      "step": 8860
    },
    {
      "epoch": 1.1865030264521954,
      "grad_norm": 0.6926862001419067,
      "learning_rate": 3.022296544035675e-05,
      "loss": 0.8803,
      "step": 8870
    },
    {
      "epoch": 1.1878406848811156,
      "grad_norm": 0.6554896235466003,
      "learning_rate": 3.0200668896321072e-05,
      "loss": 0.8472,
      "step": 8880
    },
    {
      "epoch": 1.1891783433100358,
      "grad_norm": 0.6900491118431091,
      "learning_rate": 3.0178372352285396e-05,
      "loss": 0.8826,
      "step": 8890
    },
    {
      "epoch": 1.190516001738956,
      "grad_norm": 0.6881018877029419,
      "learning_rate": 3.0156075808249723e-05,
      "loss": 0.8759,
      "step": 8900
    },
    {
      "epoch": 1.1918536601678762,
      "grad_norm": 0.6393043994903564,
      "learning_rate": 3.0133779264214047e-05,
      "loss": 0.8625,
      "step": 8910
    },
    {
      "epoch": 1.1931913185967964,
      "grad_norm": 0.7073138356208801,
      "learning_rate": 3.0111482720178375e-05,
      "loss": 0.8541,
      "step": 8920
    },
    {
      "epoch": 1.1945289770257164,
      "grad_norm": 0.6549396514892578,
      "learning_rate": 3.00891861761427e-05,
      "loss": 0.8321,
      "step": 8930
    },
    {
      "epoch": 1.1958666354546366,
      "grad_norm": 0.7123689651489258,
      "learning_rate": 3.0066889632107026e-05,
      "loss": 0.8549,
      "step": 8940
    },
    {
      "epoch": 1.1972042938835568,
      "grad_norm": 0.7145563960075378,
      "learning_rate": 3.004459308807135e-05,
      "loss": 0.8665,
      "step": 8950
    },
    {
      "epoch": 1.198541952312477,
      "grad_norm": 0.6718332171440125,
      "learning_rate": 3.0022296544035677e-05,
      "loss": 0.8786,
      "step": 8960
    },
    {
      "epoch": 1.1998796107413972,
      "grad_norm": 0.6201209425926208,
      "learning_rate": 3e-05,
      "loss": 0.8791,
      "step": 8970
    },
    {
      "epoch": 1.2012172691703173,
      "grad_norm": 0.6661424040794373,
      "learning_rate": 2.9977703455964328e-05,
      "loss": 0.888,
      "step": 8980
    },
    {
      "epoch": 1.2025549275992375,
      "grad_norm": 0.6619938015937805,
      "learning_rate": 2.9955406911928652e-05,
      "loss": 0.8709,
      "step": 8990
    },
    {
      "epoch": 1.2038925860281577,
      "grad_norm": 0.6863569021224976,
      "learning_rate": 2.993311036789298e-05,
      "loss": 0.8633,
      "step": 9000
    },
    {
      "epoch": 1.205230244457078,
      "grad_norm": 0.6882833242416382,
      "learning_rate": 2.9910813823857303e-05,
      "loss": 0.878,
      "step": 9010
    },
    {
      "epoch": 1.2065679028859981,
      "grad_norm": 0.6760345101356506,
      "learning_rate": 2.988851727982163e-05,
      "loss": 0.868,
      "step": 9020
    },
    {
      "epoch": 1.2079055613149183,
      "grad_norm": 0.6517123579978943,
      "learning_rate": 2.9866220735785954e-05,
      "loss": 0.8685,
      "step": 9030
    },
    {
      "epoch": 1.2092432197438385,
      "grad_norm": 0.6413756608963013,
      "learning_rate": 2.9843924191750278e-05,
      "loss": 0.8525,
      "step": 9040
    },
    {
      "epoch": 1.2105808781727585,
      "grad_norm": 0.6445450782775879,
      "learning_rate": 2.9821627647714605e-05,
      "loss": 0.874,
      "step": 9050
    },
    {
      "epoch": 1.2119185366016787,
      "grad_norm": 0.6708728075027466,
      "learning_rate": 2.979933110367893e-05,
      "loss": 0.8806,
      "step": 9060
    },
    {
      "epoch": 1.213256195030599,
      "grad_norm": 0.6845903992652893,
      "learning_rate": 2.9777034559643257e-05,
      "loss": 0.8712,
      "step": 9070
    },
    {
      "epoch": 1.214593853459519,
      "grad_norm": 0.69368577003479,
      "learning_rate": 2.975473801560758e-05,
      "loss": 0.8604,
      "step": 9080
    },
    {
      "epoch": 1.2159315118884393,
      "grad_norm": 0.714318573474884,
      "learning_rate": 2.9732441471571908e-05,
      "loss": 0.8678,
      "step": 9090
    },
    {
      "epoch": 1.2172691703173595,
      "grad_norm": 0.6662846207618713,
      "learning_rate": 2.971014492753623e-05,
      "loss": 0.8705,
      "step": 9100
    },
    {
      "epoch": 1.2186068287462797,
      "grad_norm": 0.6494625210762024,
      "learning_rate": 2.9687848383500562e-05,
      "loss": 0.8684,
      "step": 9110
    },
    {
      "epoch": 1.2199444871751999,
      "grad_norm": 0.6143115162849426,
      "learning_rate": 2.9665551839464883e-05,
      "loss": 0.8777,
      "step": 9120
    },
    {
      "epoch": 1.22128214560412,
      "grad_norm": 0.615010142326355,
      "learning_rate": 2.9643255295429213e-05,
      "loss": 0.868,
      "step": 9130
    },
    {
      "epoch": 1.22261980403304,
      "grad_norm": 0.7331852316856384,
      "learning_rate": 2.9620958751393534e-05,
      "loss": 0.8906,
      "step": 9140
    },
    {
      "epoch": 1.2239574624619602,
      "grad_norm": 0.7023767232894897,
      "learning_rate": 2.9598662207357864e-05,
      "loss": 0.8828,
      "step": 9150
    },
    {
      "epoch": 1.2252951208908804,
      "grad_norm": 0.7159485816955566,
      "learning_rate": 2.9576365663322185e-05,
      "loss": 0.856,
      "step": 9160
    },
    {
      "epoch": 1.2266327793198006,
      "grad_norm": 0.6821110844612122,
      "learning_rate": 2.955406911928651e-05,
      "loss": 0.8345,
      "step": 9170
    },
    {
      "epoch": 1.2279704377487208,
      "grad_norm": 0.6539527773857117,
      "learning_rate": 2.953177257525084e-05,
      "loss": 0.8574,
      "step": 9180
    },
    {
      "epoch": 1.229308096177641,
      "grad_norm": 0.6775563359260559,
      "learning_rate": 2.950947603121516e-05,
      "loss": 0.8597,
      "step": 9190
    },
    {
      "epoch": 1.2306457546065612,
      "grad_norm": 0.6656110286712646,
      "learning_rate": 2.948717948717949e-05,
      "loss": 0.8571,
      "step": 9200
    },
    {
      "epoch": 1.2319834130354814,
      "grad_norm": 0.6840603947639465,
      "learning_rate": 2.946488294314381e-05,
      "loss": 0.8557,
      "step": 9210
    },
    {
      "epoch": 1.2333210714644016,
      "grad_norm": 0.6380136609077454,
      "learning_rate": 2.9442586399108142e-05,
      "loss": 0.8493,
      "step": 9220
    },
    {
      "epoch": 1.2346587298933218,
      "grad_norm": 0.6261102557182312,
      "learning_rate": 2.9420289855072462e-05,
      "loss": 0.9079,
      "step": 9230
    },
    {
      "epoch": 1.235996388322242,
      "grad_norm": 0.6487118005752563,
      "learning_rate": 2.9397993311036793e-05,
      "loss": 0.8498,
      "step": 9240
    },
    {
      "epoch": 1.2373340467511622,
      "grad_norm": 0.7465745210647583,
      "learning_rate": 2.9375696767001117e-05,
      "loss": 0.8587,
      "step": 9250
    },
    {
      "epoch": 1.2386717051800822,
      "grad_norm": 0.6364240050315857,
      "learning_rate": 2.9353400222965444e-05,
      "loss": 0.8376,
      "step": 9260
    },
    {
      "epoch": 1.2400093636090024,
      "grad_norm": 0.6346690654754639,
      "learning_rate": 2.9331103678929768e-05,
      "loss": 0.9028,
      "step": 9270
    },
    {
      "epoch": 1.2413470220379226,
      "grad_norm": 0.6146904230117798,
      "learning_rate": 2.9308807134894095e-05,
      "loss": 0.8307,
      "step": 9280
    },
    {
      "epoch": 1.2426846804668428,
      "grad_norm": 0.6418126225471497,
      "learning_rate": 2.928651059085842e-05,
      "loss": 0.8534,
      "step": 9290
    },
    {
      "epoch": 1.244022338895763,
      "grad_norm": 0.580601692199707,
      "learning_rate": 2.9264214046822746e-05,
      "loss": 0.8889,
      "step": 9300
    },
    {
      "epoch": 1.2453599973246832,
      "grad_norm": 0.662838339805603,
      "learning_rate": 2.924191750278707e-05,
      "loss": 0.8414,
      "step": 9310
    },
    {
      "epoch": 1.2466976557536034,
      "grad_norm": 0.614548921585083,
      "learning_rate": 2.921962095875139e-05,
      "loss": 0.8764,
      "step": 9320
    },
    {
      "epoch": 1.2480353141825236,
      "grad_norm": 0.6503021717071533,
      "learning_rate": 2.919732441471572e-05,
      "loss": 0.8868,
      "step": 9330
    },
    {
      "epoch": 1.2493729726114438,
      "grad_norm": 0.6685996651649475,
      "learning_rate": 2.9175027870680045e-05,
      "loss": 0.8818,
      "step": 9340
    },
    {
      "epoch": 1.2507106310403637,
      "grad_norm": 0.6834791302680969,
      "learning_rate": 2.9152731326644373e-05,
      "loss": 0.8584,
      "step": 9350
    },
    {
      "epoch": 1.252048289469284,
      "grad_norm": 0.6673011183738708,
      "learning_rate": 2.9130434782608696e-05,
      "loss": 0.8718,
      "step": 9360
    },
    {
      "epoch": 1.2533859478982041,
      "grad_norm": 0.620945394039154,
      "learning_rate": 2.9108138238573024e-05,
      "loss": 0.8712,
      "step": 9370
    },
    {
      "epoch": 1.2547236063271243,
      "grad_norm": 0.7093073129653931,
      "learning_rate": 2.9085841694537348e-05,
      "loss": 0.8513,
      "step": 9380
    },
    {
      "epoch": 1.2560612647560445,
      "grad_norm": 0.7669113278388977,
      "learning_rate": 2.9063545150501675e-05,
      "loss": 0.8799,
      "step": 9390
    },
    {
      "epoch": 1.2573989231849647,
      "grad_norm": 0.7223570942878723,
      "learning_rate": 2.9041248606466e-05,
      "loss": 0.8996,
      "step": 9400
    },
    {
      "epoch": 1.258736581613885,
      "grad_norm": 0.6705424189567566,
      "learning_rate": 2.9018952062430326e-05,
      "loss": 0.9071,
      "step": 9410
    },
    {
      "epoch": 1.260074240042805,
      "grad_norm": 0.733871340751648,
      "learning_rate": 2.899665551839465e-05,
      "loss": 0.8783,
      "step": 9420
    },
    {
      "epoch": 1.2614118984717253,
      "grad_norm": 0.7027148008346558,
      "learning_rate": 2.8974358974358977e-05,
      "loss": 0.8461,
      "step": 9430
    },
    {
      "epoch": 1.2627495569006455,
      "grad_norm": 0.6642775535583496,
      "learning_rate": 2.89520624303233e-05,
      "loss": 0.8828,
      "step": 9440
    },
    {
      "epoch": 1.2640872153295657,
      "grad_norm": 0.6668092012405396,
      "learning_rate": 2.8929765886287625e-05,
      "loss": 0.8871,
      "step": 9450
    },
    {
      "epoch": 1.265424873758486,
      "grad_norm": 0.7339465618133545,
      "learning_rate": 2.8907469342251952e-05,
      "loss": 0.8627,
      "step": 9460
    },
    {
      "epoch": 1.2667625321874059,
      "grad_norm": 0.7008122205734253,
      "learning_rate": 2.8885172798216276e-05,
      "loss": 0.863,
      "step": 9470
    },
    {
      "epoch": 1.268100190616326,
      "grad_norm": 0.7126266360282898,
      "learning_rate": 2.8862876254180603e-05,
      "loss": 0.8729,
      "step": 9480
    },
    {
      "epoch": 1.2694378490452463,
      "grad_norm": 0.7108739018440247,
      "learning_rate": 2.8840579710144927e-05,
      "loss": 0.8574,
      "step": 9490
    },
    {
      "epoch": 1.2707755074741665,
      "grad_norm": 0.659752607345581,
      "learning_rate": 2.8818283166109255e-05,
      "loss": 0.8641,
      "step": 9500
    },
    {
      "epoch": 1.2721131659030867,
      "grad_norm": 0.6139850616455078,
      "learning_rate": 2.879598662207358e-05,
      "loss": 0.8541,
      "step": 9510
    },
    {
      "epoch": 1.2734508243320068,
      "grad_norm": 0.6779764890670776,
      "learning_rate": 2.8773690078037906e-05,
      "loss": 0.8562,
      "step": 9520
    },
    {
      "epoch": 1.274788482760927,
      "grad_norm": 0.6907296776771545,
      "learning_rate": 2.875139353400223e-05,
      "loss": 0.8699,
      "step": 9530
    },
    {
      "epoch": 1.2761261411898472,
      "grad_norm": 0.6819544434547424,
      "learning_rate": 2.8729096989966557e-05,
      "loss": 0.8746,
      "step": 9540
    },
    {
      "epoch": 1.2774637996187672,
      "grad_norm": 0.7868484854698181,
      "learning_rate": 2.870680044593088e-05,
      "loss": 0.8833,
      "step": 9550
    },
    {
      "epoch": 1.2788014580476874,
      "grad_norm": 0.6705886125564575,
      "learning_rate": 2.868450390189521e-05,
      "loss": 0.8632,
      "step": 9560
    },
    {
      "epoch": 1.2801391164766076,
      "grad_norm": 0.6253829002380371,
      "learning_rate": 2.8662207357859532e-05,
      "loss": 0.8701,
      "step": 9570
    },
    {
      "epoch": 1.2814767749055278,
      "grad_norm": 0.6716762185096741,
      "learning_rate": 2.8639910813823863e-05,
      "loss": 0.8757,
      "step": 9580
    },
    {
      "epoch": 1.282814433334448,
      "grad_norm": 0.682202160358429,
      "learning_rate": 2.8617614269788183e-05,
      "loss": 0.8579,
      "step": 9590
    },
    {
      "epoch": 1.2841520917633682,
      "grad_norm": 0.6873485445976257,
      "learning_rate": 2.8595317725752507e-05,
      "loss": 0.8882,
      "step": 9600
    },
    {
      "epoch": 1.2854897501922884,
      "grad_norm": 0.6423261761665344,
      "learning_rate": 2.8573021181716834e-05,
      "loss": 0.8412,
      "step": 9610
    },
    {
      "epoch": 1.2868274086212086,
      "grad_norm": 0.6540555953979492,
      "learning_rate": 2.8550724637681158e-05,
      "loss": 0.857,
      "step": 9620
    },
    {
      "epoch": 1.2881650670501288,
      "grad_norm": 0.7073960900306702,
      "learning_rate": 2.852842809364549e-05,
      "loss": 0.8734,
      "step": 9630
    },
    {
      "epoch": 1.289502725479049,
      "grad_norm": 0.6600174307823181,
      "learning_rate": 2.850613154960981e-05,
      "loss": 0.8455,
      "step": 9640
    },
    {
      "epoch": 1.2908403839079692,
      "grad_norm": 0.6639668345451355,
      "learning_rate": 2.848383500557414e-05,
      "loss": 0.8402,
      "step": 9650
    },
    {
      "epoch": 1.2921780423368894,
      "grad_norm": 0.6758043169975281,
      "learning_rate": 2.846153846153846e-05,
      "loss": 0.885,
      "step": 9660
    },
    {
      "epoch": 1.2935157007658096,
      "grad_norm": 0.6914310455322266,
      "learning_rate": 2.843924191750279e-05,
      "loss": 0.8641,
      "step": 9670
    },
    {
      "epoch": 1.2948533591947295,
      "grad_norm": 0.6299353241920471,
      "learning_rate": 2.841694537346711e-05,
      "loss": 0.8438,
      "step": 9680
    },
    {
      "epoch": 1.2961910176236497,
      "grad_norm": 0.7144021987915039,
      "learning_rate": 2.8394648829431442e-05,
      "loss": 0.8509,
      "step": 9690
    },
    {
      "epoch": 1.29752867605257,
      "grad_norm": 0.6589462161064148,
      "learning_rate": 2.8372352285395766e-05,
      "loss": 0.8424,
      "step": 9700
    },
    {
      "epoch": 1.2988663344814901,
      "grad_norm": 0.7637476921081543,
      "learning_rate": 2.8350055741360093e-05,
      "loss": 0.8671,
      "step": 9710
    },
    {
      "epoch": 1.3002039929104103,
      "grad_norm": 0.6213951110839844,
      "learning_rate": 2.8327759197324417e-05,
      "loss": 0.8591,
      "step": 9720
    },
    {
      "epoch": 1.3015416513393305,
      "grad_norm": 0.6519984602928162,
      "learning_rate": 2.8305462653288738e-05,
      "loss": 0.8687,
      "step": 9730
    },
    {
      "epoch": 1.3028793097682507,
      "grad_norm": 0.719133734703064,
      "learning_rate": 2.828316610925307e-05,
      "loss": 0.8642,
      "step": 9740
    },
    {
      "epoch": 1.304216968197171,
      "grad_norm": 0.6922861337661743,
      "learning_rate": 2.826086956521739e-05,
      "loss": 0.8834,
      "step": 9750
    },
    {
      "epoch": 1.305554626626091,
      "grad_norm": 0.6393188834190369,
      "learning_rate": 2.823857302118172e-05,
      "loss": 0.8517,
      "step": 9760
    },
    {
      "epoch": 1.306892285055011,
      "grad_norm": 0.6976654529571533,
      "learning_rate": 2.8216276477146043e-05,
      "loss": 0.8764,
      "step": 9770
    },
    {
      "epoch": 1.3082299434839313,
      "grad_norm": 0.627177357673645,
      "learning_rate": 2.819397993311037e-05,
      "loss": 0.8435,
      "step": 9780
    },
    {
      "epoch": 1.3095676019128515,
      "grad_norm": 0.8012517094612122,
      "learning_rate": 2.8171683389074695e-05,
      "loss": 0.8603,
      "step": 9790
    },
    {
      "epoch": 1.3109052603417717,
      "grad_norm": 0.6347975730895996,
      "learning_rate": 2.8149386845039022e-05,
      "loss": 0.8937,
      "step": 9800
    },
    {
      "epoch": 1.3122429187706919,
      "grad_norm": 0.6460016965866089,
      "learning_rate": 2.8127090301003346e-05,
      "loss": 0.865,
      "step": 9810
    },
    {
      "epoch": 1.313580577199612,
      "grad_norm": 0.6434261798858643,
      "learning_rate": 2.8104793756967673e-05,
      "loss": 0.8719,
      "step": 9820
    },
    {
      "epoch": 1.3149182356285323,
      "grad_norm": 0.6342810988426208,
      "learning_rate": 2.8082497212931997e-05,
      "loss": 0.8891,
      "step": 9830
    },
    {
      "epoch": 1.3162558940574525,
      "grad_norm": 0.7236226201057434,
      "learning_rate": 2.8060200668896324e-05,
      "loss": 0.8432,
      "step": 9840
    },
    {
      "epoch": 1.3175935524863727,
      "grad_norm": 0.6385544538497925,
      "learning_rate": 2.8037904124860648e-05,
      "loss": 0.8502,
      "step": 9850
    },
    {
      "epoch": 1.3189312109152929,
      "grad_norm": 0.6508695483207703,
      "learning_rate": 2.8015607580824972e-05,
      "loss": 0.8384,
      "step": 9860
    },
    {
      "epoch": 1.320268869344213,
      "grad_norm": 0.6393324136734009,
      "learning_rate": 2.79933110367893e-05,
      "loss": 0.862,
      "step": 9870
    },
    {
      "epoch": 1.3216065277731333,
      "grad_norm": 0.6672255992889404,
      "learning_rate": 2.7971014492753623e-05,
      "loss": 0.8612,
      "step": 9880
    },
    {
      "epoch": 1.3229441862020532,
      "grad_norm": 0.6963310837745667,
      "learning_rate": 2.794871794871795e-05,
      "loss": 0.8883,
      "step": 9890
    },
    {
      "epoch": 1.3242818446309734,
      "grad_norm": 0.6324018836021423,
      "learning_rate": 2.7926421404682274e-05,
      "loss": 0.8713,
      "step": 9900
    },
    {
      "epoch": 1.3256195030598936,
      "grad_norm": 0.6951503753662109,
      "learning_rate": 2.79041248606466e-05,
      "loss": 0.8538,
      "step": 9910
    },
    {
      "epoch": 1.3269571614888138,
      "grad_norm": 0.5938833951950073,
      "learning_rate": 2.7881828316610925e-05,
      "loss": 0.868,
      "step": 9920
    },
    {
      "epoch": 1.328294819917734,
      "grad_norm": 0.6355523467063904,
      "learning_rate": 2.7859531772575253e-05,
      "loss": 0.8615,
      "step": 9930
    },
    {
      "epoch": 1.3296324783466542,
      "grad_norm": 0.639529824256897,
      "learning_rate": 2.7837235228539576e-05,
      "loss": 0.8593,
      "step": 9940
    },
    {
      "epoch": 1.3309701367755744,
      "grad_norm": 0.6445759534835815,
      "learning_rate": 2.7814938684503904e-05,
      "loss": 0.8488,
      "step": 9950
    },
    {
      "epoch": 1.3323077952044946,
      "grad_norm": 0.6404306292533875,
      "learning_rate": 2.7792642140468228e-05,
      "loss": 0.852,
      "step": 9960
    },
    {
      "epoch": 1.3336454536334146,
      "grad_norm": 0.660948634147644,
      "learning_rate": 2.7770345596432555e-05,
      "loss": 0.8542,
      "step": 9970
    },
    {
      "epoch": 1.3349831120623348,
      "grad_norm": 0.6295641660690308,
      "learning_rate": 2.774804905239688e-05,
      "loss": 0.8883,
      "step": 9980
    },
    {
      "epoch": 1.336320770491255,
      "grad_norm": 0.634563684463501,
      "learning_rate": 2.772575250836121e-05,
      "loss": 0.8487,
      "step": 9990
    },
    {
      "epoch": 1.3376584289201752,
      "grad_norm": 0.6703812479972839,
      "learning_rate": 2.770345596432553e-05,
      "loss": 0.8854,
      "step": 10000
    },
    {
      "epoch": 1.3389960873490954,
      "grad_norm": 0.6408697962760925,
      "learning_rate": 2.7681159420289854e-05,
      "loss": 0.8594,
      "step": 10010
    },
    {
      "epoch": 1.3403337457780156,
      "grad_norm": 0.6085620522499084,
      "learning_rate": 2.765886287625418e-05,
      "loss": 0.8366,
      "step": 10020
    },
    {
      "epoch": 1.3416714042069358,
      "grad_norm": 0.7038812041282654,
      "learning_rate": 2.7636566332218505e-05,
      "loss": 0.9015,
      "step": 10030
    },
    {
      "epoch": 1.343009062635856,
      "grad_norm": 0.6934060454368591,
      "learning_rate": 2.7614269788182832e-05,
      "loss": 0.869,
      "step": 10040
    },
    {
      "epoch": 1.3443467210647762,
      "grad_norm": 0.6660127639770508,
      "learning_rate": 2.7591973244147156e-05,
      "loss": 0.8643,
      "step": 10050
    },
    {
      "epoch": 1.3456843794936963,
      "grad_norm": 0.7508851289749146,
      "learning_rate": 2.7569676700111487e-05,
      "loss": 0.8648,
      "step": 10060
    },
    {
      "epoch": 1.3470220379226165,
      "grad_norm": 0.6040788888931274,
      "learning_rate": 2.7547380156075807e-05,
      "loss": 0.8803,
      "step": 10070
    },
    {
      "epoch": 1.3483596963515367,
      "grad_norm": 0.676217257976532,
      "learning_rate": 2.7525083612040138e-05,
      "loss": 0.8472,
      "step": 10080
    },
    {
      "epoch": 1.349697354780457,
      "grad_norm": 0.5963518023490906,
      "learning_rate": 2.750278706800446e-05,
      "loss": 0.8738,
      "step": 10090
    },
    {
      "epoch": 1.351035013209377,
      "grad_norm": 0.7247270941734314,
      "learning_rate": 2.748049052396879e-05,
      "loss": 0.8787,
      "step": 10100
    },
    {
      "epoch": 1.352372671638297,
      "grad_norm": 0.6202956438064575,
      "learning_rate": 2.745819397993311e-05,
      "loss": 0.8728,
      "step": 10110
    },
    {
      "epoch": 1.3537103300672173,
      "grad_norm": 0.6271024346351624,
      "learning_rate": 2.743589743589744e-05,
      "loss": 0.8481,
      "step": 10120
    },
    {
      "epoch": 1.3550479884961375,
      "grad_norm": 0.6503884196281433,
      "learning_rate": 2.7413600891861764e-05,
      "loss": 0.8648,
      "step": 10130
    },
    {
      "epoch": 1.3563856469250577,
      "grad_norm": 0.6738189458847046,
      "learning_rate": 2.7391304347826085e-05,
      "loss": 0.86,
      "step": 10140
    },
    {
      "epoch": 1.357723305353978,
      "grad_norm": 0.6549925208091736,
      "learning_rate": 2.7369007803790415e-05,
      "loss": 0.8353,
      "step": 10150
    },
    {
      "epoch": 1.359060963782898,
      "grad_norm": 0.683695912361145,
      "learning_rate": 2.7346711259754736e-05,
      "loss": 0.8741,
      "step": 10160
    },
    {
      "epoch": 1.3603986222118183,
      "grad_norm": 0.6702660918235779,
      "learning_rate": 2.7324414715719066e-05,
      "loss": 0.861,
      "step": 10170
    },
    {
      "epoch": 1.3617362806407383,
      "grad_norm": 0.6660727858543396,
      "learning_rate": 2.7302118171683387e-05,
      "loss": 0.8899,
      "step": 10180
    },
    {
      "epoch": 1.3630739390696585,
      "grad_norm": 0.6123337149620056,
      "learning_rate": 2.7279821627647718e-05,
      "loss": 0.8554,
      "step": 10190
    },
    {
      "epoch": 1.3644115974985787,
      "grad_norm": 0.6460930109024048,
      "learning_rate": 2.725752508361204e-05,
      "loss": 0.8637,
      "step": 10200
    },
    {
      "epoch": 1.3657492559274989,
      "grad_norm": 0.643277108669281,
      "learning_rate": 2.723522853957637e-05,
      "loss": 0.8717,
      "step": 10210
    },
    {
      "epoch": 1.367086914356419,
      "grad_norm": 0.6046974658966064,
      "learning_rate": 2.7212931995540693e-05,
      "loss": 0.8456,
      "step": 10220
    },
    {
      "epoch": 1.3684245727853392,
      "grad_norm": 0.7115755677223206,
      "learning_rate": 2.719063545150502e-05,
      "loss": 0.8664,
      "step": 10230
    },
    {
      "epoch": 1.3697622312142594,
      "grad_norm": 0.6777831315994263,
      "learning_rate": 2.7168338907469344e-05,
      "loss": 0.8682,
      "step": 10240
    },
    {
      "epoch": 1.3710998896431796,
      "grad_norm": 0.6682579517364502,
      "learning_rate": 2.714604236343367e-05,
      "loss": 0.8666,
      "step": 10250
    },
    {
      "epoch": 1.3724375480720998,
      "grad_norm": 0.6062295436859131,
      "learning_rate": 2.7123745819397995e-05,
      "loss": 0.8674,
      "step": 10260
    },
    {
      "epoch": 1.37377520650102,
      "grad_norm": 0.6750136613845825,
      "learning_rate": 2.7101449275362322e-05,
      "loss": 0.8694,
      "step": 10270
    },
    {
      "epoch": 1.3751128649299402,
      "grad_norm": 0.7356799244880676,
      "learning_rate": 2.7079152731326646e-05,
      "loss": 0.8504,
      "step": 10280
    },
    {
      "epoch": 1.3764505233588604,
      "grad_norm": 0.6526396870613098,
      "learning_rate": 2.705685618729097e-05,
      "loss": 0.8556,
      "step": 10290
    },
    {
      "epoch": 1.3777881817877806,
      "grad_norm": 0.629520058631897,
      "learning_rate": 2.7034559643255297e-05,
      "loss": 0.8789,
      "step": 10300
    },
    {
      "epoch": 1.3791258402167006,
      "grad_norm": 0.7062785029411316,
      "learning_rate": 2.701226309921962e-05,
      "loss": 0.8613,
      "step": 10310
    },
    {
      "epoch": 1.3804634986456208,
      "grad_norm": 0.6615326404571533,
      "learning_rate": 2.6989966555183948e-05,
      "loss": 0.8431,
      "step": 10320
    },
    {
      "epoch": 1.381801157074541,
      "grad_norm": 0.6673943996429443,
      "learning_rate": 2.6967670011148272e-05,
      "loss": 0.8689,
      "step": 10330
    },
    {
      "epoch": 1.3831388155034612,
      "grad_norm": 0.648536741733551,
      "learning_rate": 2.69453734671126e-05,
      "loss": 0.8682,
      "step": 10340
    },
    {
      "epoch": 1.3844764739323814,
      "grad_norm": 0.6626381874084473,
      "learning_rate": 2.6923076923076923e-05,
      "loss": 0.8535,
      "step": 10350
    },
    {
      "epoch": 1.3858141323613016,
      "grad_norm": 0.643872082233429,
      "learning_rate": 2.690078037904125e-05,
      "loss": 0.857,
      "step": 10360
    },
    {
      "epoch": 1.3871517907902218,
      "grad_norm": 0.6403111219406128,
      "learning_rate": 2.6878483835005574e-05,
      "loss": 0.8493,
      "step": 10370
    },
    {
      "epoch": 1.388489449219142,
      "grad_norm": 0.6721373200416565,
      "learning_rate": 2.6856187290969902e-05,
      "loss": 0.8663,
      "step": 10380
    },
    {
      "epoch": 1.389827107648062,
      "grad_norm": 0.7514716386795044,
      "learning_rate": 2.6833890746934226e-05,
      "loss": 0.8692,
      "step": 10390
    },
    {
      "epoch": 1.3911647660769821,
      "grad_norm": 0.700829267501831,
      "learning_rate": 2.6811594202898553e-05,
      "loss": 0.8621,
      "step": 10400
    },
    {
      "epoch": 1.3925024245059023,
      "grad_norm": 0.7082879543304443,
      "learning_rate": 2.6789297658862877e-05,
      "loss": 0.868,
      "step": 10410
    },
    {
      "epoch": 1.3938400829348225,
      "grad_norm": 0.6703070998191833,
      "learning_rate": 2.67670011148272e-05,
      "loss": 0.8373,
      "step": 10420
    },
    {
      "epoch": 1.3951777413637427,
      "grad_norm": 0.674324095249176,
      "learning_rate": 2.6744704570791528e-05,
      "loss": 0.871,
      "step": 10430
    },
    {
      "epoch": 1.396515399792663,
      "grad_norm": 0.6730296015739441,
      "learning_rate": 2.6722408026755852e-05,
      "loss": 0.8768,
      "step": 10440
    },
    {
      "epoch": 1.3978530582215831,
      "grad_norm": 0.6109100580215454,
      "learning_rate": 2.670011148272018e-05,
      "loss": 0.8528,
      "step": 10450
    },
    {
      "epoch": 1.3991907166505033,
      "grad_norm": 0.6966679096221924,
      "learning_rate": 2.6677814938684503e-05,
      "loss": 0.8636,
      "step": 10460
    },
    {
      "epoch": 1.4005283750794235,
      "grad_norm": 0.6764125227928162,
      "learning_rate": 2.665551839464883e-05,
      "loss": 0.8641,
      "step": 10470
    },
    {
      "epoch": 1.4018660335083437,
      "grad_norm": 0.6849717497825623,
      "learning_rate": 2.6633221850613154e-05,
      "loss": 0.8518,
      "step": 10480
    },
    {
      "epoch": 1.403203691937264,
      "grad_norm": 0.6544718742370605,
      "learning_rate": 2.6610925306577485e-05,
      "loss": 0.852,
      "step": 10490
    },
    {
      "epoch": 1.404541350366184,
      "grad_norm": 0.6549354195594788,
      "learning_rate": 2.6588628762541805e-05,
      "loss": 0.8681,
      "step": 10500
    },
    {
      "epoch": 1.4058790087951043,
      "grad_norm": 0.5972777009010315,
      "learning_rate": 2.6566332218506136e-05,
      "loss": 0.8459,
      "step": 10510
    },
    {
      "epoch": 1.4072166672240243,
      "grad_norm": 0.6669219136238098,
      "learning_rate": 2.6544035674470456e-05,
      "loss": 0.8508,
      "step": 10520
    },
    {
      "epoch": 1.4085543256529445,
      "grad_norm": 0.6295815706253052,
      "learning_rate": 2.6521739130434787e-05,
      "loss": 0.8815,
      "step": 10530
    },
    {
      "epoch": 1.4098919840818647,
      "grad_norm": 0.6071106791496277,
      "learning_rate": 2.6499442586399108e-05,
      "loss": 0.854,
      "step": 10540
    },
    {
      "epoch": 1.4112296425107849,
      "grad_norm": 0.7046390771865845,
      "learning_rate": 2.6477146042363438e-05,
      "loss": 0.8835,
      "step": 10550
    },
    {
      "epoch": 1.412567300939705,
      "grad_norm": 0.63355553150177,
      "learning_rate": 2.6454849498327762e-05,
      "loss": 0.876,
      "step": 10560
    },
    {
      "epoch": 1.4139049593686253,
      "grad_norm": 0.7035483717918396,
      "learning_rate": 2.6432552954292083e-05,
      "loss": 0.8643,
      "step": 10570
    },
    {
      "epoch": 1.4152426177975455,
      "grad_norm": 0.6859781742095947,
      "learning_rate": 2.6410256410256413e-05,
      "loss": 0.8621,
      "step": 10580
    },
    {
      "epoch": 1.4165802762264657,
      "grad_norm": 0.6798073053359985,
      "learning_rate": 2.6387959866220734e-05,
      "loss": 0.8578,
      "step": 10590
    },
    {
      "epoch": 1.4179179346553856,
      "grad_norm": 0.6430560350418091,
      "learning_rate": 2.6365663322185064e-05,
      "loss": 0.8785,
      "step": 10600
    },
    {
      "epoch": 1.4192555930843058,
      "grad_norm": 0.6212288737297058,
      "learning_rate": 2.6343366778149385e-05,
      "loss": 0.8485,
      "step": 10610
    },
    {
      "epoch": 1.420593251513226,
      "grad_norm": 0.6818721294403076,
      "learning_rate": 2.6321070234113716e-05,
      "loss": 0.8771,
      "step": 10620
    },
    {
      "epoch": 1.4219309099421462,
      "grad_norm": 0.6118592619895935,
      "learning_rate": 2.629877369007804e-05,
      "loss": 0.8466,
      "step": 10630
    },
    {
      "epoch": 1.4232685683710664,
      "grad_norm": 0.6796302795410156,
      "learning_rate": 2.6276477146042367e-05,
      "loss": 0.8615,
      "step": 10640
    },
    {
      "epoch": 1.4246062267999866,
      "grad_norm": 0.6521089673042297,
      "learning_rate": 2.625418060200669e-05,
      "loss": 0.8672,
      "step": 10650
    },
    {
      "epoch": 1.4259438852289068,
      "grad_norm": 0.6932123899459839,
      "learning_rate": 2.6231884057971018e-05,
      "loss": 0.8581,
      "step": 10660
    },
    {
      "epoch": 1.427281543657827,
      "grad_norm": 0.6653109788894653,
      "learning_rate": 2.6209587513935342e-05,
      "loss": 0.8542,
      "step": 10670
    },
    {
      "epoch": 1.4286192020867472,
      "grad_norm": 0.6801559925079346,
      "learning_rate": 2.618729096989967e-05,
      "loss": 0.87,
      "step": 10680
    },
    {
      "epoch": 1.4299568605156674,
      "grad_norm": 0.6338356137275696,
      "learning_rate": 2.6164994425863993e-05,
      "loss": 0.8482,
      "step": 10690
    },
    {
      "epoch": 1.4312945189445876,
      "grad_norm": 0.6756296157836914,
      "learning_rate": 2.6142697881828317e-05,
      "loss": 0.8711,
      "step": 10700
    },
    {
      "epoch": 1.4326321773735078,
      "grad_norm": 0.6223725080490112,
      "learning_rate": 2.6120401337792644e-05,
      "loss": 0.8897,
      "step": 10710
    },
    {
      "epoch": 1.433969835802428,
      "grad_norm": 0.6738623976707458,
      "learning_rate": 2.6098104793756968e-05,
      "loss": 0.8633,
      "step": 10720
    },
    {
      "epoch": 1.435307494231348,
      "grad_norm": 0.7094070911407471,
      "learning_rate": 2.6075808249721295e-05,
      "loss": 0.8803,
      "step": 10730
    },
    {
      "epoch": 1.4366451526602682,
      "grad_norm": 0.6151424050331116,
      "learning_rate": 2.605351170568562e-05,
      "loss": 0.8417,
      "step": 10740
    },
    {
      "epoch": 1.4379828110891884,
      "grad_norm": 0.7005527019500732,
      "learning_rate": 2.6031215161649946e-05,
      "loss": 0.8594,
      "step": 10750
    },
    {
      "epoch": 1.4393204695181085,
      "grad_norm": 0.5943439602851868,
      "learning_rate": 2.600891861761427e-05,
      "loss": 0.8429,
      "step": 10760
    },
    {
      "epoch": 1.4406581279470287,
      "grad_norm": 0.6626438498497009,
      "learning_rate": 2.5986622073578597e-05,
      "loss": 0.8538,
      "step": 10770
    },
    {
      "epoch": 1.441995786375949,
      "grad_norm": 0.6724194288253784,
      "learning_rate": 2.596432552954292e-05,
      "loss": 0.8625,
      "step": 10780
    },
    {
      "epoch": 1.4433334448048691,
      "grad_norm": 0.7216963171958923,
      "learning_rate": 2.594202898550725e-05,
      "loss": 0.832,
      "step": 10790
    },
    {
      "epoch": 1.4446711032337893,
      "grad_norm": 0.7585955858230591,
      "learning_rate": 2.5919732441471573e-05,
      "loss": 0.8713,
      "step": 10800
    },
    {
      "epoch": 1.4460087616627093,
      "grad_norm": 0.65943843126297,
      "learning_rate": 2.58974358974359e-05,
      "loss": 0.8599,
      "step": 10810
    },
    {
      "epoch": 1.4473464200916295,
      "grad_norm": 0.9096161127090454,
      "learning_rate": 2.5875139353400224e-05,
      "loss": 0.8675,
      "step": 10820
    },
    {
      "epoch": 1.4486840785205497,
      "grad_norm": 0.6829474568367004,
      "learning_rate": 2.585284280936455e-05,
      "loss": 0.8699,
      "step": 10830
    },
    {
      "epoch": 1.45002173694947,
      "grad_norm": 0.5970133543014526,
      "learning_rate": 2.5830546265328875e-05,
      "loss": 0.8541,
      "step": 10840
    },
    {
      "epoch": 1.45135939537839,
      "grad_norm": 0.7113572359085083,
      "learning_rate": 2.58082497212932e-05,
      "loss": 0.8596,
      "step": 10850
    },
    {
      "epoch": 1.4526970538073103,
      "grad_norm": 0.6366267204284668,
      "learning_rate": 2.5785953177257526e-05,
      "loss": 0.8725,
      "step": 10860
    },
    {
      "epoch": 1.4540347122362305,
      "grad_norm": 0.667048990726471,
      "learning_rate": 2.576365663322185e-05,
      "loss": 0.8513,
      "step": 10870
    },
    {
      "epoch": 1.4553723706651507,
      "grad_norm": 0.8667017817497253,
      "learning_rate": 2.5741360089186177e-05,
      "loss": 0.868,
      "step": 10880
    },
    {
      "epoch": 1.4567100290940709,
      "grad_norm": 0.6898093819618225,
      "learning_rate": 2.57190635451505e-05,
      "loss": 0.8549,
      "step": 10890
    },
    {
      "epoch": 1.458047687522991,
      "grad_norm": 0.6831161379814148,
      "learning_rate": 2.5696767001114828e-05,
      "loss": 0.8648,
      "step": 10900
    },
    {
      "epoch": 1.4593853459519113,
      "grad_norm": 0.6949218511581421,
      "learning_rate": 2.5674470457079152e-05,
      "loss": 0.8659,
      "step": 10910
    },
    {
      "epoch": 1.4607230043808315,
      "grad_norm": 0.6750102639198303,
      "learning_rate": 2.5652173913043483e-05,
      "loss": 0.8832,
      "step": 10920
    },
    {
      "epoch": 1.4620606628097517,
      "grad_norm": 0.61276775598526,
      "learning_rate": 2.5629877369007803e-05,
      "loss": 0.856,
      "step": 10930
    },
    {
      "epoch": 1.4633983212386716,
      "grad_norm": 0.642905592918396,
      "learning_rate": 2.5607580824972134e-05,
      "loss": 0.8776,
      "step": 10940
    },
    {
      "epoch": 1.4647359796675918,
      "grad_norm": 0.6197250485420227,
      "learning_rate": 2.5585284280936454e-05,
      "loss": 0.8653,
      "step": 10950
    },
    {
      "epoch": 1.466073638096512,
      "grad_norm": 0.6495434045791626,
      "learning_rate": 2.5562987736900785e-05,
      "loss": 0.8678,
      "step": 10960
    },
    {
      "epoch": 1.4674112965254322,
      "grad_norm": 0.6276832222938538,
      "learning_rate": 2.5540691192865106e-05,
      "loss": 0.8707,
      "step": 10970
    },
    {
      "epoch": 1.4687489549543524,
      "grad_norm": 0.6526870131492615,
      "learning_rate": 2.551839464882943e-05,
      "loss": 0.8594,
      "step": 10980
    },
    {
      "epoch": 1.4700866133832726,
      "grad_norm": 0.6513860821723938,
      "learning_rate": 2.549609810479376e-05,
      "loss": 0.8489,
      "step": 10990
    },
    {
      "epoch": 1.4714242718121928,
      "grad_norm": 0.6383877992630005,
      "learning_rate": 2.547380156075808e-05,
      "loss": 0.9278,
      "step": 11000
    },
    {
      "epoch": 1.4727619302411128,
      "grad_norm": 0.6447703242301941,
      "learning_rate": 2.545150501672241e-05,
      "loss": 0.8813,
      "step": 11010
    },
    {
      "epoch": 1.474099588670033,
      "grad_norm": 0.6242958903312683,
      "learning_rate": 2.5429208472686732e-05,
      "loss": 0.8515,
      "step": 11020
    },
    {
      "epoch": 1.4754372470989532,
      "grad_norm": 0.6682453155517578,
      "learning_rate": 2.5406911928651062e-05,
      "loss": 0.873,
      "step": 11030
    },
    {
      "epoch": 1.4767749055278734,
      "grad_norm": 0.7004200220108032,
      "learning_rate": 2.5384615384615383e-05,
      "loss": 0.8737,
      "step": 11040
    },
    {
      "epoch": 1.4781125639567936,
      "grad_norm": 0.6421642899513245,
      "learning_rate": 2.5362318840579714e-05,
      "loss": 0.8669,
      "step": 11050
    },
    {
      "epoch": 1.4794502223857138,
      "grad_norm": 0.6445274949073792,
      "learning_rate": 2.5340022296544037e-05,
      "loss": 0.851,
      "step": 11060
    },
    {
      "epoch": 1.480787880814634,
      "grad_norm": 0.6492639183998108,
      "learning_rate": 2.5317725752508365e-05,
      "loss": 0.8634,
      "step": 11070
    },
    {
      "epoch": 1.4821255392435542,
      "grad_norm": 0.6584739685058594,
      "learning_rate": 2.529542920847269e-05,
      "loss": 0.8757,
      "step": 11080
    },
    {
      "epoch": 1.4834631976724744,
      "grad_norm": 0.6598654985427856,
      "learning_rate": 2.5273132664437016e-05,
      "loss": 0.8513,
      "step": 11090
    },
    {
      "epoch": 1.4848008561013946,
      "grad_norm": 0.6753244400024414,
      "learning_rate": 2.525083612040134e-05,
      "loss": 0.875,
      "step": 11100
    },
    {
      "epoch": 1.4861385145303148,
      "grad_norm": 0.7295107245445251,
      "learning_rate": 2.522853957636566e-05,
      "loss": 0.8529,
      "step": 11110
    },
    {
      "epoch": 1.487476172959235,
      "grad_norm": 0.6282659769058228,
      "learning_rate": 2.520624303232999e-05,
      "loss": 0.8527,
      "step": 11120
    },
    {
      "epoch": 1.4888138313881552,
      "grad_norm": 0.6804162263870239,
      "learning_rate": 2.5183946488294315e-05,
      "loss": 0.868,
      "step": 11130
    },
    {
      "epoch": 1.4901514898170753,
      "grad_norm": 0.6141003966331482,
      "learning_rate": 2.5161649944258642e-05,
      "loss": 0.8617,
      "step": 11140
    },
    {
      "epoch": 1.4914891482459953,
      "grad_norm": 0.7222961187362671,
      "learning_rate": 2.5139353400222966e-05,
      "loss": 0.8872,
      "step": 11150
    },
    {
      "epoch": 1.4928268066749155,
      "grad_norm": 0.6530949473381042,
      "learning_rate": 2.5117056856187293e-05,
      "loss": 0.8736,
      "step": 11160
    },
    {
      "epoch": 1.4941644651038357,
      "grad_norm": 0.6425726413726807,
      "learning_rate": 2.5094760312151617e-05,
      "loss": 0.8577,
      "step": 11170
    },
    {
      "epoch": 1.495502123532756,
      "grad_norm": 0.6396917700767517,
      "learning_rate": 2.5072463768115944e-05,
      "loss": 0.8827,
      "step": 11180
    },
    {
      "epoch": 1.496839781961676,
      "grad_norm": 0.6099726557731628,
      "learning_rate": 2.5050167224080268e-05,
      "loss": 0.8879,
      "step": 11190
    },
    {
      "epoch": 1.4981774403905963,
      "grad_norm": 0.6341944336891174,
      "learning_rate": 2.5027870680044595e-05,
      "loss": 0.8701,
      "step": 11200
    },
    {
      "epoch": 1.4995150988195165,
      "grad_norm": 0.714976966381073,
      "learning_rate": 2.500557413600892e-05,
      "loss": 0.8475,
      "step": 11210
    },
    {
      "epoch": 1.5008527572484365,
      "grad_norm": 0.6498013138771057,
      "learning_rate": 2.4983277591973243e-05,
      "loss": 0.8656,
      "step": 11220
    },
    {
      "epoch": 1.5021904156773567,
      "grad_norm": 0.6703901886940002,
      "learning_rate": 2.496098104793757e-05,
      "loss": 0.8945,
      "step": 11230
    },
    {
      "epoch": 1.5035280741062769,
      "grad_norm": 0.6334692239761353,
      "learning_rate": 2.4938684503901898e-05,
      "loss": 0.8756,
      "step": 11240
    },
    {
      "epoch": 1.504865732535197,
      "grad_norm": 0.7087447047233582,
      "learning_rate": 2.491638795986622e-05,
      "loss": 0.8369,
      "step": 11250
    },
    {
      "epoch": 1.5062033909641173,
      "grad_norm": 0.7113161683082581,
      "learning_rate": 2.489409141583055e-05,
      "loss": 0.8868,
      "step": 11260
    },
    {
      "epoch": 1.5075410493930375,
      "grad_norm": 0.6704371571540833,
      "learning_rate": 2.4871794871794873e-05,
      "loss": 0.8721,
      "step": 11270
    },
    {
      "epoch": 1.5088787078219577,
      "grad_norm": 0.6361607909202576,
      "learning_rate": 2.48494983277592e-05,
      "loss": 0.8562,
      "step": 11280
    },
    {
      "epoch": 1.5102163662508779,
      "grad_norm": 0.6105776429176331,
      "learning_rate": 2.4827201783723524e-05,
      "loss": 0.8797,
      "step": 11290
    },
    {
      "epoch": 1.511554024679798,
      "grad_norm": 0.6375657916069031,
      "learning_rate": 2.480490523968785e-05,
      "loss": 0.853,
      "step": 11300
    },
    {
      "epoch": 1.5128916831087182,
      "grad_norm": 0.691980242729187,
      "learning_rate": 2.4782608695652175e-05,
      "loss": 0.8677,
      "step": 11310
    },
    {
      "epoch": 1.5142293415376384,
      "grad_norm": 0.6992663145065308,
      "learning_rate": 2.47603121516165e-05,
      "loss": 0.8627,
      "step": 11320
    },
    {
      "epoch": 1.5155669999665586,
      "grad_norm": 0.725088357925415,
      "learning_rate": 2.4738015607580826e-05,
      "loss": 0.8584,
      "step": 11330
    },
    {
      "epoch": 1.5169046583954788,
      "grad_norm": 0.6007336378097534,
      "learning_rate": 2.471571906354515e-05,
      "loss": 0.8749,
      "step": 11340
    },
    {
      "epoch": 1.518242316824399,
      "grad_norm": 0.6503906846046448,
      "learning_rate": 2.4693422519509477e-05,
      "loss": 0.8891,
      "step": 11350
    },
    {
      "epoch": 1.5195799752533192,
      "grad_norm": 0.7194754481315613,
      "learning_rate": 2.46711259754738e-05,
      "loss": 0.8747,
      "step": 11360
    },
    {
      "epoch": 1.5209176336822392,
      "grad_norm": 0.6172993779182434,
      "learning_rate": 2.464882943143813e-05,
      "loss": 0.8726,
      "step": 11370
    },
    {
      "epoch": 1.5222552921111594,
      "grad_norm": 0.6344271898269653,
      "learning_rate": 2.4626532887402452e-05,
      "loss": 0.8745,
      "step": 11380
    },
    {
      "epoch": 1.5235929505400796,
      "grad_norm": 0.6414950489997864,
      "learning_rate": 2.460423634336678e-05,
      "loss": 0.8755,
      "step": 11390
    },
    {
      "epoch": 1.5249306089689998,
      "grad_norm": 0.6248849034309387,
      "learning_rate": 2.4581939799331104e-05,
      "loss": 0.8665,
      "step": 11400
    },
    {
      "epoch": 1.52626826739792,
      "grad_norm": 0.5813246965408325,
      "learning_rate": 2.455964325529543e-05,
      "loss": 0.8456,
      "step": 11410
    },
    {
      "epoch": 1.52760592582684,
      "grad_norm": 0.6348872780799866,
      "learning_rate": 2.4537346711259758e-05,
      "loss": 0.8726,
      "step": 11420
    },
    {
      "epoch": 1.5289435842557602,
      "grad_norm": 0.7328654527664185,
      "learning_rate": 2.4515050167224082e-05,
      "loss": 0.8747,
      "step": 11430
    },
    {
      "epoch": 1.5302812426846804,
      "grad_norm": 0.7735117077827454,
      "learning_rate": 2.449275362318841e-05,
      "loss": 0.8608,
      "step": 11440
    },
    {
      "epoch": 1.5316189011136006,
      "grad_norm": 0.6562034487724304,
      "learning_rate": 2.4470457079152733e-05,
      "loss": 0.8613,
      "step": 11450
    },
    {
      "epoch": 1.5329565595425207,
      "grad_norm": 0.6512119770050049,
      "learning_rate": 2.4448160535117057e-05,
      "loss": 0.8705,
      "step": 11460
    },
    {
      "epoch": 1.534294217971441,
      "grad_norm": 0.6828389763832092,
      "learning_rate": 2.442586399108138e-05,
      "loss": 0.8749,
      "step": 11470
    },
    {
      "epoch": 1.5356318764003611,
      "grad_norm": 0.6536902785301208,
      "learning_rate": 2.4403567447045708e-05,
      "loss": 0.8702,
      "step": 11480
    },
    {
      "epoch": 1.5369695348292813,
      "grad_norm": 0.6715162992477417,
      "learning_rate": 2.4381270903010035e-05,
      "loss": 0.8736,
      "step": 11490
    },
    {
      "epoch": 1.5383071932582015,
      "grad_norm": 0.6384007930755615,
      "learning_rate": 2.435897435897436e-05,
      "loss": 0.8746,
      "step": 11500
    },
    {
      "epoch": 1.5396448516871217,
      "grad_norm": 0.6700693964958191,
      "learning_rate": 2.4336677814938687e-05,
      "loss": 0.8718,
      "step": 11510
    },
    {
      "epoch": 1.540982510116042,
      "grad_norm": 0.643363356590271,
      "learning_rate": 2.431438127090301e-05,
      "loss": 0.8432,
      "step": 11520
    },
    {
      "epoch": 1.5423201685449621,
      "grad_norm": 0.6328558325767517,
      "learning_rate": 2.4292084726867338e-05,
      "loss": 0.8622,
      "step": 11530
    },
    {
      "epoch": 1.5436578269738823,
      "grad_norm": 0.6543498635292053,
      "learning_rate": 2.426978818283166e-05,
      "loss": 0.8627,
      "step": 11540
    },
    {
      "epoch": 1.5449954854028025,
      "grad_norm": 0.7048279047012329,
      "learning_rate": 2.424749163879599e-05,
      "loss": 0.8715,
      "step": 11550
    },
    {
      "epoch": 1.5463331438317227,
      "grad_norm": 0.6109763383865356,
      "learning_rate": 2.4225195094760313e-05,
      "loss": 0.8607,
      "step": 11560
    },
    {
      "epoch": 1.547670802260643,
      "grad_norm": 0.6051744222640991,
      "learning_rate": 2.420289855072464e-05,
      "loss": 0.8812,
      "step": 11570
    },
    {
      "epoch": 1.5490084606895629,
      "grad_norm": 0.6867024302482605,
      "learning_rate": 2.4180602006688964e-05,
      "loss": 0.8739,
      "step": 11580
    },
    {
      "epoch": 1.550346119118483,
      "grad_norm": 0.6638216972351074,
      "learning_rate": 2.415830546265329e-05,
      "loss": 0.8848,
      "step": 11590
    },
    {
      "epoch": 1.5516837775474033,
      "grad_norm": 0.6972298622131348,
      "learning_rate": 2.4136008918617615e-05,
      "loss": 0.8455,
      "step": 11600
    },
    {
      "epoch": 1.5530214359763235,
      "grad_norm": 0.6392918229103088,
      "learning_rate": 2.411371237458194e-05,
      "loss": 0.8832,
      "step": 11610
    },
    {
      "epoch": 1.5543590944052437,
      "grad_norm": 0.6754636764526367,
      "learning_rate": 2.4091415830546266e-05,
      "loss": 0.8727,
      "step": 11620
    },
    {
      "epoch": 1.5556967528341636,
      "grad_norm": 0.6449739933013916,
      "learning_rate": 2.406911928651059e-05,
      "loss": 0.8592,
      "step": 11630
    },
    {
      "epoch": 1.5570344112630838,
      "grad_norm": 0.7010354399681091,
      "learning_rate": 2.4046822742474917e-05,
      "loss": 0.8794,
      "step": 11640
    },
    {
      "epoch": 1.558372069692004,
      "grad_norm": 0.6435742378234863,
      "learning_rate": 2.402452619843924e-05,
      "loss": 0.8637,
      "step": 11650
    },
    {
      "epoch": 1.5597097281209242,
      "grad_norm": 0.5968673825263977,
      "learning_rate": 2.400222965440357e-05,
      "loss": 0.8608,
      "step": 11660
    },
    {
      "epoch": 1.5610473865498444,
      "grad_norm": 0.7628459930419922,
      "learning_rate": 2.3979933110367896e-05,
      "loss": 0.888,
      "step": 11670
    },
    {
      "epoch": 1.5623850449787646,
      "grad_norm": 0.6452785134315491,
      "learning_rate": 2.395763656633222e-05,
      "loss": 0.8747,
      "step": 11680
    },
    {
      "epoch": 1.5637227034076848,
      "grad_norm": 0.6791155934333801,
      "learning_rate": 2.3935340022296547e-05,
      "loss": 0.8642,
      "step": 11690
    },
    {
      "epoch": 1.565060361836605,
      "grad_norm": 0.6266025304794312,
      "learning_rate": 2.391304347826087e-05,
      "loss": 0.8833,
      "step": 11700
    },
    {
      "epoch": 1.5663980202655252,
      "grad_norm": 0.6731223464012146,
      "learning_rate": 2.3890746934225198e-05,
      "loss": 0.8609,
      "step": 11710
    },
    {
      "epoch": 1.5677356786944454,
      "grad_norm": 0.6862613558769226,
      "learning_rate": 2.3868450390189522e-05,
      "loss": 0.8627,
      "step": 11720
    },
    {
      "epoch": 1.5690733371233656,
      "grad_norm": 0.6474347114562988,
      "learning_rate": 2.384615384615385e-05,
      "loss": 0.8646,
      "step": 11730
    },
    {
      "epoch": 1.5704109955522858,
      "grad_norm": 0.6399881839752197,
      "learning_rate": 2.3823857302118173e-05,
      "loss": 0.861,
      "step": 11740
    },
    {
      "epoch": 1.571748653981206,
      "grad_norm": 0.6786615252494812,
      "learning_rate": 2.3801560758082497e-05,
      "loss": 0.8545,
      "step": 11750
    },
    {
      "epoch": 1.5730863124101262,
      "grad_norm": 0.6073382496833801,
      "learning_rate": 2.3779264214046824e-05,
      "loss": 0.861,
      "step": 11760
    },
    {
      "epoch": 1.5744239708390464,
      "grad_norm": 0.6352263689041138,
      "learning_rate": 2.3756967670011148e-05,
      "loss": 0.857,
      "step": 11770
    },
    {
      "epoch": 1.5757616292679666,
      "grad_norm": 0.628675103187561,
      "learning_rate": 2.3734671125975475e-05,
      "loss": 0.859,
      "step": 11780
    },
    {
      "epoch": 1.5770992876968866,
      "grad_norm": 0.6623336672782898,
      "learning_rate": 2.37123745819398e-05,
      "loss": 0.849,
      "step": 11790
    },
    {
      "epoch": 1.5784369461258068,
      "grad_norm": 0.6741111874580383,
      "learning_rate": 2.3690078037904127e-05,
      "loss": 0.8744,
      "step": 11800
    },
    {
      "epoch": 1.579774604554727,
      "grad_norm": 0.6771042943000793,
      "learning_rate": 2.366778149386845e-05,
      "loss": 0.8998,
      "step": 11810
    },
    {
      "epoch": 1.5811122629836472,
      "grad_norm": 0.6466215252876282,
      "learning_rate": 2.3645484949832778e-05,
      "loss": 0.8566,
      "step": 11820
    },
    {
      "epoch": 1.5824499214125674,
      "grad_norm": 0.6451229453086853,
      "learning_rate": 2.36231884057971e-05,
      "loss": 0.843,
      "step": 11830
    },
    {
      "epoch": 1.5837875798414873,
      "grad_norm": 0.6477819681167603,
      "learning_rate": 2.360089186176143e-05,
      "loss": 0.8954,
      "step": 11840
    },
    {
      "epoch": 1.5851252382704075,
      "grad_norm": 0.6213256120681763,
      "learning_rate": 2.3578595317725756e-05,
      "loss": 0.8979,
      "step": 11850
    },
    {
      "epoch": 1.5864628966993277,
      "grad_norm": 0.7260134220123291,
      "learning_rate": 2.355629877369008e-05,
      "loss": 0.8598,
      "step": 11860
    },
    {
      "epoch": 1.587800555128248,
      "grad_norm": 0.7260795831680298,
      "learning_rate": 2.3534002229654407e-05,
      "loss": 0.8503,
      "step": 11870
    },
    {
      "epoch": 1.5891382135571681,
      "grad_norm": 0.6927421689033508,
      "learning_rate": 2.3511705685618728e-05,
      "loss": 0.8853,
      "step": 11880
    },
    {
      "epoch": 1.5904758719860883,
      "grad_norm": 0.6152214407920837,
      "learning_rate": 2.3489409141583055e-05,
      "loss": 0.8925,
      "step": 11890
    },
    {
      "epoch": 1.5918135304150085,
      "grad_norm": 0.6974706649780273,
      "learning_rate": 2.346711259754738e-05,
      "loss": 0.8578,
      "step": 11900
    },
    {
      "epoch": 1.5931511888439287,
      "grad_norm": 0.6949673295021057,
      "learning_rate": 2.3444816053511706e-05,
      "loss": 0.839,
      "step": 11910
    },
    {
      "epoch": 1.594488847272849,
      "grad_norm": 0.6175399422645569,
      "learning_rate": 2.3422519509476034e-05,
      "loss": 0.854,
      "step": 11920
    },
    {
      "epoch": 1.595826505701769,
      "grad_norm": 0.6310625076293945,
      "learning_rate": 2.3400222965440357e-05,
      "loss": 0.863,
      "step": 11930
    },
    {
      "epoch": 1.5971641641306893,
      "grad_norm": 0.6200588941574097,
      "learning_rate": 2.3377926421404685e-05,
      "loss": 0.8394,
      "step": 11940
    },
    {
      "epoch": 1.5985018225596095,
      "grad_norm": 0.685468852519989,
      "learning_rate": 2.335562987736901e-05,
      "loss": 0.8702,
      "step": 11950
    },
    {
      "epoch": 1.5998394809885297,
      "grad_norm": 0.7233327031135559,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.8605,
      "step": 11960
    },
    {
      "epoch": 1.6011771394174499,
      "grad_norm": 0.6383735537528992,
      "learning_rate": 2.331103678929766e-05,
      "loss": 0.88,
      "step": 11970
    },
    {
      "epoch": 1.60251479784637,
      "grad_norm": 0.668890118598938,
      "learning_rate": 2.3288740245261987e-05,
      "loss": 0.8508,
      "step": 11980
    },
    {
      "epoch": 1.60385245627529,
      "grad_norm": 0.6592408418655396,
      "learning_rate": 2.326644370122631e-05,
      "loss": 0.8681,
      "step": 11990
    },
    {
      "epoch": 1.6051901147042102,
      "grad_norm": 0.7240493297576904,
      "learning_rate": 2.3244147157190638e-05,
      "loss": 0.8886,
      "step": 12000
    },
    {
      "epoch": 1.6065277731331304,
      "grad_norm": 0.6779526472091675,
      "learning_rate": 2.3221850613154962e-05,
      "loss": 0.8696,
      "step": 12010
    },
    {
      "epoch": 1.6078654315620506,
      "grad_norm": 0.7663412094116211,
      "learning_rate": 2.3199554069119286e-05,
      "loss": 0.8708,
      "step": 12020
    },
    {
      "epoch": 1.6092030899909708,
      "grad_norm": 0.7029663920402527,
      "learning_rate": 2.3177257525083613e-05,
      "loss": 0.849,
      "step": 12030
    },
    {
      "epoch": 1.610540748419891,
      "grad_norm": 0.768028736114502,
      "learning_rate": 2.3154960981047937e-05,
      "loss": 0.8541,
      "step": 12040
    },
    {
      "epoch": 1.611878406848811,
      "grad_norm": 0.6546469926834106,
      "learning_rate": 2.3132664437012264e-05,
      "loss": 0.883,
      "step": 12050
    },
    {
      "epoch": 1.6132160652777312,
      "grad_norm": 0.5926589965820312,
      "learning_rate": 2.3110367892976588e-05,
      "loss": 0.8532,
      "step": 12060
    },
    {
      "epoch": 1.6145537237066514,
      "grad_norm": 0.6416486501693726,
      "learning_rate": 2.3088071348940915e-05,
      "loss": 0.8809,
      "step": 12070
    },
    {
      "epoch": 1.6158913821355716,
      "grad_norm": 0.6579166054725647,
      "learning_rate": 2.306577480490524e-05,
      "loss": 0.8744,
      "step": 12080
    },
    {
      "epoch": 1.6172290405644918,
      "grad_norm": 0.5948958992958069,
      "learning_rate": 2.3043478260869567e-05,
      "loss": 0.8434,
      "step": 12090
    },
    {
      "epoch": 1.618566698993412,
      "grad_norm": 0.6970032453536987,
      "learning_rate": 2.3021181716833894e-05,
      "loss": 0.8403,
      "step": 12100
    },
    {
      "epoch": 1.6199043574223322,
      "grad_norm": 0.6602076888084412,
      "learning_rate": 2.2998885172798218e-05,
      "loss": 0.8819,
      "step": 12110
    },
    {
      "epoch": 1.6212420158512524,
      "grad_norm": 0.639562726020813,
      "learning_rate": 2.2976588628762545e-05,
      "loss": 0.8266,
      "step": 12120
    },
    {
      "epoch": 1.6225796742801726,
      "grad_norm": 0.602847695350647,
      "learning_rate": 2.295429208472687e-05,
      "loss": 0.8656,
      "step": 12130
    },
    {
      "epoch": 1.6239173327090928,
      "grad_norm": 0.747962236404419,
      "learning_rate": 2.2931995540691196e-05,
      "loss": 0.8694,
      "step": 12140
    },
    {
      "epoch": 1.625254991138013,
      "grad_norm": 0.7206100225448608,
      "learning_rate": 2.2909698996655517e-05,
      "loss": 0.8885,
      "step": 12150
    },
    {
      "epoch": 1.6265926495669332,
      "grad_norm": 0.6767705678939819,
      "learning_rate": 2.2887402452619844e-05,
      "loss": 0.8503,
      "step": 12160
    },
    {
      "epoch": 1.6279303079958534,
      "grad_norm": 0.6817945241928101,
      "learning_rate": 2.286510590858417e-05,
      "loss": 0.8517,
      "step": 12170
    },
    {
      "epoch": 1.6292679664247736,
      "grad_norm": 0.6533694863319397,
      "learning_rate": 2.2842809364548495e-05,
      "loss": 0.9001,
      "step": 12180
    },
    {
      "epoch": 1.6306056248536938,
      "grad_norm": 0.6281678676605225,
      "learning_rate": 2.2820512820512822e-05,
      "loss": 0.8459,
      "step": 12190
    },
    {
      "epoch": 1.6319432832826137,
      "grad_norm": 0.7091767191886902,
      "learning_rate": 2.2798216276477146e-05,
      "loss": 0.8782,
      "step": 12200
    },
    {
      "epoch": 1.633280941711534,
      "grad_norm": 0.676625669002533,
      "learning_rate": 2.2775919732441473e-05,
      "loss": 0.8325,
      "step": 12210
    },
    {
      "epoch": 1.6346186001404541,
      "grad_norm": 0.662551760673523,
      "learning_rate": 2.2753623188405797e-05,
      "loss": 0.8752,
      "step": 12220
    },
    {
      "epoch": 1.6359562585693743,
      "grad_norm": 0.6533100605010986,
      "learning_rate": 2.2731326644370125e-05,
      "loss": 0.8573,
      "step": 12230
    },
    {
      "epoch": 1.6372939169982945,
      "grad_norm": 0.7559541463851929,
      "learning_rate": 2.270903010033445e-05,
      "loss": 0.8797,
      "step": 12240
    },
    {
      "epoch": 1.6386315754272147,
      "grad_norm": 0.6190677881240845,
      "learning_rate": 2.2686733556298776e-05,
      "loss": 0.8458,
      "step": 12250
    },
    {
      "epoch": 1.6399692338561347,
      "grad_norm": 0.6257892847061157,
      "learning_rate": 2.26644370122631e-05,
      "loss": 0.8361,
      "step": 12260
    },
    {
      "epoch": 1.6413068922850549,
      "grad_norm": 0.6681759357452393,
      "learning_rate": 2.2642140468227427e-05,
      "loss": 0.8589,
      "step": 12270
    },
    {
      "epoch": 1.642644550713975,
      "grad_norm": 0.7088903188705444,
      "learning_rate": 2.2619843924191754e-05,
      "loss": 0.8778,
      "step": 12280
    },
    {
      "epoch": 1.6439822091428953,
      "grad_norm": 0.6735780239105225,
      "learning_rate": 2.2597547380156075e-05,
      "loss": 0.8385,
      "step": 12290
    },
    {
      "epoch": 1.6453198675718155,
      "grad_norm": 0.6188080906867981,
      "learning_rate": 2.2575250836120402e-05,
      "loss": 0.8431,
      "step": 12300
    },
    {
      "epoch": 1.6466575260007357,
      "grad_norm": 0.6204162240028381,
      "learning_rate": 2.2552954292084726e-05,
      "loss": 0.8776,
      "step": 12310
    },
    {
      "epoch": 1.6479951844296559,
      "grad_norm": 0.716182291507721,
      "learning_rate": 2.2530657748049053e-05,
      "loss": 0.904,
      "step": 12320
    },
    {
      "epoch": 1.649332842858576,
      "grad_norm": 0.6538474559783936,
      "learning_rate": 2.2508361204013377e-05,
      "loss": 0.8455,
      "step": 12330
    },
    {
      "epoch": 1.6506705012874963,
      "grad_norm": 0.5948954224586487,
      "learning_rate": 2.2486064659977704e-05,
      "loss": 0.8744,
      "step": 12340
    },
    {
      "epoch": 1.6520081597164165,
      "grad_norm": 0.6487875580787659,
      "learning_rate": 2.246376811594203e-05,
      "loss": 0.8627,
      "step": 12350
    },
    {
      "epoch": 1.6533458181453367,
      "grad_norm": 0.684395968914032,
      "learning_rate": 2.2441471571906355e-05,
      "loss": 0.8772,
      "step": 12360
    },
    {
      "epoch": 1.6546834765742569,
      "grad_norm": 0.6535452604293823,
      "learning_rate": 2.2419175027870683e-05,
      "loss": 0.8716,
      "step": 12370
    },
    {
      "epoch": 1.656021135003177,
      "grad_norm": 0.6402643322944641,
      "learning_rate": 2.2396878483835007e-05,
      "loss": 0.8846,
      "step": 12380
    },
    {
      "epoch": 1.6573587934320972,
      "grad_norm": 0.6866879463195801,
      "learning_rate": 2.2374581939799334e-05,
      "loss": 0.8574,
      "step": 12390
    },
    {
      "epoch": 1.6586964518610174,
      "grad_norm": 0.698590874671936,
      "learning_rate": 2.2352285395763658e-05,
      "loss": 0.8446,
      "step": 12400
    },
    {
      "epoch": 1.6600341102899374,
      "grad_norm": 0.6144953966140747,
      "learning_rate": 2.2329988851727985e-05,
      "loss": 0.8705,
      "step": 12410
    },
    {
      "epoch": 1.6613717687188576,
      "grad_norm": 0.7022151947021484,
      "learning_rate": 2.230769230769231e-05,
      "loss": 0.8897,
      "step": 12420
    },
    {
      "epoch": 1.6627094271477778,
      "grad_norm": 0.743813693523407,
      "learning_rate": 2.2285395763656633e-05,
      "loss": 0.8432,
      "step": 12430
    },
    {
      "epoch": 1.664047085576698,
      "grad_norm": 0.723680853843689,
      "learning_rate": 2.226309921962096e-05,
      "loss": 0.8634,
      "step": 12440
    },
    {
      "epoch": 1.6653847440056182,
      "grad_norm": 0.6553919315338135,
      "learning_rate": 2.2240802675585284e-05,
      "loss": 0.8695,
      "step": 12450
    },
    {
      "epoch": 1.6667224024345382,
      "grad_norm": 0.6942351460456848,
      "learning_rate": 2.221850613154961e-05,
      "loss": 0.8879,
      "step": 12460
    },
    {
      "epoch": 1.6680600608634584,
      "grad_norm": 0.6155970692634583,
      "learning_rate": 2.2196209587513935e-05,
      "loss": 0.8768,
      "step": 12470
    },
    {
      "epoch": 1.6693977192923786,
      "grad_norm": 0.7244983315467834,
      "learning_rate": 2.2173913043478262e-05,
      "loss": 0.8752,
      "step": 12480
    },
    {
      "epoch": 1.6707353777212988,
      "grad_norm": 0.6470041871070862,
      "learning_rate": 2.2151616499442586e-05,
      "loss": 0.8903,
      "step": 12490
    },
    {
      "epoch": 1.672073036150219,
      "grad_norm": 0.6619865298271179,
      "learning_rate": 2.2129319955406913e-05,
      "loss": 0.8744,
      "step": 12500
    },
    {
      "epoch": 1.6734106945791392,
      "grad_norm": 0.7111703753471375,
      "learning_rate": 2.2107023411371237e-05,
      "loss": 0.8646,
      "step": 12510
    },
    {
      "epoch": 1.6747483530080594,
      "grad_norm": 0.6646931171417236,
      "learning_rate": 2.2084726867335565e-05,
      "loss": 0.8499,
      "step": 12520
    },
    {
      "epoch": 1.6760860114369796,
      "grad_norm": 0.66557377576828,
      "learning_rate": 2.2062430323299892e-05,
      "loss": 0.8479,
      "step": 12530
    },
    {
      "epoch": 1.6774236698658997,
      "grad_norm": 0.6645378470420837,
      "learning_rate": 2.2040133779264216e-05,
      "loss": 0.8829,
      "step": 12540
    },
    {
      "epoch": 1.67876132829482,
      "grad_norm": 0.7006475925445557,
      "learning_rate": 2.2017837235228543e-05,
      "loss": 0.8629,
      "step": 12550
    },
    {
      "epoch": 1.6800989867237401,
      "grad_norm": 0.6545883417129517,
      "learning_rate": 2.1995540691192867e-05,
      "loss": 0.8774,
      "step": 12560
    },
    {
      "epoch": 1.6814366451526603,
      "grad_norm": 0.6616479158401489,
      "learning_rate": 2.197324414715719e-05,
      "loss": 0.8732,
      "step": 12570
    },
    {
      "epoch": 1.6827743035815805,
      "grad_norm": 0.6031704545021057,
      "learning_rate": 2.1950947603121515e-05,
      "loss": 0.8395,
      "step": 12580
    },
    {
      "epoch": 1.6841119620105007,
      "grad_norm": 0.6518896222114563,
      "learning_rate": 2.1928651059085842e-05,
      "loss": 0.8666,
      "step": 12590
    },
    {
      "epoch": 1.685449620439421,
      "grad_norm": 0.6335771083831787,
      "learning_rate": 2.190635451505017e-05,
      "loss": 0.851,
      "step": 12600
    },
    {
      "epoch": 1.6867872788683411,
      "grad_norm": 0.6590381264686584,
      "learning_rate": 2.1884057971014493e-05,
      "loss": 0.8435,
      "step": 12610
    },
    {
      "epoch": 1.688124937297261,
      "grad_norm": 0.679747998714447,
      "learning_rate": 2.186176142697882e-05,
      "loss": 0.8827,
      "step": 12620
    },
    {
      "epoch": 1.6894625957261813,
      "grad_norm": 0.6394287943840027,
      "learning_rate": 2.1839464882943144e-05,
      "loss": 0.842,
      "step": 12630
    },
    {
      "epoch": 1.6908002541551015,
      "grad_norm": 0.6515951156616211,
      "learning_rate": 2.181716833890747e-05,
      "loss": 0.8401,
      "step": 12640
    },
    {
      "epoch": 1.6921379125840217,
      "grad_norm": 0.6594231724739075,
      "learning_rate": 2.1794871794871795e-05,
      "loss": 0.877,
      "step": 12650
    },
    {
      "epoch": 1.6934755710129419,
      "grad_norm": 0.6321912407875061,
      "learning_rate": 2.1772575250836123e-05,
      "loss": 0.846,
      "step": 12660
    },
    {
      "epoch": 1.6948132294418619,
      "grad_norm": 0.6344267725944519,
      "learning_rate": 2.1750278706800447e-05,
      "loss": 0.8645,
      "step": 12670
    },
    {
      "epoch": 1.696150887870782,
      "grad_norm": 0.6583555340766907,
      "learning_rate": 2.1727982162764774e-05,
      "loss": 0.8711,
      "step": 12680
    },
    {
      "epoch": 1.6974885462997023,
      "grad_norm": 0.6495739817619324,
      "learning_rate": 2.1705685618729098e-05,
      "loss": 0.8702,
      "step": 12690
    },
    {
      "epoch": 1.6988262047286224,
      "grad_norm": 0.6811754703521729,
      "learning_rate": 2.1683389074693425e-05,
      "loss": 0.8672,
      "step": 12700
    },
    {
      "epoch": 1.7001638631575426,
      "grad_norm": 0.6920193433761597,
      "learning_rate": 2.166109253065775e-05,
      "loss": 0.8791,
      "step": 12710
    },
    {
      "epoch": 1.7015015215864628,
      "grad_norm": 0.6129072904586792,
      "learning_rate": 2.1638795986622073e-05,
      "loss": 0.902,
      "step": 12720
    },
    {
      "epoch": 1.702839180015383,
      "grad_norm": 0.6524997353553772,
      "learning_rate": 2.16164994425864e-05,
      "loss": 0.8853,
      "step": 12730
    },
    {
      "epoch": 1.7041768384443032,
      "grad_norm": 0.631521463394165,
      "learning_rate": 2.1594202898550724e-05,
      "loss": 0.8519,
      "step": 12740
    },
    {
      "epoch": 1.7055144968732234,
      "grad_norm": 0.6147809028625488,
      "learning_rate": 2.157190635451505e-05,
      "loss": 0.8667,
      "step": 12750
    },
    {
      "epoch": 1.7068521553021436,
      "grad_norm": 0.6551133990287781,
      "learning_rate": 2.1549609810479375e-05,
      "loss": 0.8867,
      "step": 12760
    },
    {
      "epoch": 1.7081898137310638,
      "grad_norm": 0.6458664536476135,
      "learning_rate": 2.1527313266443702e-05,
      "loss": 0.8227,
      "step": 12770
    },
    {
      "epoch": 1.709527472159984,
      "grad_norm": 0.6613452434539795,
      "learning_rate": 2.150501672240803e-05,
      "loss": 0.8764,
      "step": 12780
    },
    {
      "epoch": 1.7108651305889042,
      "grad_norm": 0.661516010761261,
      "learning_rate": 2.1482720178372353e-05,
      "loss": 0.8407,
      "step": 12790
    },
    {
      "epoch": 1.7122027890178244,
      "grad_norm": 0.6464368104934692,
      "learning_rate": 2.146042363433668e-05,
      "loss": 0.8831,
      "step": 12800
    },
    {
      "epoch": 1.7135404474467446,
      "grad_norm": 0.6980112195014954,
      "learning_rate": 2.1438127090301005e-05,
      "loss": 0.8546,
      "step": 12810
    },
    {
      "epoch": 1.7148781058756648,
      "grad_norm": 0.6706528067588806,
      "learning_rate": 2.1415830546265332e-05,
      "loss": 0.8489,
      "step": 12820
    },
    {
      "epoch": 1.7162157643045848,
      "grad_norm": 0.608568012714386,
      "learning_rate": 2.1393534002229656e-05,
      "loss": 0.8606,
      "step": 12830
    },
    {
      "epoch": 1.717553422733505,
      "grad_norm": 0.6113594174385071,
      "learning_rate": 2.1371237458193983e-05,
      "loss": 0.8557,
      "step": 12840
    },
    {
      "epoch": 1.7188910811624252,
      "grad_norm": 0.6809437274932861,
      "learning_rate": 2.1348940914158307e-05,
      "loss": 0.8842,
      "step": 12850
    },
    {
      "epoch": 1.7202287395913454,
      "grad_norm": 0.6098546385765076,
      "learning_rate": 2.132664437012263e-05,
      "loss": 0.8771,
      "step": 12860
    },
    {
      "epoch": 1.7215663980202656,
      "grad_norm": 0.7865855097770691,
      "learning_rate": 2.1304347826086958e-05,
      "loss": 0.8455,
      "step": 12870
    },
    {
      "epoch": 1.7229040564491855,
      "grad_norm": 0.6147280335426331,
      "learning_rate": 2.1282051282051282e-05,
      "loss": 0.843,
      "step": 12880
    },
    {
      "epoch": 1.7242417148781057,
      "grad_norm": 0.6548141837120056,
      "learning_rate": 2.125975473801561e-05,
      "loss": 0.8352,
      "step": 12890
    },
    {
      "epoch": 1.725579373307026,
      "grad_norm": 0.6311613917350769,
      "learning_rate": 2.1237458193979933e-05,
      "loss": 0.8691,
      "step": 12900
    },
    {
      "epoch": 1.7269170317359461,
      "grad_norm": 0.6025380492210388,
      "learning_rate": 2.121516164994426e-05,
      "loss": 0.8332,
      "step": 12910
    },
    {
      "epoch": 1.7282546901648663,
      "grad_norm": 0.6346598863601685,
      "learning_rate": 2.1192865105908584e-05,
      "loss": 0.8375,
      "step": 12920
    },
    {
      "epoch": 1.7295923485937865,
      "grad_norm": 0.6465787291526794,
      "learning_rate": 2.117056856187291e-05,
      "loss": 0.874,
      "step": 12930
    },
    {
      "epoch": 1.7309300070227067,
      "grad_norm": 0.6308232545852661,
      "learning_rate": 2.1148272017837235e-05,
      "loss": 0.8774,
      "step": 12940
    },
    {
      "epoch": 1.732267665451627,
      "grad_norm": 0.6153892278671265,
      "learning_rate": 2.1125975473801563e-05,
      "loss": 0.8642,
      "step": 12950
    },
    {
      "epoch": 1.7336053238805471,
      "grad_norm": 0.705234169960022,
      "learning_rate": 2.110367892976589e-05,
      "loss": 0.8902,
      "step": 12960
    },
    {
      "epoch": 1.7349429823094673,
      "grad_norm": 0.6014760732650757,
      "learning_rate": 2.1081382385730214e-05,
      "loss": 0.8673,
      "step": 12970
    },
    {
      "epoch": 1.7362806407383875,
      "grad_norm": 0.6322842836380005,
      "learning_rate": 2.105908584169454e-05,
      "loss": 0.8463,
      "step": 12980
    },
    {
      "epoch": 1.7376182991673077,
      "grad_norm": 0.5829820036888123,
      "learning_rate": 2.103678929765886e-05,
      "loss": 0.853,
      "step": 12990
    },
    {
      "epoch": 1.738955957596228,
      "grad_norm": 0.6323577165603638,
      "learning_rate": 2.101449275362319e-05,
      "loss": 0.8788,
      "step": 13000
    },
    {
      "epoch": 1.740293616025148,
      "grad_norm": 0.6551704406738281,
      "learning_rate": 2.0992196209587513e-05,
      "loss": 0.8619,
      "step": 13010
    },
    {
      "epoch": 1.7416312744540683,
      "grad_norm": 0.7869519591331482,
      "learning_rate": 2.096989966555184e-05,
      "loss": 0.8534,
      "step": 13020
    },
    {
      "epoch": 1.7429689328829885,
      "grad_norm": 0.6678475141525269,
      "learning_rate": 2.0947603121516167e-05,
      "loss": 0.8634,
      "step": 13030
    },
    {
      "epoch": 1.7443065913119085,
      "grad_norm": 0.6050987243652344,
      "learning_rate": 2.092530657748049e-05,
      "loss": 0.8599,
      "step": 13040
    },
    {
      "epoch": 1.7456442497408287,
      "grad_norm": 0.6928054690361023,
      "learning_rate": 2.090301003344482e-05,
      "loss": 0.8695,
      "step": 13050
    },
    {
      "epoch": 1.7469819081697489,
      "grad_norm": 0.6887152194976807,
      "learning_rate": 2.0880713489409142e-05,
      "loss": 0.872,
      "step": 13060
    },
    {
      "epoch": 1.748319566598669,
      "grad_norm": 0.6788880228996277,
      "learning_rate": 2.085841694537347e-05,
      "loss": 0.8503,
      "step": 13070
    },
    {
      "epoch": 1.7496572250275892,
      "grad_norm": 0.7157893180847168,
      "learning_rate": 2.0836120401337793e-05,
      "loss": 0.8919,
      "step": 13080
    },
    {
      "epoch": 1.7509948834565092,
      "grad_norm": 0.6522082686424255,
      "learning_rate": 2.081382385730212e-05,
      "loss": 0.8701,
      "step": 13090
    },
    {
      "epoch": 1.7523325418854294,
      "grad_norm": 0.7386086583137512,
      "learning_rate": 2.0791527313266445e-05,
      "loss": 0.8829,
      "step": 13100
    },
    {
      "epoch": 1.7536702003143496,
      "grad_norm": 0.6572800874710083,
      "learning_rate": 2.0769230769230772e-05,
      "loss": 0.878,
      "step": 13110
    },
    {
      "epoch": 1.7550078587432698,
      "grad_norm": 0.6652348637580872,
      "learning_rate": 2.0746934225195096e-05,
      "loss": 0.8512,
      "step": 13120
    },
    {
      "epoch": 1.75634551717219,
      "grad_norm": 0.6679761409759521,
      "learning_rate": 2.072463768115942e-05,
      "loss": 0.8852,
      "step": 13130
    },
    {
      "epoch": 1.7576831756011102,
      "grad_norm": 0.7056594491004944,
      "learning_rate": 2.0702341137123747e-05,
      "loss": 0.8534,
      "step": 13140
    },
    {
      "epoch": 1.7590208340300304,
      "grad_norm": 0.6525583267211914,
      "learning_rate": 2.068004459308807e-05,
      "loss": 0.8652,
      "step": 13150
    },
    {
      "epoch": 1.7603584924589506,
      "grad_norm": 0.6439124345779419,
      "learning_rate": 2.0657748049052398e-05,
      "loss": 0.8408,
      "step": 13160
    },
    {
      "epoch": 1.7616961508878708,
      "grad_norm": 0.6277629137039185,
      "learning_rate": 2.0635451505016722e-05,
      "loss": 0.8546,
      "step": 13170
    },
    {
      "epoch": 1.763033809316791,
      "grad_norm": 0.6526296138763428,
      "learning_rate": 2.061315496098105e-05,
      "loss": 0.8732,
      "step": 13180
    },
    {
      "epoch": 1.7643714677457112,
      "grad_norm": 0.7269511222839355,
      "learning_rate": 2.0590858416945373e-05,
      "loss": 0.8775,
      "step": 13190
    },
    {
      "epoch": 1.7657091261746314,
      "grad_norm": 0.560563325881958,
      "learning_rate": 2.05685618729097e-05,
      "loss": 0.8735,
      "step": 13200
    },
    {
      "epoch": 1.7670467846035516,
      "grad_norm": 0.6672399640083313,
      "learning_rate": 2.0546265328874024e-05,
      "loss": 0.8681,
      "step": 13210
    },
    {
      "epoch": 1.7683844430324718,
      "grad_norm": 0.6796360611915588,
      "learning_rate": 2.052396878483835e-05,
      "loss": 0.872,
      "step": 13220
    },
    {
      "epoch": 1.769722101461392,
      "grad_norm": 0.6056459546089172,
      "learning_rate": 2.050167224080268e-05,
      "loss": 0.8384,
      "step": 13230
    },
    {
      "epoch": 1.7710597598903122,
      "grad_norm": 0.6613274812698364,
      "learning_rate": 2.0479375696767003e-05,
      "loss": 0.8403,
      "step": 13240
    },
    {
      "epoch": 1.7723974183192321,
      "grad_norm": 0.6892728805541992,
      "learning_rate": 2.045707915273133e-05,
      "loss": 0.8927,
      "step": 13250
    },
    {
      "epoch": 1.7737350767481523,
      "grad_norm": 0.6524103283882141,
      "learning_rate": 2.0434782608695654e-05,
      "loss": 0.8749,
      "step": 13260
    },
    {
      "epoch": 1.7750727351770725,
      "grad_norm": 0.6824642419815063,
      "learning_rate": 2.0412486064659978e-05,
      "loss": 0.8742,
      "step": 13270
    },
    {
      "epoch": 1.7764103936059927,
      "grad_norm": 0.7401129603385925,
      "learning_rate": 2.03901895206243e-05,
      "loss": 0.86,
      "step": 13280
    },
    {
      "epoch": 1.777748052034913,
      "grad_norm": 0.6074931621551514,
      "learning_rate": 2.036789297658863e-05,
      "loss": 0.8756,
      "step": 13290
    },
    {
      "epoch": 1.779085710463833,
      "grad_norm": 0.6770281195640564,
      "learning_rate": 2.0345596432552956e-05,
      "loss": 0.8344,
      "step": 13300
    },
    {
      "epoch": 1.780423368892753,
      "grad_norm": 0.6682648062705994,
      "learning_rate": 2.032329988851728e-05,
      "loss": 0.8585,
      "step": 13310
    },
    {
      "epoch": 1.7817610273216733,
      "grad_norm": 0.6703150272369385,
      "learning_rate": 2.0301003344481607e-05,
      "loss": 0.8868,
      "step": 13320
    },
    {
      "epoch": 1.7830986857505935,
      "grad_norm": 0.6783479452133179,
      "learning_rate": 2.027870680044593e-05,
      "loss": 0.8879,
      "step": 13330
    },
    {
      "epoch": 1.7844363441795137,
      "grad_norm": 0.7201266884803772,
      "learning_rate": 2.025641025641026e-05,
      "loss": 0.8747,
      "step": 13340
    },
    {
      "epoch": 1.7857740026084339,
      "grad_norm": 0.6396542191505432,
      "learning_rate": 2.0234113712374582e-05,
      "loss": 0.8664,
      "step": 13350
    },
    {
      "epoch": 1.787111661037354,
      "grad_norm": 0.6961092352867126,
      "learning_rate": 2.021181716833891e-05,
      "loss": 0.8406,
      "step": 13360
    },
    {
      "epoch": 1.7884493194662743,
      "grad_norm": 0.6717017889022827,
      "learning_rate": 2.0189520624303233e-05,
      "loss": 0.8589,
      "step": 13370
    },
    {
      "epoch": 1.7897869778951945,
      "grad_norm": 0.6273303031921387,
      "learning_rate": 2.016722408026756e-05,
      "loss": 0.8672,
      "step": 13380
    },
    {
      "epoch": 1.7911246363241147,
      "grad_norm": 0.6236352324485779,
      "learning_rate": 2.0144927536231885e-05,
      "loss": 0.8812,
      "step": 13390
    },
    {
      "epoch": 1.7924622947530349,
      "grad_norm": 0.6386182904243469,
      "learning_rate": 2.012263099219621e-05,
      "loss": 0.8496,
      "step": 13400
    },
    {
      "epoch": 1.793799953181955,
      "grad_norm": 0.7284178137779236,
      "learning_rate": 2.0100334448160536e-05,
      "loss": 0.8921,
      "step": 13410
    },
    {
      "epoch": 1.7951376116108753,
      "grad_norm": 0.6354362964630127,
      "learning_rate": 2.007803790412486e-05,
      "loss": 0.8647,
      "step": 13420
    },
    {
      "epoch": 1.7964752700397955,
      "grad_norm": 0.6398695707321167,
      "learning_rate": 2.0055741360089187e-05,
      "loss": 0.8568,
      "step": 13430
    },
    {
      "epoch": 1.7978129284687157,
      "grad_norm": 0.6952180862426758,
      "learning_rate": 2.003344481605351e-05,
      "loss": 0.8885,
      "step": 13440
    },
    {
      "epoch": 1.7991505868976359,
      "grad_norm": 0.6854375600814819,
      "learning_rate": 2.0011148272017838e-05,
      "loss": 0.8657,
      "step": 13450
    },
    {
      "epoch": 1.8004882453265558,
      "grad_norm": 0.6515032649040222,
      "learning_rate": 1.9988851727982162e-05,
      "loss": 0.8819,
      "step": 13460
    },
    {
      "epoch": 1.801825903755476,
      "grad_norm": 0.6916910409927368,
      "learning_rate": 1.996655518394649e-05,
      "loss": 0.8629,
      "step": 13470
    },
    {
      "epoch": 1.8031635621843962,
      "grad_norm": 0.6883739829063416,
      "learning_rate": 1.9944258639910816e-05,
      "loss": 0.8587,
      "step": 13480
    },
    {
      "epoch": 1.8045012206133164,
      "grad_norm": 0.622844934463501,
      "learning_rate": 1.992196209587514e-05,
      "loss": 0.8529,
      "step": 13490
    },
    {
      "epoch": 1.8058388790422366,
      "grad_norm": 0.6609771847724915,
      "learning_rate": 1.9899665551839468e-05,
      "loss": 0.8862,
      "step": 13500
    },
    {
      "epoch": 1.8071765374711566,
      "grad_norm": 0.7095226645469666,
      "learning_rate": 1.987736900780379e-05,
      "loss": 0.8656,
      "step": 13510
    },
    {
      "epoch": 1.8085141959000768,
      "grad_norm": 0.6611287593841553,
      "learning_rate": 1.985507246376812e-05,
      "loss": 0.8595,
      "step": 13520
    },
    {
      "epoch": 1.809851854328997,
      "grad_norm": 0.664430558681488,
      "learning_rate": 1.9832775919732443e-05,
      "loss": 0.866,
      "step": 13530
    },
    {
      "epoch": 1.8111895127579172,
      "grad_norm": 0.6117904782295227,
      "learning_rate": 1.9810479375696766e-05,
      "loss": 0.863,
      "step": 13540
    },
    {
      "epoch": 1.8125271711868374,
      "grad_norm": 0.6232172250747681,
      "learning_rate": 1.9788182831661094e-05,
      "loss": 0.8734,
      "step": 13550
    },
    {
      "epoch": 1.8138648296157576,
      "grad_norm": 0.6967286467552185,
      "learning_rate": 1.9765886287625418e-05,
      "loss": 0.8628,
      "step": 13560
    },
    {
      "epoch": 1.8152024880446778,
      "grad_norm": 0.6763817667961121,
      "learning_rate": 1.9743589743589745e-05,
      "loss": 0.8827,
      "step": 13570
    },
    {
      "epoch": 1.816540146473598,
      "grad_norm": 0.6112052798271179,
      "learning_rate": 1.972129319955407e-05,
      "loss": 0.8627,
      "step": 13580
    },
    {
      "epoch": 1.8178778049025182,
      "grad_norm": 0.6785069704055786,
      "learning_rate": 1.9698996655518396e-05,
      "loss": 0.8485,
      "step": 13590
    },
    {
      "epoch": 1.8192154633314384,
      "grad_norm": 0.6375874876976013,
      "learning_rate": 1.967670011148272e-05,
      "loss": 0.8543,
      "step": 13600
    },
    {
      "epoch": 1.8205531217603586,
      "grad_norm": 0.6069337129592896,
      "learning_rate": 1.9654403567447047e-05,
      "loss": 0.8593,
      "step": 13610
    },
    {
      "epoch": 1.8218907801892787,
      "grad_norm": 0.6720141172409058,
      "learning_rate": 1.963210702341137e-05,
      "loss": 0.8874,
      "step": 13620
    },
    {
      "epoch": 1.823228438618199,
      "grad_norm": 0.618770182132721,
      "learning_rate": 1.96098104793757e-05,
      "loss": 0.8869,
      "step": 13630
    },
    {
      "epoch": 1.8245660970471191,
      "grad_norm": 0.7075315117835999,
      "learning_rate": 1.9587513935340022e-05,
      "loss": 0.8748,
      "step": 13640
    },
    {
      "epoch": 1.8259037554760393,
      "grad_norm": 0.5664411187171936,
      "learning_rate": 1.956521739130435e-05,
      "loss": 0.846,
      "step": 13650
    },
    {
      "epoch": 1.8272414139049595,
      "grad_norm": 0.698233425617218,
      "learning_rate": 1.9542920847268677e-05,
      "loss": 0.8732,
      "step": 13660
    },
    {
      "epoch": 1.8285790723338795,
      "grad_norm": 0.6372134685516357,
      "learning_rate": 1.9520624303233e-05,
      "loss": 0.8476,
      "step": 13670
    },
    {
      "epoch": 1.8299167307627997,
      "grad_norm": 0.7059337496757507,
      "learning_rate": 1.9498327759197325e-05,
      "loss": 0.8931,
      "step": 13680
    },
    {
      "epoch": 1.83125438919172,
      "grad_norm": 0.5964257717132568,
      "learning_rate": 1.947603121516165e-05,
      "loss": 0.8639,
      "step": 13690
    },
    {
      "epoch": 1.83259204762064,
      "grad_norm": 0.6135556101799011,
      "learning_rate": 1.9453734671125976e-05,
      "loss": 0.8545,
      "step": 13700
    },
    {
      "epoch": 1.8339297060495603,
      "grad_norm": 0.6660729646682739,
      "learning_rate": 1.94314381270903e-05,
      "loss": 0.8443,
      "step": 13710
    },
    {
      "epoch": 1.8352673644784803,
      "grad_norm": 0.6057289242744446,
      "learning_rate": 1.9409141583054627e-05,
      "loss": 0.8673,
      "step": 13720
    },
    {
      "epoch": 1.8366050229074005,
      "grad_norm": 0.6495962142944336,
      "learning_rate": 1.9386845039018954e-05,
      "loss": 0.8689,
      "step": 13730
    },
    {
      "epoch": 1.8379426813363207,
      "grad_norm": 0.6857544779777527,
      "learning_rate": 1.9364548494983278e-05,
      "loss": 0.8982,
      "step": 13740
    },
    {
      "epoch": 1.8392803397652409,
      "grad_norm": 0.6606011390686035,
      "learning_rate": 1.9342251950947605e-05,
      "loss": 0.8545,
      "step": 13750
    },
    {
      "epoch": 1.840617998194161,
      "grad_norm": 0.654626727104187,
      "learning_rate": 1.931995540691193e-05,
      "loss": 0.8728,
      "step": 13760
    },
    {
      "epoch": 1.8419556566230812,
      "grad_norm": 0.6331703662872314,
      "learning_rate": 1.9297658862876256e-05,
      "loss": 0.875,
      "step": 13770
    },
    {
      "epoch": 1.8432933150520014,
      "grad_norm": 0.6700825095176697,
      "learning_rate": 1.927536231884058e-05,
      "loss": 0.8822,
      "step": 13780
    },
    {
      "epoch": 1.8446309734809216,
      "grad_norm": 0.5895233154296875,
      "learning_rate": 1.9253065774804908e-05,
      "loss": 0.8675,
      "step": 13790
    },
    {
      "epoch": 1.8459686319098418,
      "grad_norm": 0.62833172082901,
      "learning_rate": 1.923076923076923e-05,
      "loss": 0.8735,
      "step": 13800
    },
    {
      "epoch": 1.847306290338762,
      "grad_norm": 0.6477741599082947,
      "learning_rate": 1.920847268673356e-05,
      "loss": 0.8742,
      "step": 13810
    },
    {
      "epoch": 1.8486439487676822,
      "grad_norm": 0.6266162395477295,
      "learning_rate": 1.9186176142697883e-05,
      "loss": 0.8769,
      "step": 13820
    },
    {
      "epoch": 1.8499816071966024,
      "grad_norm": 0.6402474045753479,
      "learning_rate": 1.9163879598662206e-05,
      "loss": 0.8943,
      "step": 13830
    },
    {
      "epoch": 1.8513192656255226,
      "grad_norm": 0.6112073063850403,
      "learning_rate": 1.9141583054626534e-05,
      "loss": 0.8315,
      "step": 13840
    },
    {
      "epoch": 1.8526569240544428,
      "grad_norm": 0.6490722298622131,
      "learning_rate": 1.9119286510590858e-05,
      "loss": 0.8656,
      "step": 13850
    },
    {
      "epoch": 1.853994582483363,
      "grad_norm": 0.751454770565033,
      "learning_rate": 1.9096989966555185e-05,
      "loss": 0.8895,
      "step": 13860
    },
    {
      "epoch": 1.8553322409122832,
      "grad_norm": 0.6545649170875549,
      "learning_rate": 1.907469342251951e-05,
      "loss": 0.8509,
      "step": 13870
    },
    {
      "epoch": 1.8566698993412032,
      "grad_norm": 0.605769693851471,
      "learning_rate": 1.9052396878483836e-05,
      "loss": 0.8574,
      "step": 13880
    },
    {
      "epoch": 1.8580075577701234,
      "grad_norm": 0.6399916410446167,
      "learning_rate": 1.903010033444816e-05,
      "loss": 0.8605,
      "step": 13890
    },
    {
      "epoch": 1.8593452161990436,
      "grad_norm": 0.6756651401519775,
      "learning_rate": 1.9007803790412487e-05,
      "loss": 0.8512,
      "step": 13900
    },
    {
      "epoch": 1.8606828746279638,
      "grad_norm": 0.6360005736351013,
      "learning_rate": 1.8985507246376814e-05,
      "loss": 0.8615,
      "step": 13910
    },
    {
      "epoch": 1.862020533056884,
      "grad_norm": 0.6525046825408936,
      "learning_rate": 1.896321070234114e-05,
      "loss": 0.8636,
      "step": 13920
    },
    {
      "epoch": 1.863358191485804,
      "grad_norm": 0.6257222890853882,
      "learning_rate": 1.8940914158305466e-05,
      "loss": 0.8582,
      "step": 13930
    },
    {
      "epoch": 1.8646958499147241,
      "grad_norm": 0.5773546695709229,
      "learning_rate": 1.891861761426979e-05,
      "loss": 0.8844,
      "step": 13940
    },
    {
      "epoch": 1.8660335083436443,
      "grad_norm": 0.6917885541915894,
      "learning_rate": 1.8896321070234117e-05,
      "loss": 0.8552,
      "step": 13950
    },
    {
      "epoch": 1.8673711667725645,
      "grad_norm": 0.6379852890968323,
      "learning_rate": 1.8874024526198437e-05,
      "loss": 0.8666,
      "step": 13960
    },
    {
      "epoch": 1.8687088252014847,
      "grad_norm": 0.6595092415809631,
      "learning_rate": 1.8851727982162765e-05,
      "loss": 0.8496,
      "step": 13970
    },
    {
      "epoch": 1.870046483630405,
      "grad_norm": 0.6497810482978821,
      "learning_rate": 1.8829431438127092e-05,
      "loss": 0.8604,
      "step": 13980
    },
    {
      "epoch": 1.8713841420593251,
      "grad_norm": 0.6290730834007263,
      "learning_rate": 1.8807134894091416e-05,
      "loss": 0.8677,
      "step": 13990
    },
    {
      "epoch": 1.8727218004882453,
      "grad_norm": 0.6506462693214417,
      "learning_rate": 1.8784838350055743e-05,
      "loss": 0.8904,
      "step": 14000
    },
    {
      "epoch": 1.8740594589171655,
      "grad_norm": 0.6881735324859619,
      "learning_rate": 1.8762541806020067e-05,
      "loss": 0.8671,
      "step": 14010
    },
    {
      "epoch": 1.8753971173460857,
      "grad_norm": 1.1388968229293823,
      "learning_rate": 1.8740245261984394e-05,
      "loss": 0.8587,
      "step": 14020
    },
    {
      "epoch": 1.876734775775006,
      "grad_norm": 0.6556771993637085,
      "learning_rate": 1.8717948717948718e-05,
      "loss": 0.865,
      "step": 14030
    },
    {
      "epoch": 1.878072434203926,
      "grad_norm": 0.6834882497787476,
      "learning_rate": 1.8695652173913045e-05,
      "loss": 0.9119,
      "step": 14040
    },
    {
      "epoch": 1.8794100926328463,
      "grad_norm": 0.6120278239250183,
      "learning_rate": 1.867335562987737e-05,
      "loss": 0.8506,
      "step": 14050
    },
    {
      "epoch": 1.8807477510617665,
      "grad_norm": 0.646690309047699,
      "learning_rate": 1.8651059085841696e-05,
      "loss": 0.8745,
      "step": 14060
    },
    {
      "epoch": 1.8820854094906867,
      "grad_norm": 0.6809192895889282,
      "learning_rate": 1.862876254180602e-05,
      "loss": 0.84,
      "step": 14070
    },
    {
      "epoch": 1.8834230679196067,
      "grad_norm": 0.7347254753112793,
      "learning_rate": 1.8606465997770348e-05,
      "loss": 0.8493,
      "step": 14080
    },
    {
      "epoch": 1.8847607263485269,
      "grad_norm": 0.6665886044502258,
      "learning_rate": 1.8584169453734675e-05,
      "loss": 0.8665,
      "step": 14090
    },
    {
      "epoch": 1.886098384777447,
      "grad_norm": 0.6606460809707642,
      "learning_rate": 1.8561872909698995e-05,
      "loss": 0.8709,
      "step": 14100
    },
    {
      "epoch": 1.8874360432063673,
      "grad_norm": 0.5999892950057983,
      "learning_rate": 1.8539576365663323e-05,
      "loss": 0.8498,
      "step": 14110
    },
    {
      "epoch": 1.8887737016352875,
      "grad_norm": 0.6517913937568665,
      "learning_rate": 1.8517279821627646e-05,
      "loss": 0.8591,
      "step": 14120
    },
    {
      "epoch": 1.8901113600642077,
      "grad_norm": 0.6211279034614563,
      "learning_rate": 1.8494983277591974e-05,
      "loss": 0.8767,
      "step": 14130
    },
    {
      "epoch": 1.8914490184931276,
      "grad_norm": 0.635754406452179,
      "learning_rate": 1.8472686733556298e-05,
      "loss": 0.8775,
      "step": 14140
    },
    {
      "epoch": 1.8927866769220478,
      "grad_norm": 0.6943828463554382,
      "learning_rate": 1.8450390189520625e-05,
      "loss": 0.8471,
      "step": 14150
    },
    {
      "epoch": 1.894124335350968,
      "grad_norm": 0.6366477608680725,
      "learning_rate": 1.8428093645484952e-05,
      "loss": 0.8716,
      "step": 14160
    },
    {
      "epoch": 1.8954619937798882,
      "grad_norm": 0.641296923160553,
      "learning_rate": 1.8405797101449276e-05,
      "loss": 0.8572,
      "step": 14170
    },
    {
      "epoch": 1.8967996522088084,
      "grad_norm": 0.6227000951766968,
      "learning_rate": 1.8383500557413603e-05,
      "loss": 0.8857,
      "step": 14180
    },
    {
      "epoch": 1.8981373106377286,
      "grad_norm": 0.6125373840332031,
      "learning_rate": 1.8361204013377927e-05,
      "loss": 0.8428,
      "step": 14190
    },
    {
      "epoch": 1.8994749690666488,
      "grad_norm": 0.6298336386680603,
      "learning_rate": 1.8338907469342254e-05,
      "loss": 0.861,
      "step": 14200
    },
    {
      "epoch": 1.900812627495569,
      "grad_norm": 0.6786596179008484,
      "learning_rate": 1.831661092530658e-05,
      "loss": 0.8469,
      "step": 14210
    },
    {
      "epoch": 1.9021502859244892,
      "grad_norm": 0.6657390594482422,
      "learning_rate": 1.8294314381270906e-05,
      "loss": 0.8748,
      "step": 14220
    },
    {
      "epoch": 1.9034879443534094,
      "grad_norm": 0.627075731754303,
      "learning_rate": 1.827201783723523e-05,
      "loss": 0.8683,
      "step": 14230
    },
    {
      "epoch": 1.9048256027823296,
      "grad_norm": 0.7090192437171936,
      "learning_rate": 1.8249721293199553e-05,
      "loss": 0.8984,
      "step": 14240
    },
    {
      "epoch": 1.9061632612112498,
      "grad_norm": 0.6477276682853699,
      "learning_rate": 1.822742474916388e-05,
      "loss": 0.85,
      "step": 14250
    },
    {
      "epoch": 1.90750091964017,
      "grad_norm": 0.6557077169418335,
      "learning_rate": 1.8205128205128204e-05,
      "loss": 0.8813,
      "step": 14260
    },
    {
      "epoch": 1.9088385780690902,
      "grad_norm": 0.613033652305603,
      "learning_rate": 1.8182831661092532e-05,
      "loss": 0.8581,
      "step": 14270
    },
    {
      "epoch": 1.9101762364980104,
      "grad_norm": 0.6799604296684265,
      "learning_rate": 1.8160535117056856e-05,
      "loss": 0.8659,
      "step": 14280
    },
    {
      "epoch": 1.9115138949269304,
      "grad_norm": 0.6509031057357788,
      "learning_rate": 1.8138238573021183e-05,
      "loss": 0.8577,
      "step": 14290
    },
    {
      "epoch": 1.9128515533558506,
      "grad_norm": 0.659820020198822,
      "learning_rate": 1.8115942028985507e-05,
      "loss": 0.8464,
      "step": 14300
    },
    {
      "epoch": 1.9141892117847707,
      "grad_norm": 0.6145414710044861,
      "learning_rate": 1.8093645484949834e-05,
      "loss": 0.8627,
      "step": 14310
    },
    {
      "epoch": 1.915526870213691,
      "grad_norm": 0.6720951795578003,
      "learning_rate": 1.8071348940914158e-05,
      "loss": 0.8552,
      "step": 14320
    },
    {
      "epoch": 1.9168645286426111,
      "grad_norm": 0.6314961910247803,
      "learning_rate": 1.8049052396878485e-05,
      "loss": 0.8856,
      "step": 14330
    },
    {
      "epoch": 1.9182021870715313,
      "grad_norm": 0.6588656902313232,
      "learning_rate": 1.8026755852842812e-05,
      "loss": 0.8746,
      "step": 14340
    },
    {
      "epoch": 1.9195398455004513,
      "grad_norm": 0.6071060299873352,
      "learning_rate": 1.8004459308807136e-05,
      "loss": 0.8733,
      "step": 14350
    },
    {
      "epoch": 1.9208775039293715,
      "grad_norm": 0.6136508584022522,
      "learning_rate": 1.7982162764771464e-05,
      "loss": 0.8656,
      "step": 14360
    },
    {
      "epoch": 1.9222151623582917,
      "grad_norm": 0.6014556884765625,
      "learning_rate": 1.7959866220735788e-05,
      "loss": 0.8718,
      "step": 14370
    },
    {
      "epoch": 1.923552820787212,
      "grad_norm": 0.663570761680603,
      "learning_rate": 1.793756967670011e-05,
      "loss": 0.8465,
      "step": 14380
    },
    {
      "epoch": 1.924890479216132,
      "grad_norm": 0.681372344493866,
      "learning_rate": 1.7915273132664435e-05,
      "loss": 0.8432,
      "step": 14390
    },
    {
      "epoch": 1.9262281376450523,
      "grad_norm": 0.6429541110992432,
      "learning_rate": 1.7892976588628763e-05,
      "loss": 0.8606,
      "step": 14400
    },
    {
      "epoch": 1.9275657960739725,
      "grad_norm": 0.7322336435317993,
      "learning_rate": 1.787068004459309e-05,
      "loss": 0.8673,
      "step": 14410
    },
    {
      "epoch": 1.9289034545028927,
      "grad_norm": 0.598458468914032,
      "learning_rate": 1.7848383500557414e-05,
      "loss": 0.8462,
      "step": 14420
    },
    {
      "epoch": 1.9302411129318129,
      "grad_norm": 0.6242884993553162,
      "learning_rate": 1.782608695652174e-05,
      "loss": 0.8837,
      "step": 14430
    },
    {
      "epoch": 1.931578771360733,
      "grad_norm": 0.6223270297050476,
      "learning_rate": 1.7803790412486065e-05,
      "loss": 0.8644,
      "step": 14440
    },
    {
      "epoch": 1.9329164297896533,
      "grad_norm": 0.6519293189048767,
      "learning_rate": 1.7781493868450392e-05,
      "loss": 0.8459,
      "step": 14450
    },
    {
      "epoch": 1.9342540882185735,
      "grad_norm": 0.7316262125968933,
      "learning_rate": 1.7759197324414716e-05,
      "loss": 0.8501,
      "step": 14460
    },
    {
      "epoch": 1.9355917466474937,
      "grad_norm": 0.6306095719337463,
      "learning_rate": 1.7736900780379043e-05,
      "loss": 0.8818,
      "step": 14470
    },
    {
      "epoch": 1.9369294050764139,
      "grad_norm": 0.6607405543327332,
      "learning_rate": 1.7714604236343367e-05,
      "loss": 0.8547,
      "step": 14480
    },
    {
      "epoch": 1.938267063505334,
      "grad_norm": 0.6439771056175232,
      "learning_rate": 1.7692307692307694e-05,
      "loss": 0.8675,
      "step": 14490
    },
    {
      "epoch": 1.939604721934254,
      "grad_norm": 0.6556990146636963,
      "learning_rate": 1.7670011148272018e-05,
      "loss": 0.8699,
      "step": 14500
    },
    {
      "epoch": 1.9409423803631742,
      "grad_norm": 0.7226216197013855,
      "learning_rate": 1.7647714604236346e-05,
      "loss": 0.8549,
      "step": 14510
    },
    {
      "epoch": 1.9422800387920944,
      "grad_norm": 0.6458007097244263,
      "learning_rate": 1.762541806020067e-05,
      "loss": 0.8904,
      "step": 14520
    },
    {
      "epoch": 1.9436176972210146,
      "grad_norm": 0.6283391714096069,
      "learning_rate": 1.7603121516164993e-05,
      "loss": 0.8451,
      "step": 14530
    },
    {
      "epoch": 1.9449553556499348,
      "grad_norm": 0.6463046073913574,
      "learning_rate": 1.758082497212932e-05,
      "loss": 0.861,
      "step": 14540
    },
    {
      "epoch": 1.9462930140788548,
      "grad_norm": 0.72197425365448,
      "learning_rate": 1.7558528428093644e-05,
      "loss": 0.8601,
      "step": 14550
    },
    {
      "epoch": 1.947630672507775,
      "grad_norm": 0.7423925995826721,
      "learning_rate": 1.7536231884057972e-05,
      "loss": 0.9147,
      "step": 14560
    },
    {
      "epoch": 1.9489683309366952,
      "grad_norm": 0.7088714241981506,
      "learning_rate": 1.7513935340022296e-05,
      "loss": 0.8427,
      "step": 14570
    },
    {
      "epoch": 1.9503059893656154,
      "grad_norm": 0.710701584815979,
      "learning_rate": 1.7491638795986623e-05,
      "loss": 0.8625,
      "step": 14580
    },
    {
      "epoch": 1.9516436477945356,
      "grad_norm": 0.630300760269165,
      "learning_rate": 1.746934225195095e-05,
      "loss": 0.8521,
      "step": 14590
    },
    {
      "epoch": 1.9529813062234558,
      "grad_norm": 0.6096078753471375,
      "learning_rate": 1.7447045707915274e-05,
      "loss": 0.8496,
      "step": 14600
    },
    {
      "epoch": 1.954318964652376,
      "grad_norm": 0.6429049372673035,
      "learning_rate": 1.74247491638796e-05,
      "loss": 0.8819,
      "step": 14610
    },
    {
      "epoch": 1.9556566230812962,
      "grad_norm": 0.633207380771637,
      "learning_rate": 1.7402452619843925e-05,
      "loss": 0.8559,
      "step": 14620
    },
    {
      "epoch": 1.9569942815102164,
      "grad_norm": 0.6828517913818359,
      "learning_rate": 1.7380156075808252e-05,
      "loss": 0.8916,
      "step": 14630
    },
    {
      "epoch": 1.9583319399391366,
      "grad_norm": 0.6636685729026794,
      "learning_rate": 1.7357859531772576e-05,
      "loss": 0.8888,
      "step": 14640
    },
    {
      "epoch": 1.9596695983680568,
      "grad_norm": 0.6527732014656067,
      "learning_rate": 1.73355629877369e-05,
      "loss": 0.842,
      "step": 14650
    },
    {
      "epoch": 1.961007256796977,
      "grad_norm": 0.6822865605354309,
      "learning_rate": 1.7313266443701227e-05,
      "loss": 0.8638,
      "step": 14660
    },
    {
      "epoch": 1.9623449152258972,
      "grad_norm": 0.6556878685951233,
      "learning_rate": 1.729096989966555e-05,
      "loss": 0.8627,
      "step": 14670
    },
    {
      "epoch": 1.9636825736548174,
      "grad_norm": 0.6613993048667908,
      "learning_rate": 1.726867335562988e-05,
      "loss": 0.8552,
      "step": 14680
    },
    {
      "epoch": 1.9650202320837376,
      "grad_norm": 0.6174458265304565,
      "learning_rate": 1.7246376811594203e-05,
      "loss": 0.8586,
      "step": 14690
    },
    {
      "epoch": 1.9663578905126577,
      "grad_norm": 0.6764967441558838,
      "learning_rate": 1.722408026755853e-05,
      "loss": 0.8591,
      "step": 14700
    },
    {
      "epoch": 1.9676955489415777,
      "grad_norm": 0.6578160524368286,
      "learning_rate": 1.7201783723522854e-05,
      "loss": 0.8765,
      "step": 14710
    },
    {
      "epoch": 1.969033207370498,
      "grad_norm": 0.7472577095031738,
      "learning_rate": 1.717948717948718e-05,
      "loss": 0.8649,
      "step": 14720
    },
    {
      "epoch": 1.9703708657994181,
      "grad_norm": 0.6428282856941223,
      "learning_rate": 1.7157190635451505e-05,
      "loss": 0.8641,
      "step": 14730
    },
    {
      "epoch": 1.9717085242283383,
      "grad_norm": 0.5957752466201782,
      "learning_rate": 1.7134894091415832e-05,
      "loss": 0.8618,
      "step": 14740
    },
    {
      "epoch": 1.9730461826572585,
      "grad_norm": 0.6561683416366577,
      "learning_rate": 1.7112597547380156e-05,
      "loss": 0.8636,
      "step": 14750
    },
    {
      "epoch": 1.9743838410861785,
      "grad_norm": 0.6739205121994019,
      "learning_rate": 1.7090301003344483e-05,
      "loss": 0.8897,
      "step": 14760
    },
    {
      "epoch": 1.9757214995150987,
      "grad_norm": 0.6117511987686157,
      "learning_rate": 1.706800445930881e-05,
      "loss": 0.8686,
      "step": 14770
    },
    {
      "epoch": 1.9770591579440189,
      "grad_norm": 0.6549372673034668,
      "learning_rate": 1.7045707915273134e-05,
      "loss": 0.8322,
      "step": 14780
    },
    {
      "epoch": 1.978396816372939,
      "grad_norm": 0.6758108735084534,
      "learning_rate": 1.7023411371237458e-05,
      "loss": 0.8744,
      "step": 14790
    },
    {
      "epoch": 1.9797344748018593,
      "grad_norm": 0.7006625533103943,
      "learning_rate": 1.7001114827201782e-05,
      "loss": 0.8563,
      "step": 14800
    },
    {
      "epoch": 1.9810721332307795,
      "grad_norm": 0.6564531922340393,
      "learning_rate": 1.697881828316611e-05,
      "loss": 0.8827,
      "step": 14810
    },
    {
      "epoch": 1.9824097916596997,
      "grad_norm": 0.6662588119506836,
      "learning_rate": 1.6956521739130433e-05,
      "loss": 0.8479,
      "step": 14820
    },
    {
      "epoch": 1.9837474500886199,
      "grad_norm": 0.6234028339385986,
      "learning_rate": 1.693422519509476e-05,
      "loss": 0.8492,
      "step": 14830
    },
    {
      "epoch": 1.98508510851754,
      "grad_norm": 0.6425347328186035,
      "learning_rate": 1.6911928651059088e-05,
      "loss": 0.8862,
      "step": 14840
    },
    {
      "epoch": 1.9864227669464602,
      "grad_norm": 0.7050678133964539,
      "learning_rate": 1.6889632107023412e-05,
      "loss": 0.8628,
      "step": 14850
    },
    {
      "epoch": 1.9877604253753804,
      "grad_norm": 0.642026424407959,
      "learning_rate": 1.686733556298774e-05,
      "loss": 0.8501,
      "step": 14860
    },
    {
      "epoch": 1.9890980838043006,
      "grad_norm": 0.7157203555107117,
      "learning_rate": 1.6845039018952063e-05,
      "loss": 0.8813,
      "step": 14870
    },
    {
      "epoch": 1.9904357422332208,
      "grad_norm": 0.6945033669471741,
      "learning_rate": 1.682274247491639e-05,
      "loss": 0.8483,
      "step": 14880
    },
    {
      "epoch": 1.991773400662141,
      "grad_norm": 0.6440708041191101,
      "learning_rate": 1.6800445930880714e-05,
      "loss": 0.8717,
      "step": 14890
    },
    {
      "epoch": 1.9931110590910612,
      "grad_norm": 0.6446479558944702,
      "learning_rate": 1.677814938684504e-05,
      "loss": 0.8734,
      "step": 14900
    },
    {
      "epoch": 1.9944487175199814,
      "grad_norm": 0.6281595230102539,
      "learning_rate": 1.6755852842809365e-05,
      "loss": 0.8851,
      "step": 14910
    },
    {
      "epoch": 1.9957863759489014,
      "grad_norm": 0.6556540727615356,
      "learning_rate": 1.6733556298773692e-05,
      "loss": 0.8656,
      "step": 14920
    },
    {
      "epoch": 1.9971240343778216,
      "grad_norm": 0.6321813464164734,
      "learning_rate": 1.6711259754738016e-05,
      "loss": 0.8549,
      "step": 14930
    },
    {
      "epoch": 1.9984616928067418,
      "grad_norm": 0.6308098435401917,
      "learning_rate": 1.668896321070234e-05,
      "loss": 0.8584,
      "step": 14940
    },
    {
      "epoch": 1.999799351235662,
      "grad_norm": 0.6439543962478638,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.878,
      "step": 14950
    },
    {
      "epoch": 2.001137009664582,
      "grad_norm": 0.6741325855255127,
      "learning_rate": 1.664437012263099e-05,
      "loss": 0.823,
      "step": 14960
    },
    {
      "epoch": 2.002474668093502,
      "grad_norm": 0.6906075477600098,
      "learning_rate": 1.662207357859532e-05,
      "loss": 0.8304,
      "step": 14970
    },
    {
      "epoch": 2.0038123265224224,
      "grad_norm": 0.7186334729194641,
      "learning_rate": 1.6599777034559642e-05,
      "loss": 0.8376,
      "step": 14980
    },
    {
      "epoch": 2.0051499849513426,
      "grad_norm": 0.7014551758766174,
      "learning_rate": 1.657748049052397e-05,
      "loss": 0.8446,
      "step": 14990
    },
    {
      "epoch": 2.0064876433802628,
      "grad_norm": 0.6831926703453064,
      "learning_rate": 1.6555183946488294e-05,
      "loss": 0.8472,
      "step": 15000
    },
    {
      "epoch": 2.007825301809183,
      "grad_norm": 0.6894043684005737,
      "learning_rate": 1.653288740245262e-05,
      "loss": 0.8646,
      "step": 15010
    },
    {
      "epoch": 2.009162960238103,
      "grad_norm": 0.675765872001648,
      "learning_rate": 1.6510590858416948e-05,
      "loss": 0.8203,
      "step": 15020
    },
    {
      "epoch": 2.0105006186670233,
      "grad_norm": 0.6936489939689636,
      "learning_rate": 1.6488294314381272e-05,
      "loss": 0.8265,
      "step": 15030
    },
    {
      "epoch": 2.0118382770959435,
      "grad_norm": 0.6507262587547302,
      "learning_rate": 1.64659977703456e-05,
      "loss": 0.8412,
      "step": 15040
    },
    {
      "epoch": 2.0131759355248637,
      "grad_norm": 0.6572941541671753,
      "learning_rate": 1.6443701226309923e-05,
      "loss": 0.8637,
      "step": 15050
    },
    {
      "epoch": 2.014513593953784,
      "grad_norm": 0.649280309677124,
      "learning_rate": 1.642140468227425e-05,
      "loss": 0.8452,
      "step": 15060
    },
    {
      "epoch": 2.015851252382704,
      "grad_norm": 0.7613534927368164,
      "learning_rate": 1.639910813823857e-05,
      "loss": 0.8631,
      "step": 15070
    },
    {
      "epoch": 2.0171889108116243,
      "grad_norm": 0.6813507080078125,
      "learning_rate": 1.6376811594202898e-05,
      "loss": 0.8222,
      "step": 15080
    },
    {
      "epoch": 2.0185265692405445,
      "grad_norm": 0.6332900524139404,
      "learning_rate": 1.6354515050167226e-05,
      "loss": 0.8162,
      "step": 15090
    },
    {
      "epoch": 2.0198642276694647,
      "grad_norm": 0.7641409635543823,
      "learning_rate": 1.633221850613155e-05,
      "loss": 0.8299,
      "step": 15100
    },
    {
      "epoch": 2.021201886098385,
      "grad_norm": 0.6243141889572144,
      "learning_rate": 1.6309921962095877e-05,
      "loss": 0.8206,
      "step": 15110
    },
    {
      "epoch": 2.022539544527305,
      "grad_norm": 0.6336522102355957,
      "learning_rate": 1.62876254180602e-05,
      "loss": 0.8262,
      "step": 15120
    },
    {
      "epoch": 2.0238772029562253,
      "grad_norm": 0.6928791403770447,
      "learning_rate": 1.6265328874024528e-05,
      "loss": 0.8403,
      "step": 15130
    },
    {
      "epoch": 2.0252148613851455,
      "grad_norm": 0.6820346713066101,
      "learning_rate": 1.6243032329988852e-05,
      "loss": 0.8359,
      "step": 15140
    },
    {
      "epoch": 2.0265525198140653,
      "grad_norm": 0.6653479337692261,
      "learning_rate": 1.622073578595318e-05,
      "loss": 0.8263,
      "step": 15150
    },
    {
      "epoch": 2.0278901782429855,
      "grad_norm": 0.6726263761520386,
      "learning_rate": 1.6198439241917503e-05,
      "loss": 0.8367,
      "step": 15160
    },
    {
      "epoch": 2.0292278366719056,
      "grad_norm": 0.6885805130004883,
      "learning_rate": 1.617614269788183e-05,
      "loss": 0.8534,
      "step": 15170
    },
    {
      "epoch": 2.030565495100826,
      "grad_norm": 0.6854448318481445,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 0.8402,
      "step": 15180
    },
    {
      "epoch": 2.031903153529746,
      "grad_norm": 0.6315307021141052,
      "learning_rate": 1.613154960981048e-05,
      "loss": 0.8253,
      "step": 15190
    },
    {
      "epoch": 2.0332408119586662,
      "grad_norm": 0.6740906834602356,
      "learning_rate": 1.610925306577481e-05,
      "loss": 0.8361,
      "step": 15200
    },
    {
      "epoch": 2.0345784703875864,
      "grad_norm": 0.7065014243125916,
      "learning_rate": 1.608695652173913e-05,
      "loss": 0.8576,
      "step": 15210
    },
    {
      "epoch": 2.0359161288165066,
      "grad_norm": 0.628131628036499,
      "learning_rate": 1.6064659977703456e-05,
      "loss": 0.83,
      "step": 15220
    },
    {
      "epoch": 2.037253787245427,
      "grad_norm": 0.6995598077774048,
      "learning_rate": 1.604236343366778e-05,
      "loss": 0.8339,
      "step": 15230
    },
    {
      "epoch": 2.038591445674347,
      "grad_norm": 0.7358569502830505,
      "learning_rate": 1.6020066889632107e-05,
      "loss": 0.8675,
      "step": 15240
    },
    {
      "epoch": 2.039929104103267,
      "grad_norm": 0.7546021342277527,
      "learning_rate": 1.599777034559643e-05,
      "loss": 0.8505,
      "step": 15250
    },
    {
      "epoch": 2.0412667625321874,
      "grad_norm": 0.9401183128356934,
      "learning_rate": 1.597547380156076e-05,
      "loss": 0.8524,
      "step": 15260
    },
    {
      "epoch": 2.0426044209611076,
      "grad_norm": 0.6312670707702637,
      "learning_rate": 1.5953177257525086e-05,
      "loss": 0.8207,
      "step": 15270
    },
    {
      "epoch": 2.043942079390028,
      "grad_norm": 0.682580828666687,
      "learning_rate": 1.593088071348941e-05,
      "loss": 0.8524,
      "step": 15280
    },
    {
      "epoch": 2.045279737818948,
      "grad_norm": 0.6649585366249084,
      "learning_rate": 1.5908584169453737e-05,
      "loss": 0.8417,
      "step": 15290
    },
    {
      "epoch": 2.046617396247868,
      "grad_norm": 0.7612360715866089,
      "learning_rate": 1.588628762541806e-05,
      "loss": 0.805,
      "step": 15300
    },
    {
      "epoch": 2.0479550546767884,
      "grad_norm": 0.774670422077179,
      "learning_rate": 1.5863991081382388e-05,
      "loss": 0.8525,
      "step": 15310
    },
    {
      "epoch": 2.0492927131057086,
      "grad_norm": 0.7420874238014221,
      "learning_rate": 1.5841694537346712e-05,
      "loss": 0.8647,
      "step": 15320
    },
    {
      "epoch": 2.050630371534629,
      "grad_norm": 0.673804521560669,
      "learning_rate": 1.581939799331104e-05,
      "loss": 0.8084,
      "step": 15330
    },
    {
      "epoch": 2.051968029963549,
      "grad_norm": 0.7228033542633057,
      "learning_rate": 1.5797101449275363e-05,
      "loss": 0.8479,
      "step": 15340
    },
    {
      "epoch": 2.053305688392469,
      "grad_norm": 0.732337474822998,
      "learning_rate": 1.5774804905239687e-05,
      "loss": 0.8242,
      "step": 15350
    },
    {
      "epoch": 2.054643346821389,
      "grad_norm": 0.6557018160820007,
      "learning_rate": 1.5752508361204014e-05,
      "loss": 0.8609,
      "step": 15360
    },
    {
      "epoch": 2.055981005250309,
      "grad_norm": 0.8345051407814026,
      "learning_rate": 1.5730211817168338e-05,
      "loss": 0.8476,
      "step": 15370
    },
    {
      "epoch": 2.0573186636792293,
      "grad_norm": 0.6700716018676758,
      "learning_rate": 1.5707915273132665e-05,
      "loss": 0.8325,
      "step": 15380
    },
    {
      "epoch": 2.0586563221081495,
      "grad_norm": 0.6941998600959778,
      "learning_rate": 1.568561872909699e-05,
      "loss": 0.8344,
      "step": 15390
    },
    {
      "epoch": 2.0599939805370697,
      "grad_norm": 0.7182976603507996,
      "learning_rate": 1.5663322185061317e-05,
      "loss": 0.8213,
      "step": 15400
    },
    {
      "epoch": 2.06133163896599,
      "grad_norm": 0.6385403275489807,
      "learning_rate": 1.564102564102564e-05,
      "loss": 0.82,
      "step": 15410
    },
    {
      "epoch": 2.06266929739491,
      "grad_norm": 0.6633094549179077,
      "learning_rate": 1.5618729096989968e-05,
      "loss": 0.828,
      "step": 15420
    },
    {
      "epoch": 2.0640069558238303,
      "grad_norm": 0.6536054015159607,
      "learning_rate": 1.559643255295429e-05,
      "loss": 0.8477,
      "step": 15430
    },
    {
      "epoch": 2.0653446142527505,
      "grad_norm": 0.6773852705955505,
      "learning_rate": 1.557413600891862e-05,
      "loss": 0.8374,
      "step": 15440
    },
    {
      "epoch": 2.0666822726816707,
      "grad_norm": 0.6760568618774414,
      "learning_rate": 1.5551839464882946e-05,
      "loss": 0.8162,
      "step": 15450
    },
    {
      "epoch": 2.068019931110591,
      "grad_norm": 0.6628260612487793,
      "learning_rate": 1.552954292084727e-05,
      "loss": 0.8207,
      "step": 15460
    },
    {
      "epoch": 2.069357589539511,
      "grad_norm": 0.6997703909873962,
      "learning_rate": 1.5507246376811597e-05,
      "loss": 0.8218,
      "step": 15470
    },
    {
      "epoch": 2.0706952479684313,
      "grad_norm": 0.6648243069648743,
      "learning_rate": 1.548494983277592e-05,
      "loss": 0.8188,
      "step": 15480
    },
    {
      "epoch": 2.0720329063973515,
      "grad_norm": 0.6082021594047546,
      "learning_rate": 1.5462653288740245e-05,
      "loss": 0.8163,
      "step": 15490
    },
    {
      "epoch": 2.0733705648262717,
      "grad_norm": 0.7350712418556213,
      "learning_rate": 1.544035674470457e-05,
      "loss": 0.8252,
      "step": 15500
    },
    {
      "epoch": 2.074708223255192,
      "grad_norm": 0.6351609230041504,
      "learning_rate": 1.5418060200668896e-05,
      "loss": 0.8327,
      "step": 15510
    },
    {
      "epoch": 2.076045881684112,
      "grad_norm": 0.7173452377319336,
      "learning_rate": 1.5395763656633224e-05,
      "loss": 0.845,
      "step": 15520
    },
    {
      "epoch": 2.0773835401130323,
      "grad_norm": 0.6961542963981628,
      "learning_rate": 1.5373467112597547e-05,
      "loss": 0.8361,
      "step": 15530
    },
    {
      "epoch": 2.0787211985419525,
      "grad_norm": 0.7052386999130249,
      "learning_rate": 1.5351170568561875e-05,
      "loss": 0.8674,
      "step": 15540
    },
    {
      "epoch": 2.0800588569708727,
      "grad_norm": 0.6870942115783691,
      "learning_rate": 1.53288740245262e-05,
      "loss": 0.8362,
      "step": 15550
    },
    {
      "epoch": 2.081396515399793,
      "grad_norm": 0.6254249811172485,
      "learning_rate": 1.5306577480490526e-05,
      "loss": 0.8257,
      "step": 15560
    },
    {
      "epoch": 2.0827341738287126,
      "grad_norm": 0.703648567199707,
      "learning_rate": 1.528428093645485e-05,
      "loss": 0.8414,
      "step": 15570
    },
    {
      "epoch": 2.084071832257633,
      "grad_norm": 0.6565263867378235,
      "learning_rate": 1.5261984392419177e-05,
      "loss": 0.8609,
      "step": 15580
    },
    {
      "epoch": 2.085409490686553,
      "grad_norm": 0.7448270916938782,
      "learning_rate": 1.5239687848383503e-05,
      "loss": 0.8352,
      "step": 15590
    },
    {
      "epoch": 2.086747149115473,
      "grad_norm": 0.678335964679718,
      "learning_rate": 1.5217391304347828e-05,
      "loss": 0.848,
      "step": 15600
    },
    {
      "epoch": 2.0880848075443934,
      "grad_norm": 0.6779804229736328,
      "learning_rate": 1.5195094760312154e-05,
      "loss": 0.8403,
      "step": 15610
    },
    {
      "epoch": 2.0894224659733136,
      "grad_norm": 0.6486919522285461,
      "learning_rate": 1.517279821627648e-05,
      "loss": 0.8364,
      "step": 15620
    },
    {
      "epoch": 2.090760124402234,
      "grad_norm": 0.6276295185089111,
      "learning_rate": 1.5150501672240801e-05,
      "loss": 0.8129,
      "step": 15630
    },
    {
      "epoch": 2.092097782831154,
      "grad_norm": 0.6802119612693787,
      "learning_rate": 1.5128205128205129e-05,
      "loss": 0.8522,
      "step": 15640
    },
    {
      "epoch": 2.093435441260074,
      "grad_norm": 0.7781357765197754,
      "learning_rate": 1.5105908584169454e-05,
      "loss": 0.8499,
      "step": 15650
    },
    {
      "epoch": 2.0947730996889944,
      "grad_norm": 0.7050586342811584,
      "learning_rate": 1.508361204013378e-05,
      "loss": 0.8285,
      "step": 15660
    },
    {
      "epoch": 2.0961107581179146,
      "grad_norm": 0.6895449161529541,
      "learning_rate": 1.5061315496098105e-05,
      "loss": 0.8371,
      "step": 15670
    },
    {
      "epoch": 2.097448416546835,
      "grad_norm": 0.6958582401275635,
      "learning_rate": 1.5039018952062431e-05,
      "loss": 0.8376,
      "step": 15680
    },
    {
      "epoch": 2.098786074975755,
      "grad_norm": 0.7107146978378296,
      "learning_rate": 1.5016722408026757e-05,
      "loss": 0.8702,
      "step": 15690
    },
    {
      "epoch": 2.100123733404675,
      "grad_norm": 0.6352638006210327,
      "learning_rate": 1.4994425863991082e-05,
      "loss": 0.8334,
      "step": 15700
    },
    {
      "epoch": 2.1014613918335954,
      "grad_norm": 0.6289107799530029,
      "learning_rate": 1.4972129319955408e-05,
      "loss": 0.818,
      "step": 15710
    },
    {
      "epoch": 2.1027990502625156,
      "grad_norm": 0.6506332159042358,
      "learning_rate": 1.4949832775919733e-05,
      "loss": 0.8311,
      "step": 15720
    },
    {
      "epoch": 2.1041367086914358,
      "grad_norm": 0.6735500693321228,
      "learning_rate": 1.4927536231884059e-05,
      "loss": 0.8424,
      "step": 15730
    },
    {
      "epoch": 2.105474367120356,
      "grad_norm": 0.7150118947029114,
      "learning_rate": 1.4905239687848384e-05,
      "loss": 0.8349,
      "step": 15740
    },
    {
      "epoch": 2.106812025549276,
      "grad_norm": 0.6904240250587463,
      "learning_rate": 1.4882943143812712e-05,
      "loss": 0.8243,
      "step": 15750
    },
    {
      "epoch": 2.1081496839781964,
      "grad_norm": 0.6956101655960083,
      "learning_rate": 1.4860646599777034e-05,
      "loss": 0.8508,
      "step": 15760
    },
    {
      "epoch": 2.1094873424071166,
      "grad_norm": 0.6740187406539917,
      "learning_rate": 1.483835005574136e-05,
      "loss": 0.8334,
      "step": 15770
    },
    {
      "epoch": 2.1108250008360363,
      "grad_norm": 0.6788790822029114,
      "learning_rate": 1.4816053511705685e-05,
      "loss": 0.866,
      "step": 15780
    },
    {
      "epoch": 2.1121626592649565,
      "grad_norm": 0.7140653729438782,
      "learning_rate": 1.479375696767001e-05,
      "loss": 0.843,
      "step": 15790
    },
    {
      "epoch": 2.1135003176938767,
      "grad_norm": 0.6155518889427185,
      "learning_rate": 1.4771460423634336e-05,
      "loss": 0.8488,
      "step": 15800
    },
    {
      "epoch": 2.114837976122797,
      "grad_norm": 0.6469929814338684,
      "learning_rate": 1.4749163879598662e-05,
      "loss": 0.8383,
      "step": 15810
    },
    {
      "epoch": 2.116175634551717,
      "grad_norm": 0.6930462121963501,
      "learning_rate": 1.4726867335562989e-05,
      "loss": 0.8188,
      "step": 15820
    },
    {
      "epoch": 2.1175132929806373,
      "grad_norm": 0.6693437695503235,
      "learning_rate": 1.4704570791527315e-05,
      "loss": 0.8421,
      "step": 15830
    },
    {
      "epoch": 2.1188509514095575,
      "grad_norm": 0.6930723786354065,
      "learning_rate": 1.468227424749164e-05,
      "loss": 0.8576,
      "step": 15840
    },
    {
      "epoch": 2.1201886098384777,
      "grad_norm": 0.6326183080673218,
      "learning_rate": 1.4659977703455966e-05,
      "loss": 0.8322,
      "step": 15850
    },
    {
      "epoch": 2.121526268267398,
      "grad_norm": 0.6297175884246826,
      "learning_rate": 1.4637681159420291e-05,
      "loss": 0.8352,
      "step": 15860
    },
    {
      "epoch": 2.122863926696318,
      "grad_norm": 0.7041166424751282,
      "learning_rate": 1.4615384615384617e-05,
      "loss": 0.8183,
      "step": 15870
    },
    {
      "epoch": 2.1242015851252383,
      "grad_norm": 0.6547060608863831,
      "learning_rate": 1.4593088071348943e-05,
      "loss": 0.8669,
      "step": 15880
    },
    {
      "epoch": 2.1255392435541585,
      "grad_norm": 0.6661016941070557,
      "learning_rate": 1.4570791527313268e-05,
      "loss": 0.829,
      "step": 15890
    },
    {
      "epoch": 2.1268769019830787,
      "grad_norm": 0.6406869888305664,
      "learning_rate": 1.4548494983277592e-05,
      "loss": 0.8195,
      "step": 15900
    },
    {
      "epoch": 2.128214560411999,
      "grad_norm": 0.7401942014694214,
      "learning_rate": 1.4526198439241918e-05,
      "loss": 0.8305,
      "step": 15910
    },
    {
      "epoch": 2.129552218840919,
      "grad_norm": 0.6948599219322205,
      "learning_rate": 1.4503901895206243e-05,
      "loss": 0.8611,
      "step": 15920
    },
    {
      "epoch": 2.1308898772698392,
      "grad_norm": 0.6690046787261963,
      "learning_rate": 1.4481605351170569e-05,
      "loss": 0.8365,
      "step": 15930
    },
    {
      "epoch": 2.1322275356987594,
      "grad_norm": 0.7471900582313538,
      "learning_rate": 1.4459308807134894e-05,
      "loss": 0.848,
      "step": 15940
    },
    {
      "epoch": 2.1335651941276796,
      "grad_norm": 0.6634005904197693,
      "learning_rate": 1.443701226309922e-05,
      "loss": 0.8249,
      "step": 15950
    },
    {
      "epoch": 2.1349028525566,
      "grad_norm": 0.7210509777069092,
      "learning_rate": 1.4414715719063545e-05,
      "loss": 0.8582,
      "step": 15960
    },
    {
      "epoch": 2.13624051098552,
      "grad_norm": 0.6635289788246155,
      "learning_rate": 1.4392419175027871e-05,
      "loss": 0.8358,
      "step": 15970
    },
    {
      "epoch": 2.1375781694144402,
      "grad_norm": 0.7084854245185852,
      "learning_rate": 1.4370122630992197e-05,
      "loss": 0.8547,
      "step": 15980
    },
    {
      "epoch": 2.13891582784336,
      "grad_norm": 0.7859167456626892,
      "learning_rate": 1.4347826086956522e-05,
      "loss": 0.8315,
      "step": 15990
    },
    {
      "epoch": 2.14025348627228,
      "grad_norm": 0.700043797492981,
      "learning_rate": 1.4325529542920848e-05,
      "loss": 0.8229,
      "step": 16000
    },
    {
      "epoch": 2.1415911447012004,
      "grad_norm": 0.6907942891120911,
      "learning_rate": 1.4303232998885175e-05,
      "loss": 0.8351,
      "step": 16010
    },
    {
      "epoch": 2.1429288031301206,
      "grad_norm": 0.7181026339530945,
      "learning_rate": 1.42809364548495e-05,
      "loss": 0.8156,
      "step": 16020
    },
    {
      "epoch": 2.1442664615590408,
      "grad_norm": 0.7117822766304016,
      "learning_rate": 1.4258639910813826e-05,
      "loss": 0.8618,
      "step": 16030
    },
    {
      "epoch": 2.145604119987961,
      "grad_norm": 0.6502979397773743,
      "learning_rate": 1.4236343366778148e-05,
      "loss": 0.8392,
      "step": 16040
    },
    {
      "epoch": 2.146941778416881,
      "grad_norm": 0.6845847368240356,
      "learning_rate": 1.4214046822742474e-05,
      "loss": 0.8508,
      "step": 16050
    },
    {
      "epoch": 2.1482794368458014,
      "grad_norm": 0.7345606088638306,
      "learning_rate": 1.41917502787068e-05,
      "loss": 0.843,
      "step": 16060
    },
    {
      "epoch": 2.1496170952747216,
      "grad_norm": 0.6316173672676086,
      "learning_rate": 1.4169453734671125e-05,
      "loss": 0.8244,
      "step": 16070
    },
    {
      "epoch": 2.1509547537036418,
      "grad_norm": 0.7418650388717651,
      "learning_rate": 1.4147157190635452e-05,
      "loss": 0.8536,
      "step": 16080
    },
    {
      "epoch": 2.152292412132562,
      "grad_norm": 0.7552705407142639,
      "learning_rate": 1.4124860646599778e-05,
      "loss": 0.8377,
      "step": 16090
    },
    {
      "epoch": 2.153630070561482,
      "grad_norm": 0.7314440011978149,
      "learning_rate": 1.4102564102564104e-05,
      "loss": 0.8479,
      "step": 16100
    },
    {
      "epoch": 2.1549677289904023,
      "grad_norm": 0.7132818102836609,
      "learning_rate": 1.4080267558528429e-05,
      "loss": 0.8401,
      "step": 16110
    },
    {
      "epoch": 2.1563053874193225,
      "grad_norm": 0.650977611541748,
      "learning_rate": 1.4057971014492755e-05,
      "loss": 0.8462,
      "step": 16120
    },
    {
      "epoch": 2.1576430458482427,
      "grad_norm": 0.6815112829208374,
      "learning_rate": 1.403567447045708e-05,
      "loss": 0.8592,
      "step": 16130
    },
    {
      "epoch": 2.158980704277163,
      "grad_norm": 0.7343469262123108,
      "learning_rate": 1.4013377926421406e-05,
      "loss": 0.8687,
      "step": 16140
    },
    {
      "epoch": 2.160318362706083,
      "grad_norm": 0.6684229969978333,
      "learning_rate": 1.3991081382385731e-05,
      "loss": 0.8429,
      "step": 16150
    },
    {
      "epoch": 2.1616560211350033,
      "grad_norm": 0.6527325510978699,
      "learning_rate": 1.3968784838350057e-05,
      "loss": 0.8214,
      "step": 16160
    },
    {
      "epoch": 2.1629936795639235,
      "grad_norm": 0.7359947562217712,
      "learning_rate": 1.3946488294314383e-05,
      "loss": 0.8443,
      "step": 16170
    },
    {
      "epoch": 2.1643313379928437,
      "grad_norm": 0.7044106125831604,
      "learning_rate": 1.3924191750278706e-05,
      "loss": 0.848,
      "step": 16180
    },
    {
      "epoch": 2.165668996421764,
      "grad_norm": 0.6819502115249634,
      "learning_rate": 1.3901895206243032e-05,
      "loss": 0.8606,
      "step": 16190
    },
    {
      "epoch": 2.1670066548506837,
      "grad_norm": 0.7197907567024231,
      "learning_rate": 1.3879598662207358e-05,
      "loss": 0.8044,
      "step": 16200
    },
    {
      "epoch": 2.168344313279604,
      "grad_norm": 0.7556570172309875,
      "learning_rate": 1.3857302118171683e-05,
      "loss": 0.8275,
      "step": 16210
    },
    {
      "epoch": 2.169681971708524,
      "grad_norm": 0.6735523343086243,
      "learning_rate": 1.3835005574136009e-05,
      "loss": 0.8363,
      "step": 16220
    },
    {
      "epoch": 2.1710196301374443,
      "grad_norm": 0.645840048789978,
      "learning_rate": 1.3812709030100334e-05,
      "loss": 0.8484,
      "step": 16230
    },
    {
      "epoch": 2.1723572885663645,
      "grad_norm": 0.6934475302696228,
      "learning_rate": 1.379041248606466e-05,
      "loss": 0.8525,
      "step": 16240
    },
    {
      "epoch": 2.1736949469952846,
      "grad_norm": 0.6455333232879639,
      "learning_rate": 1.3768115942028985e-05,
      "loss": 0.8376,
      "step": 16250
    },
    {
      "epoch": 2.175032605424205,
      "grad_norm": 0.6874380707740784,
      "learning_rate": 1.3745819397993313e-05,
      "loss": 0.8001,
      "step": 16260
    },
    {
      "epoch": 2.176370263853125,
      "grad_norm": 0.655774712562561,
      "learning_rate": 1.3723522853957638e-05,
      "loss": 0.8487,
      "step": 16270
    },
    {
      "epoch": 2.1777079222820452,
      "grad_norm": 0.7284954190254211,
      "learning_rate": 1.3701226309921964e-05,
      "loss": 0.8403,
      "step": 16280
    },
    {
      "epoch": 2.1790455807109654,
      "grad_norm": 0.6026668548583984,
      "learning_rate": 1.367892976588629e-05,
      "loss": 0.8388,
      "step": 16290
    },
    {
      "epoch": 2.1803832391398856,
      "grad_norm": 0.6785810589790344,
      "learning_rate": 1.3656633221850615e-05,
      "loss": 0.8281,
      "step": 16300
    },
    {
      "epoch": 2.181720897568806,
      "grad_norm": 0.6834210753440857,
      "learning_rate": 1.363433667781494e-05,
      "loss": 0.8479,
      "step": 16310
    },
    {
      "epoch": 2.183058555997726,
      "grad_norm": 0.6450226306915283,
      "learning_rate": 1.3612040133779263e-05,
      "loss": 0.8101,
      "step": 16320
    },
    {
      "epoch": 2.184396214426646,
      "grad_norm": 0.8832374811172485,
      "learning_rate": 1.358974358974359e-05,
      "loss": 0.8426,
      "step": 16330
    },
    {
      "epoch": 2.1857338728555664,
      "grad_norm": 0.6225557923316956,
      "learning_rate": 1.3567447045707916e-05,
      "loss": 0.853,
      "step": 16340
    },
    {
      "epoch": 2.1870715312844866,
      "grad_norm": 0.6787270903587341,
      "learning_rate": 1.3545150501672241e-05,
      "loss": 0.8136,
      "step": 16350
    },
    {
      "epoch": 2.188409189713407,
      "grad_norm": 0.7111114263534546,
      "learning_rate": 1.3522853957636567e-05,
      "loss": 0.8496,
      "step": 16360
    },
    {
      "epoch": 2.189746848142327,
      "grad_norm": 0.6682127118110657,
      "learning_rate": 1.3500557413600892e-05,
      "loss": 0.8132,
      "step": 16370
    },
    {
      "epoch": 2.191084506571247,
      "grad_norm": 0.6891046762466431,
      "learning_rate": 1.3478260869565218e-05,
      "loss": 0.8381,
      "step": 16380
    },
    {
      "epoch": 2.1924221650001674,
      "grad_norm": 0.7283604741096497,
      "learning_rate": 1.3455964325529543e-05,
      "loss": 0.8456,
      "step": 16390
    },
    {
      "epoch": 2.1937598234290876,
      "grad_norm": 0.6909322142601013,
      "learning_rate": 1.3433667781493869e-05,
      "loss": 0.847,
      "step": 16400
    },
    {
      "epoch": 2.1950974818580073,
      "grad_norm": 0.6602782607078552,
      "learning_rate": 1.3411371237458195e-05,
      "loss": 0.8179,
      "step": 16410
    },
    {
      "epoch": 2.1964351402869275,
      "grad_norm": 0.7869844436645508,
      "learning_rate": 1.338907469342252e-05,
      "loss": 0.8734,
      "step": 16420
    },
    {
      "epoch": 2.1977727987158477,
      "grad_norm": 0.7468708753585815,
      "learning_rate": 1.3366778149386846e-05,
      "loss": 0.8756,
      "step": 16430
    },
    {
      "epoch": 2.199110457144768,
      "grad_norm": 0.6100991368293762,
      "learning_rate": 1.3344481605351173e-05,
      "loss": 0.8278,
      "step": 16440
    },
    {
      "epoch": 2.200448115573688,
      "grad_norm": 0.6628062129020691,
      "learning_rate": 1.3322185061315499e-05,
      "loss": 0.845,
      "step": 16450
    },
    {
      "epoch": 2.2017857740026083,
      "grad_norm": 0.6554341912269592,
      "learning_rate": 1.329988851727982e-05,
      "loss": 0.8453,
      "step": 16460
    },
    {
      "epoch": 2.2031234324315285,
      "grad_norm": 0.6914805769920349,
      "learning_rate": 1.3277591973244146e-05,
      "loss": 0.8458,
      "step": 16470
    },
    {
      "epoch": 2.2044610908604487,
      "grad_norm": 0.6748400330543518,
      "learning_rate": 1.3255295429208472e-05,
      "loss": 0.8312,
      "step": 16480
    },
    {
      "epoch": 2.205798749289369,
      "grad_norm": 0.6857201457023621,
      "learning_rate": 1.3232998885172798e-05,
      "loss": 0.8384,
      "step": 16490
    },
    {
      "epoch": 2.207136407718289,
      "grad_norm": 0.7383608818054199,
      "learning_rate": 1.3210702341137123e-05,
      "loss": 0.8526,
      "step": 16500
    },
    {
      "epoch": 2.2084740661472093,
      "grad_norm": 0.7110224366188049,
      "learning_rate": 1.318840579710145e-05,
      "loss": 0.8317,
      "step": 16510
    },
    {
      "epoch": 2.2098117245761295,
      "grad_norm": 0.7526863813400269,
      "learning_rate": 1.3166109253065776e-05,
      "loss": 0.8467,
      "step": 16520
    },
    {
      "epoch": 2.2111493830050497,
      "grad_norm": 0.7408971786499023,
      "learning_rate": 1.3143812709030102e-05,
      "loss": 0.8375,
      "step": 16530
    },
    {
      "epoch": 2.21248704143397,
      "grad_norm": 0.6260021924972534,
      "learning_rate": 1.3121516164994427e-05,
      "loss": 0.8314,
      "step": 16540
    },
    {
      "epoch": 2.21382469986289,
      "grad_norm": 0.6542813181877136,
      "learning_rate": 1.3099219620958753e-05,
      "loss": 0.871,
      "step": 16550
    },
    {
      "epoch": 2.2151623582918103,
      "grad_norm": 0.6967830061912537,
      "learning_rate": 1.3076923076923078e-05,
      "loss": 0.8383,
      "step": 16560
    },
    {
      "epoch": 2.2165000167207305,
      "grad_norm": 0.696747899055481,
      "learning_rate": 1.3054626532887404e-05,
      "loss": 0.8511,
      "step": 16570
    },
    {
      "epoch": 2.2178376751496507,
      "grad_norm": 0.6342881321907043,
      "learning_rate": 1.303232998885173e-05,
      "loss": 0.8507,
      "step": 16580
    },
    {
      "epoch": 2.219175333578571,
      "grad_norm": 0.6695420742034912,
      "learning_rate": 1.3010033444816055e-05,
      "loss": 0.8363,
      "step": 16590
    },
    {
      "epoch": 2.220512992007491,
      "grad_norm": 0.7186545133590698,
      "learning_rate": 1.2987736900780379e-05,
      "loss": 0.8273,
      "step": 16600
    },
    {
      "epoch": 2.2218506504364113,
      "grad_norm": 0.7277381420135498,
      "learning_rate": 1.2965440356744704e-05,
      "loss": 0.8541,
      "step": 16610
    },
    {
      "epoch": 2.223188308865331,
      "grad_norm": 0.7176023721694946,
      "learning_rate": 1.294314381270903e-05,
      "loss": 0.8609,
      "step": 16620
    },
    {
      "epoch": 2.2245259672942512,
      "grad_norm": 0.7056664824485779,
      "learning_rate": 1.2920847268673356e-05,
      "loss": 0.8261,
      "step": 16630
    },
    {
      "epoch": 2.2258636257231714,
      "grad_norm": 0.6600198149681091,
      "learning_rate": 1.2898550724637681e-05,
      "loss": 0.8423,
      "step": 16640
    },
    {
      "epoch": 2.2272012841520916,
      "grad_norm": 0.5894295573234558,
      "learning_rate": 1.2876254180602007e-05,
      "loss": 0.8416,
      "step": 16650
    },
    {
      "epoch": 2.228538942581012,
      "grad_norm": 0.6572135090827942,
      "learning_rate": 1.2853957636566332e-05,
      "loss": 0.8664,
      "step": 16660
    },
    {
      "epoch": 2.229876601009932,
      "grad_norm": 0.6588666439056396,
      "learning_rate": 1.2831661092530658e-05,
      "loss": 0.8576,
      "step": 16670
    },
    {
      "epoch": 2.231214259438852,
      "grad_norm": 0.6905719637870789,
      "learning_rate": 1.2809364548494983e-05,
      "loss": 0.8473,
      "step": 16680
    },
    {
      "epoch": 2.2325519178677724,
      "grad_norm": 0.7373539209365845,
      "learning_rate": 1.278706800445931e-05,
      "loss": 0.8328,
      "step": 16690
    },
    {
      "epoch": 2.2338895762966926,
      "grad_norm": 0.7107921838760376,
      "learning_rate": 1.2764771460423636e-05,
      "loss": 0.828,
      "step": 16700
    },
    {
      "epoch": 2.235227234725613,
      "grad_norm": 0.6961853504180908,
      "learning_rate": 1.2742474916387962e-05,
      "loss": 0.8151,
      "step": 16710
    },
    {
      "epoch": 2.236564893154533,
      "grad_norm": 0.7335192561149597,
      "learning_rate": 1.2720178372352287e-05,
      "loss": 0.8632,
      "step": 16720
    },
    {
      "epoch": 2.237902551583453,
      "grad_norm": 0.669358491897583,
      "learning_rate": 1.2697881828316613e-05,
      "loss": 0.8473,
      "step": 16730
    },
    {
      "epoch": 2.2392402100123734,
      "grad_norm": 0.6621600389480591,
      "learning_rate": 1.2675585284280935e-05,
      "loss": 0.8244,
      "step": 16740
    },
    {
      "epoch": 2.2405778684412936,
      "grad_norm": 0.7064948678016663,
      "learning_rate": 1.265328874024526e-05,
      "loss": 0.8543,
      "step": 16750
    },
    {
      "epoch": 2.241915526870214,
      "grad_norm": 0.7062839865684509,
      "learning_rate": 1.2630992196209588e-05,
      "loss": 0.8613,
      "step": 16760
    },
    {
      "epoch": 2.243253185299134,
      "grad_norm": 0.7061159014701843,
      "learning_rate": 1.2608695652173914e-05,
      "loss": 0.848,
      "step": 16770
    },
    {
      "epoch": 2.244590843728054,
      "grad_norm": 0.8064941763877869,
      "learning_rate": 1.258639910813824e-05,
      "loss": 0.8269,
      "step": 16780
    },
    {
      "epoch": 2.2459285021569744,
      "grad_norm": 0.6719012260437012,
      "learning_rate": 1.2564102564102565e-05,
      "loss": 0.8457,
      "step": 16790
    },
    {
      "epoch": 2.2472661605858946,
      "grad_norm": 0.6718325614929199,
      "learning_rate": 1.254180602006689e-05,
      "loss": 0.8476,
      "step": 16800
    },
    {
      "epoch": 2.2486038190148148,
      "grad_norm": 0.6928899884223938,
      "learning_rate": 1.2519509476031216e-05,
      "loss": 0.8191,
      "step": 16810
    },
    {
      "epoch": 2.249941477443735,
      "grad_norm": 0.7420215010643005,
      "learning_rate": 1.2497212931995542e-05,
      "loss": 0.8619,
      "step": 16820
    },
    {
      "epoch": 2.2512791358726547,
      "grad_norm": 0.6815014481544495,
      "learning_rate": 1.2474916387959867e-05,
      "loss": 0.8453,
      "step": 16830
    },
    {
      "epoch": 2.252616794301575,
      "grad_norm": 0.6945222616195679,
      "learning_rate": 1.2452619843924191e-05,
      "loss": 0.8756,
      "step": 16840
    },
    {
      "epoch": 2.253954452730495,
      "grad_norm": 0.8007532358169556,
      "learning_rate": 1.2430323299888518e-05,
      "loss": 0.8265,
      "step": 16850
    },
    {
      "epoch": 2.2552921111594153,
      "grad_norm": 0.6993554830551147,
      "learning_rate": 1.2408026755852844e-05,
      "loss": 0.8435,
      "step": 16860
    },
    {
      "epoch": 2.2566297695883355,
      "grad_norm": 0.6929347515106201,
      "learning_rate": 1.238573021181717e-05,
      "loss": 0.8159,
      "step": 16870
    },
    {
      "epoch": 2.2579674280172557,
      "grad_norm": 0.7175235748291016,
      "learning_rate": 1.2363433667781495e-05,
      "loss": 0.8399,
      "step": 16880
    },
    {
      "epoch": 2.259305086446176,
      "grad_norm": 0.7193416953086853,
      "learning_rate": 1.234113712374582e-05,
      "loss": 0.8416,
      "step": 16890
    },
    {
      "epoch": 2.260642744875096,
      "grad_norm": 0.9748609066009521,
      "learning_rate": 1.2318840579710146e-05,
      "loss": 0.8487,
      "step": 16900
    },
    {
      "epoch": 2.2619804033040163,
      "grad_norm": 0.6663805246353149,
      "learning_rate": 1.229654403567447e-05,
      "loss": 0.8758,
      "step": 16910
    },
    {
      "epoch": 2.2633180617329365,
      "grad_norm": 0.6773489117622375,
      "learning_rate": 1.2274247491638796e-05,
      "loss": 0.8352,
      "step": 16920
    },
    {
      "epoch": 2.2646557201618567,
      "grad_norm": 0.744850218296051,
      "learning_rate": 1.2251950947603121e-05,
      "loss": 0.8575,
      "step": 16930
    },
    {
      "epoch": 2.265993378590777,
      "grad_norm": 0.6634360551834106,
      "learning_rate": 1.2229654403567448e-05,
      "loss": 0.8298,
      "step": 16940
    },
    {
      "epoch": 2.267331037019697,
      "grad_norm": 0.7434285879135132,
      "learning_rate": 1.2207357859531774e-05,
      "loss": 0.832,
      "step": 16950
    },
    {
      "epoch": 2.2686686954486173,
      "grad_norm": 0.6797789335250854,
      "learning_rate": 1.21850613154961e-05,
      "loss": 0.8183,
      "step": 16960
    },
    {
      "epoch": 2.2700063538775375,
      "grad_norm": 0.6403242945671082,
      "learning_rate": 1.2162764771460425e-05,
      "loss": 0.8456,
      "step": 16970
    },
    {
      "epoch": 2.2713440123064577,
      "grad_norm": 0.6092551350593567,
      "learning_rate": 1.2140468227424749e-05,
      "loss": 0.8247,
      "step": 16980
    },
    {
      "epoch": 2.272681670735378,
      "grad_norm": 0.7423738241195679,
      "learning_rate": 1.2118171683389075e-05,
      "loss": 0.8455,
      "step": 16990
    },
    {
      "epoch": 2.274019329164298,
      "grad_norm": 0.665117084980011,
      "learning_rate": 1.20958751393534e-05,
      "loss": 0.8378,
      "step": 17000
    },
    {
      "epoch": 2.2753569875932182,
      "grad_norm": 0.6307120323181152,
      "learning_rate": 1.2073578595317726e-05,
      "loss": 0.8209,
      "step": 17010
    },
    {
      "epoch": 2.276694646022138,
      "grad_norm": 0.6342702507972717,
      "learning_rate": 1.2051282051282051e-05,
      "loss": 0.8604,
      "step": 17020
    },
    {
      "epoch": 2.2780323044510586,
      "grad_norm": 0.711648166179657,
      "learning_rate": 1.2028985507246379e-05,
      "loss": 0.8504,
      "step": 17030
    },
    {
      "epoch": 2.2793699628799784,
      "grad_norm": 0.701002299785614,
      "learning_rate": 1.2006688963210704e-05,
      "loss": 0.8455,
      "step": 17040
    },
    {
      "epoch": 2.2807076213088986,
      "grad_norm": 0.7161061763763428,
      "learning_rate": 1.1984392419175028e-05,
      "loss": 0.8535,
      "step": 17050
    },
    {
      "epoch": 2.282045279737819,
      "grad_norm": 0.6842841506004333,
      "learning_rate": 1.1962095875139354e-05,
      "loss": 0.8522,
      "step": 17060
    },
    {
      "epoch": 2.283382938166739,
      "grad_norm": 0.7288981676101685,
      "learning_rate": 1.193979933110368e-05,
      "loss": 0.8247,
      "step": 17070
    },
    {
      "epoch": 2.284720596595659,
      "grad_norm": 0.6626722812652588,
      "learning_rate": 1.1917502787068005e-05,
      "loss": 0.8406,
      "step": 17080
    },
    {
      "epoch": 2.2860582550245794,
      "grad_norm": 0.7215436697006226,
      "learning_rate": 1.189520624303233e-05,
      "loss": 0.8316,
      "step": 17090
    },
    {
      "epoch": 2.2873959134534996,
      "grad_norm": 0.6991207599639893,
      "learning_rate": 1.1872909698996656e-05,
      "loss": 0.8271,
      "step": 17100
    },
    {
      "epoch": 2.2887335718824198,
      "grad_norm": 0.702549397945404,
      "learning_rate": 1.1850613154960981e-05,
      "loss": 0.8274,
      "step": 17110
    },
    {
      "epoch": 2.29007123031134,
      "grad_norm": 0.7044080495834351,
      "learning_rate": 1.1828316610925307e-05,
      "loss": 0.8165,
      "step": 17120
    },
    {
      "epoch": 2.29140888874026,
      "grad_norm": 0.713735818862915,
      "learning_rate": 1.1806020066889633e-05,
      "loss": 0.8432,
      "step": 17130
    },
    {
      "epoch": 2.2927465471691804,
      "grad_norm": 0.6571128964424133,
      "learning_rate": 1.1783723522853958e-05,
      "loss": 0.8417,
      "step": 17140
    },
    {
      "epoch": 2.2940842055981006,
      "grad_norm": 0.675464928150177,
      "learning_rate": 1.1761426978818284e-05,
      "loss": 0.8325,
      "step": 17150
    },
    {
      "epoch": 2.2954218640270208,
      "grad_norm": 0.6329365372657776,
      "learning_rate": 1.173913043478261e-05,
      "loss": 0.8507,
      "step": 17160
    },
    {
      "epoch": 2.296759522455941,
      "grad_norm": 0.7041489481925964,
      "learning_rate": 1.1716833890746935e-05,
      "loss": 0.8273,
      "step": 17170
    },
    {
      "epoch": 2.298097180884861,
      "grad_norm": 0.7417690753936768,
      "learning_rate": 1.169453734671126e-05,
      "loss": 0.8181,
      "step": 17180
    },
    {
      "epoch": 2.2994348393137813,
      "grad_norm": 0.6574972867965698,
      "learning_rate": 1.1672240802675586e-05,
      "loss": 0.8439,
      "step": 17190
    },
    {
      "epoch": 2.3007724977427015,
      "grad_norm": 0.6950040459632874,
      "learning_rate": 1.1649944258639912e-05,
      "loss": 0.8188,
      "step": 17200
    },
    {
      "epoch": 2.3021101561716217,
      "grad_norm": 0.7406788468360901,
      "learning_rate": 1.1627647714604237e-05,
      "loss": 0.8616,
      "step": 17210
    },
    {
      "epoch": 2.303447814600542,
      "grad_norm": 0.6856030225753784,
      "learning_rate": 1.1605351170568563e-05,
      "loss": 0.8535,
      "step": 17220
    },
    {
      "epoch": 2.3047854730294617,
      "grad_norm": 0.6626178622245789,
      "learning_rate": 1.1583054626532888e-05,
      "loss": 0.8401,
      "step": 17230
    },
    {
      "epoch": 2.3061231314583823,
      "grad_norm": 0.6948542594909668,
      "learning_rate": 1.1560758082497214e-05,
      "loss": 0.8426,
      "step": 17240
    },
    {
      "epoch": 2.307460789887302,
      "grad_norm": 0.7428217530250549,
      "learning_rate": 1.153846153846154e-05,
      "loss": 0.8456,
      "step": 17250
    },
    {
      "epoch": 2.3087984483162223,
      "grad_norm": 0.9138075709342957,
      "learning_rate": 1.1516164994425863e-05,
      "loss": 0.8279,
      "step": 17260
    },
    {
      "epoch": 2.3101361067451425,
      "grad_norm": 0.6995937824249268,
      "learning_rate": 1.1493868450390189e-05,
      "loss": 0.848,
      "step": 17270
    },
    {
      "epoch": 2.3114737651740627,
      "grad_norm": 0.7070840001106262,
      "learning_rate": 1.1471571906354516e-05,
      "loss": 0.8289,
      "step": 17280
    },
    {
      "epoch": 2.312811423602983,
      "grad_norm": 0.6378173828125,
      "learning_rate": 1.1449275362318842e-05,
      "loss": 0.8482,
      "step": 17290
    },
    {
      "epoch": 2.314149082031903,
      "grad_norm": 0.6996623873710632,
      "learning_rate": 1.1426978818283167e-05,
      "loss": 0.8556,
      "step": 17300
    },
    {
      "epoch": 2.3154867404608233,
      "grad_norm": 0.7053138017654419,
      "learning_rate": 1.1404682274247493e-05,
      "loss": 0.8512,
      "step": 17310
    },
    {
      "epoch": 2.3168243988897435,
      "grad_norm": 0.6678026914596558,
      "learning_rate": 1.1382385730211819e-05,
      "loss": 0.8401,
      "step": 17320
    },
    {
      "epoch": 2.3181620573186636,
      "grad_norm": 0.707910418510437,
      "learning_rate": 1.1360089186176142e-05,
      "loss": 0.8443,
      "step": 17330
    },
    {
      "epoch": 2.319499715747584,
      "grad_norm": 0.6319613456726074,
      "learning_rate": 1.1337792642140468e-05,
      "loss": 0.8424,
      "step": 17340
    },
    {
      "epoch": 2.320837374176504,
      "grad_norm": 0.6694600582122803,
      "learning_rate": 1.1315496098104794e-05,
      "loss": 0.8294,
      "step": 17350
    },
    {
      "epoch": 2.3221750326054242,
      "grad_norm": 0.701509952545166,
      "learning_rate": 1.129319955406912e-05,
      "loss": 0.8263,
      "step": 17360
    },
    {
      "epoch": 2.3235126910343444,
      "grad_norm": 0.6963521838188171,
      "learning_rate": 1.1270903010033446e-05,
      "loss": 0.8195,
      "step": 17370
    },
    {
      "epoch": 2.3248503494632646,
      "grad_norm": 0.6210605502128601,
      "learning_rate": 1.1248606465997772e-05,
      "loss": 0.8332,
      "step": 17380
    },
    {
      "epoch": 2.326188007892185,
      "grad_norm": 0.6672598719596863,
      "learning_rate": 1.1226309921962098e-05,
      "loss": 0.8333,
      "step": 17390
    },
    {
      "epoch": 2.327525666321105,
      "grad_norm": 0.6601480841636658,
      "learning_rate": 1.1204013377926421e-05,
      "loss": 0.8365,
      "step": 17400
    },
    {
      "epoch": 2.328863324750025,
      "grad_norm": 0.7103092074394226,
      "learning_rate": 1.1181716833890747e-05,
      "loss": 0.8241,
      "step": 17410
    },
    {
      "epoch": 2.3302009831789454,
      "grad_norm": 0.6664367914199829,
      "learning_rate": 1.1159420289855073e-05,
      "loss": 0.836,
      "step": 17420
    },
    {
      "epoch": 2.3315386416078656,
      "grad_norm": 0.7194159626960754,
      "learning_rate": 1.1137123745819398e-05,
      "loss": 0.8442,
      "step": 17430
    },
    {
      "epoch": 2.3328763000367854,
      "grad_norm": 0.6967479586601257,
      "learning_rate": 1.1114827201783724e-05,
      "loss": 0.8641,
      "step": 17440
    },
    {
      "epoch": 2.334213958465706,
      "grad_norm": 1.939645528793335,
      "learning_rate": 1.109253065774805e-05,
      "loss": 0.8507,
      "step": 17450
    },
    {
      "epoch": 2.3355516168946258,
      "grad_norm": 0.7188189625740051,
      "learning_rate": 1.1070234113712377e-05,
      "loss": 0.8463,
      "step": 17460
    },
    {
      "epoch": 2.336889275323546,
      "grad_norm": 0.620707631111145,
      "learning_rate": 1.10479375696767e-05,
      "loss": 0.8378,
      "step": 17470
    },
    {
      "epoch": 2.338226933752466,
      "grad_norm": 0.6589311361312866,
      "learning_rate": 1.1025641025641026e-05,
      "loss": 0.8372,
      "step": 17480
    },
    {
      "epoch": 2.3395645921813863,
      "grad_norm": 0.6985431909561157,
      "learning_rate": 1.1003344481605352e-05,
      "loss": 0.849,
      "step": 17490
    },
    {
      "epoch": 2.3409022506103065,
      "grad_norm": 0.6857808828353882,
      "learning_rate": 1.0981047937569677e-05,
      "loss": 0.8455,
      "step": 17500
    },
    {
      "epoch": 2.3422399090392267,
      "grad_norm": 0.6749571561813354,
      "learning_rate": 1.0958751393534003e-05,
      "loss": 0.8411,
      "step": 17510
    },
    {
      "epoch": 2.343577567468147,
      "grad_norm": 0.6981449127197266,
      "learning_rate": 1.0936454849498328e-05,
      "loss": 0.8408,
      "step": 17520
    },
    {
      "epoch": 2.344915225897067,
      "grad_norm": 0.6859517693519592,
      "learning_rate": 1.0914158305462654e-05,
      "loss": 0.8291,
      "step": 17530
    },
    {
      "epoch": 2.3462528843259873,
      "grad_norm": 0.6851731538772583,
      "learning_rate": 1.089186176142698e-05,
      "loss": 0.8464,
      "step": 17540
    },
    {
      "epoch": 2.3475905427549075,
      "grad_norm": 0.6926800608634949,
      "learning_rate": 1.0869565217391305e-05,
      "loss": 0.8339,
      "step": 17550
    },
    {
      "epoch": 2.3489282011838277,
      "grad_norm": 0.750494658946991,
      "learning_rate": 1.084726867335563e-05,
      "loss": 0.8642,
      "step": 17560
    },
    {
      "epoch": 2.350265859612748,
      "grad_norm": 0.6859624981880188,
      "learning_rate": 1.0824972129319956e-05,
      "loss": 0.864,
      "step": 17570
    },
    {
      "epoch": 2.351603518041668,
      "grad_norm": 0.6363610029220581,
      "learning_rate": 1.0802675585284282e-05,
      "loss": 0.846,
      "step": 17580
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 0.6670017242431641,
      "learning_rate": 1.0780379041248607e-05,
      "loss": 0.8169,
      "step": 17590
    },
    {
      "epoch": 2.3542788348995085,
      "grad_norm": 0.659302830696106,
      "learning_rate": 1.0758082497212931e-05,
      "loss": 0.8547,
      "step": 17600
    },
    {
      "epoch": 2.3556164933284287,
      "grad_norm": 0.6167821884155273,
      "learning_rate": 1.0735785953177257e-05,
      "loss": 0.8356,
      "step": 17610
    },
    {
      "epoch": 2.356954151757349,
      "grad_norm": 0.6610995531082153,
      "learning_rate": 1.0713489409141584e-05,
      "loss": 0.8488,
      "step": 17620
    },
    {
      "epoch": 2.358291810186269,
      "grad_norm": 0.6773895621299744,
      "learning_rate": 1.069119286510591e-05,
      "loss": 0.8504,
      "step": 17630
    },
    {
      "epoch": 2.3596294686151893,
      "grad_norm": 0.7186696529388428,
      "learning_rate": 1.0668896321070235e-05,
      "loss": 0.839,
      "step": 17640
    },
    {
      "epoch": 2.360967127044109,
      "grad_norm": 0.7367663383483887,
      "learning_rate": 1.064659977703456e-05,
      "loss": 0.8285,
      "step": 17650
    },
    {
      "epoch": 2.3623047854730297,
      "grad_norm": 0.7704758048057556,
      "learning_rate": 1.0624303232998886e-05,
      "loss": 0.8316,
      "step": 17660
    },
    {
      "epoch": 2.3636424439019494,
      "grad_norm": 0.67653489112854,
      "learning_rate": 1.060200668896321e-05,
      "loss": 0.8446,
      "step": 17670
    },
    {
      "epoch": 2.3649801023308696,
      "grad_norm": 0.7410112023353577,
      "learning_rate": 1.0579710144927536e-05,
      "loss": 0.8181,
      "step": 17680
    },
    {
      "epoch": 2.36631776075979,
      "grad_norm": 0.7154935598373413,
      "learning_rate": 1.0557413600891861e-05,
      "loss": 0.8373,
      "step": 17690
    },
    {
      "epoch": 2.36765541918871,
      "grad_norm": 0.678286612033844,
      "learning_rate": 1.0535117056856187e-05,
      "loss": 0.8395,
      "step": 17700
    },
    {
      "epoch": 2.3689930776176302,
      "grad_norm": 0.6519719958305359,
      "learning_rate": 1.0512820512820514e-05,
      "loss": 0.8395,
      "step": 17710
    },
    {
      "epoch": 2.3703307360465504,
      "grad_norm": 0.6573905944824219,
      "learning_rate": 1.049052396878484e-05,
      "loss": 0.8308,
      "step": 17720
    },
    {
      "epoch": 2.3716683944754706,
      "grad_norm": 0.6988141536712646,
      "learning_rate": 1.0468227424749165e-05,
      "loss": 0.8141,
      "step": 17730
    },
    {
      "epoch": 2.373006052904391,
      "grad_norm": 0.7063793540000916,
      "learning_rate": 1.044593088071349e-05,
      "loss": 0.8276,
      "step": 17740
    },
    {
      "epoch": 2.374343711333311,
      "grad_norm": 0.6802451014518738,
      "learning_rate": 1.0423634336677815e-05,
      "loss": 0.8628,
      "step": 17750
    },
    {
      "epoch": 2.375681369762231,
      "grad_norm": 0.6776620149612427,
      "learning_rate": 1.040133779264214e-05,
      "loss": 0.8285,
      "step": 17760
    },
    {
      "epoch": 2.3770190281911514,
      "grad_norm": 0.6474900841712952,
      "learning_rate": 1.0379041248606466e-05,
      "loss": 0.8174,
      "step": 17770
    },
    {
      "epoch": 2.3783566866200716,
      "grad_norm": 0.6475964784622192,
      "learning_rate": 1.0356744704570792e-05,
      "loss": 0.8217,
      "step": 17780
    },
    {
      "epoch": 2.379694345048992,
      "grad_norm": 0.6485322117805481,
      "learning_rate": 1.0334448160535117e-05,
      "loss": 0.8364,
      "step": 17790
    },
    {
      "epoch": 2.381032003477912,
      "grad_norm": 0.6925600171089172,
      "learning_rate": 1.0312151616499443e-05,
      "loss": 0.8261,
      "step": 17800
    },
    {
      "epoch": 2.382369661906832,
      "grad_norm": 0.7200392484664917,
      "learning_rate": 1.0289855072463768e-05,
      "loss": 0.8677,
      "step": 17810
    },
    {
      "epoch": 2.3837073203357524,
      "grad_norm": 0.7256235480308533,
      "learning_rate": 1.0267558528428094e-05,
      "loss": 0.8353,
      "step": 17820
    },
    {
      "epoch": 2.3850449787646726,
      "grad_norm": 0.7074437141418457,
      "learning_rate": 1.024526198439242e-05,
      "loss": 0.8192,
      "step": 17830
    },
    {
      "epoch": 2.386382637193593,
      "grad_norm": 0.7849981188774109,
      "learning_rate": 1.0222965440356745e-05,
      "loss": 0.8574,
      "step": 17840
    },
    {
      "epoch": 2.387720295622513,
      "grad_norm": 0.6638988256454468,
      "learning_rate": 1.020066889632107e-05,
      "loss": 0.8449,
      "step": 17850
    },
    {
      "epoch": 2.3890579540514327,
      "grad_norm": 0.6716136336326599,
      "learning_rate": 1.0178372352285396e-05,
      "loss": 0.8425,
      "step": 17860
    },
    {
      "epoch": 2.3903956124803534,
      "grad_norm": 0.6383289098739624,
      "learning_rate": 1.0156075808249722e-05,
      "loss": 0.8303,
      "step": 17870
    },
    {
      "epoch": 2.391733270909273,
      "grad_norm": 0.6678834557533264,
      "learning_rate": 1.0133779264214047e-05,
      "loss": 0.8251,
      "step": 17880
    },
    {
      "epoch": 2.3930709293381933,
      "grad_norm": 0.7185128927230835,
      "learning_rate": 1.0111482720178373e-05,
      "loss": 0.8486,
      "step": 17890
    },
    {
      "epoch": 2.3944085877671135,
      "grad_norm": 0.7664228081703186,
      "learning_rate": 1.0089186176142699e-05,
      "loss": 0.8282,
      "step": 17900
    },
    {
      "epoch": 2.3957462461960337,
      "grad_norm": 0.6313486695289612,
      "learning_rate": 1.0066889632107024e-05,
      "loss": 0.8266,
      "step": 17910
    },
    {
      "epoch": 2.397083904624954,
      "grad_norm": 0.6800886988639832,
      "learning_rate": 1.004459308807135e-05,
      "loss": 0.8435,
      "step": 17920
    },
    {
      "epoch": 2.398421563053874,
      "grad_norm": 0.6425897479057312,
      "learning_rate": 1.0022296544035675e-05,
      "loss": 0.8323,
      "step": 17930
    },
    {
      "epoch": 2.3997592214827943,
      "grad_norm": 0.7183921337127686,
      "learning_rate": 1e-05,
      "loss": 0.8225,
      "step": 17940
    },
    {
      "epoch": 2.4010968799117145,
      "grad_norm": 0.7155557870864868,
      "learning_rate": 9.977703455964325e-06,
      "loss": 0.8556,
      "step": 17950
    },
    {
      "epoch": 2.4024345383406347,
      "grad_norm": 0.7288824915885925,
      "learning_rate": 9.95540691192865e-06,
      "loss": 0.8525,
      "step": 17960
    },
    {
      "epoch": 2.403772196769555,
      "grad_norm": 0.710543155670166,
      "learning_rate": 9.933110367892978e-06,
      "loss": 0.846,
      "step": 17970
    },
    {
      "epoch": 2.405109855198475,
      "grad_norm": 0.6835444569587708,
      "learning_rate": 9.910813823857303e-06,
      "loss": 0.8549,
      "step": 17980
    },
    {
      "epoch": 2.4064475136273953,
      "grad_norm": 0.6824221611022949,
      "learning_rate": 9.888517279821629e-06,
      "loss": 0.8507,
      "step": 17990
    },
    {
      "epoch": 2.4077851720563155,
      "grad_norm": 0.7058979272842407,
      "learning_rate": 9.866220735785954e-06,
      "loss": 0.8497,
      "step": 18000
    },
    {
      "epoch": 2.4091228304852357,
      "grad_norm": 0.7529179453849792,
      "learning_rate": 9.84392419175028e-06,
      "loss": 0.8695,
      "step": 18010
    },
    {
      "epoch": 2.410460488914156,
      "grad_norm": 0.768878698348999,
      "learning_rate": 9.821627647714604e-06,
      "loss": 0.8397,
      "step": 18020
    },
    {
      "epoch": 2.411798147343076,
      "grad_norm": 0.7146627306938171,
      "learning_rate": 9.79933110367893e-06,
      "loss": 0.8299,
      "step": 18030
    },
    {
      "epoch": 2.4131358057719963,
      "grad_norm": 0.7187120914459229,
      "learning_rate": 9.777034559643255e-06,
      "loss": 0.8187,
      "step": 18040
    },
    {
      "epoch": 2.4144734642009165,
      "grad_norm": 0.6918665766716003,
      "learning_rate": 9.75473801560758e-06,
      "loss": 0.8209,
      "step": 18050
    },
    {
      "epoch": 2.4158111226298367,
      "grad_norm": 0.7085474729537964,
      "learning_rate": 9.732441471571908e-06,
      "loss": 0.8439,
      "step": 18060
    },
    {
      "epoch": 2.4171487810587564,
      "grad_norm": 0.7090738415718079,
      "learning_rate": 9.710144927536233e-06,
      "loss": 0.8427,
      "step": 18070
    },
    {
      "epoch": 2.418486439487677,
      "grad_norm": 0.7078962326049805,
      "learning_rate": 9.687848383500559e-06,
      "loss": 0.8562,
      "step": 18080
    },
    {
      "epoch": 2.419824097916597,
      "grad_norm": 0.7644951343536377,
      "learning_rate": 9.665551839464883e-06,
      "loss": 0.8411,
      "step": 18090
    },
    {
      "epoch": 2.421161756345517,
      "grad_norm": 0.7069937586784363,
      "learning_rate": 9.643255295429208e-06,
      "loss": 0.8521,
      "step": 18100
    },
    {
      "epoch": 2.422499414774437,
      "grad_norm": 0.6375795602798462,
      "learning_rate": 9.620958751393534e-06,
      "loss": 0.8594,
      "step": 18110
    },
    {
      "epoch": 2.4238370732033574,
      "grad_norm": 0.7634680867195129,
      "learning_rate": 9.59866220735786e-06,
      "loss": 0.8401,
      "step": 18120
    },
    {
      "epoch": 2.4251747316322776,
      "grad_norm": 0.7246211171150208,
      "learning_rate": 9.576365663322185e-06,
      "loss": 0.8244,
      "step": 18130
    },
    {
      "epoch": 2.426512390061198,
      "grad_norm": 0.6697351336479187,
      "learning_rate": 9.55406911928651e-06,
      "loss": 0.8071,
      "step": 18140
    },
    {
      "epoch": 2.427850048490118,
      "grad_norm": 0.630846381187439,
      "learning_rate": 9.531772575250838e-06,
      "loss": 0.8341,
      "step": 18150
    },
    {
      "epoch": 2.429187706919038,
      "grad_norm": 0.6542903184890747,
      "learning_rate": 9.509476031215162e-06,
      "loss": 0.8008,
      "step": 18160
    },
    {
      "epoch": 2.4305253653479584,
      "grad_norm": 0.6961824893951416,
      "learning_rate": 9.487179487179487e-06,
      "loss": 0.8358,
      "step": 18170
    },
    {
      "epoch": 2.4318630237768786,
      "grad_norm": 0.669751763343811,
      "learning_rate": 9.464882943143813e-06,
      "loss": 0.8354,
      "step": 18180
    },
    {
      "epoch": 2.4332006822057988,
      "grad_norm": 0.6510717868804932,
      "learning_rate": 9.442586399108138e-06,
      "loss": 0.8251,
      "step": 18190
    },
    {
      "epoch": 2.434538340634719,
      "grad_norm": 0.7775507569313049,
      "learning_rate": 9.420289855072464e-06,
      "loss": 0.8353,
      "step": 18200
    },
    {
      "epoch": 2.435875999063639,
      "grad_norm": 0.7141050100326538,
      "learning_rate": 9.39799331103679e-06,
      "loss": 0.8176,
      "step": 18210
    },
    {
      "epoch": 2.4372136574925594,
      "grad_norm": 0.7160826921463013,
      "learning_rate": 9.375696767001115e-06,
      "loss": 0.8617,
      "step": 18220
    },
    {
      "epoch": 2.4385513159214796,
      "grad_norm": 0.6944016814231873,
      "learning_rate": 9.35340022296544e-06,
      "loss": 0.8363,
      "step": 18230
    },
    {
      "epoch": 2.4398889743503998,
      "grad_norm": 0.6898228526115417,
      "learning_rate": 9.331103678929766e-06,
      "loss": 0.8349,
      "step": 18240
    },
    {
      "epoch": 2.44122663277932,
      "grad_norm": 0.6586337685585022,
      "learning_rate": 9.308807134894092e-06,
      "loss": 0.8245,
      "step": 18250
    },
    {
      "epoch": 2.44256429120824,
      "grad_norm": 0.717072069644928,
      "learning_rate": 9.286510590858418e-06,
      "loss": 0.832,
      "step": 18260
    },
    {
      "epoch": 2.4439019496371603,
      "grad_norm": 0.7014639377593994,
      "learning_rate": 9.264214046822743e-06,
      "loss": 0.8173,
      "step": 18270
    },
    {
      "epoch": 2.44523960806608,
      "grad_norm": 0.6761096715927124,
      "learning_rate": 9.241917502787069e-06,
      "loss": 0.8261,
      "step": 18280
    },
    {
      "epoch": 2.4465772664950007,
      "grad_norm": 0.684104859828949,
      "learning_rate": 9.219620958751394e-06,
      "loss": 0.8113,
      "step": 18290
    },
    {
      "epoch": 2.4479149249239205,
      "grad_norm": 0.6628851890563965,
      "learning_rate": 9.197324414715718e-06,
      "loss": 0.8326,
      "step": 18300
    },
    {
      "epoch": 2.4492525833528407,
      "grad_norm": 0.6794319152832031,
      "learning_rate": 9.175027870680045e-06,
      "loss": 0.8291,
      "step": 18310
    },
    {
      "epoch": 2.450590241781761,
      "grad_norm": 0.7059429883956909,
      "learning_rate": 9.152731326644371e-06,
      "loss": 0.8426,
      "step": 18320
    },
    {
      "epoch": 2.451927900210681,
      "grad_norm": 0.6648179292678833,
      "learning_rate": 9.130434782608697e-06,
      "loss": 0.8222,
      "step": 18330
    },
    {
      "epoch": 2.4532655586396013,
      "grad_norm": 0.6427719593048096,
      "learning_rate": 9.108138238573022e-06,
      "loss": 0.8303,
      "step": 18340
    },
    {
      "epoch": 2.4546032170685215,
      "grad_norm": 0.6703912019729614,
      "learning_rate": 9.085841694537348e-06,
      "loss": 0.8173,
      "step": 18350
    },
    {
      "epoch": 2.4559408754974417,
      "grad_norm": 0.7382047772407532,
      "learning_rate": 9.063545150501673e-06,
      "loss": 0.8182,
      "step": 18360
    },
    {
      "epoch": 2.457278533926362,
      "grad_norm": 0.6514355540275574,
      "learning_rate": 9.041248606465997e-06,
      "loss": 0.8523,
      "step": 18370
    },
    {
      "epoch": 2.458616192355282,
      "grad_norm": 0.7240651249885559,
      "learning_rate": 9.018952062430323e-06,
      "loss": 0.8278,
      "step": 18380
    },
    {
      "epoch": 2.4599538507842023,
      "grad_norm": 0.6822778582572937,
      "learning_rate": 8.996655518394648e-06,
      "loss": 0.8505,
      "step": 18390
    },
    {
      "epoch": 2.4612915092131225,
      "grad_norm": 0.6713423728942871,
      "learning_rate": 8.974358974358976e-06,
      "loss": 0.8268,
      "step": 18400
    },
    {
      "epoch": 2.4626291676420426,
      "grad_norm": 0.6636071801185608,
      "learning_rate": 8.952062430323301e-06,
      "loss": 0.8362,
      "step": 18410
    },
    {
      "epoch": 2.463966826070963,
      "grad_norm": 0.7562931776046753,
      "learning_rate": 8.929765886287627e-06,
      "loss": 0.8204,
      "step": 18420
    },
    {
      "epoch": 2.465304484499883,
      "grad_norm": 0.694770097732544,
      "learning_rate": 8.907469342251952e-06,
      "loss": 0.8192,
      "step": 18430
    },
    {
      "epoch": 2.4666421429288032,
      "grad_norm": 0.7150300145149231,
      "learning_rate": 8.885172798216276e-06,
      "loss": 0.8424,
      "step": 18440
    },
    {
      "epoch": 2.4679798013577234,
      "grad_norm": 0.763961136341095,
      "learning_rate": 8.862876254180602e-06,
      "loss": 0.8402,
      "step": 18450
    },
    {
      "epoch": 2.4693174597866436,
      "grad_norm": 0.652001678943634,
      "learning_rate": 8.840579710144927e-06,
      "loss": 0.8393,
      "step": 18460
    },
    {
      "epoch": 2.470655118215564,
      "grad_norm": 0.6535506844520569,
      "learning_rate": 8.818283166109253e-06,
      "loss": 0.8263,
      "step": 18470
    },
    {
      "epoch": 2.471992776644484,
      "grad_norm": 0.7055779099464417,
      "learning_rate": 8.795986622073578e-06,
      "loss": 0.8547,
      "step": 18480
    },
    {
      "epoch": 2.4733304350734038,
      "grad_norm": 0.6516575217247009,
      "learning_rate": 8.773690078037906e-06,
      "loss": 0.831,
      "step": 18490
    },
    {
      "epoch": 2.4746680935023244,
      "grad_norm": 0.7102975845336914,
      "learning_rate": 8.751393534002231e-06,
      "loss": 0.8514,
      "step": 18500
    },
    {
      "epoch": 2.476005751931244,
      "grad_norm": 0.753553569316864,
      "learning_rate": 8.729096989966555e-06,
      "loss": 0.846,
      "step": 18510
    },
    {
      "epoch": 2.4773434103601644,
      "grad_norm": 0.6993732452392578,
      "learning_rate": 8.70680044593088e-06,
      "loss": 0.8526,
      "step": 18520
    },
    {
      "epoch": 2.4786810687890846,
      "grad_norm": 0.690071702003479,
      "learning_rate": 8.684503901895206e-06,
      "loss": 0.8393,
      "step": 18530
    },
    {
      "epoch": 2.4800187272180048,
      "grad_norm": 0.6769042015075684,
      "learning_rate": 8.662207357859532e-06,
      "loss": 0.8366,
      "step": 18540
    },
    {
      "epoch": 2.481356385646925,
      "grad_norm": 0.72654789686203,
      "learning_rate": 8.639910813823858e-06,
      "loss": 0.8527,
      "step": 18550
    },
    {
      "epoch": 2.482694044075845,
      "grad_norm": 0.6643568277359009,
      "learning_rate": 8.617614269788183e-06,
      "loss": 0.8211,
      "step": 18560
    },
    {
      "epoch": 2.4840317025047653,
      "grad_norm": 0.6663455963134766,
      "learning_rate": 8.595317725752509e-06,
      "loss": 0.8275,
      "step": 18570
    },
    {
      "epoch": 2.4853693609336855,
      "grad_norm": 0.7691707611083984,
      "learning_rate": 8.573021181716834e-06,
      "loss": 0.8164,
      "step": 18580
    },
    {
      "epoch": 2.4867070193626057,
      "grad_norm": 0.6446710824966431,
      "learning_rate": 8.55072463768116e-06,
      "loss": 0.824,
      "step": 18590
    },
    {
      "epoch": 2.488044677791526,
      "grad_norm": 0.6752080321311951,
      "learning_rate": 8.528428093645485e-06,
      "loss": 0.8513,
      "step": 18600
    },
    {
      "epoch": 2.489382336220446,
      "grad_norm": 0.6792715191841125,
      "learning_rate": 8.506131549609811e-06,
      "loss": 0.8153,
      "step": 18610
    },
    {
      "epoch": 2.4907199946493663,
      "grad_norm": 0.6484470367431641,
      "learning_rate": 8.483835005574137e-06,
      "loss": 0.8574,
      "step": 18620
    },
    {
      "epoch": 2.4920576530782865,
      "grad_norm": 0.7163325548171997,
      "learning_rate": 8.461538461538462e-06,
      "loss": 0.8556,
      "step": 18630
    },
    {
      "epoch": 2.4933953115072067,
      "grad_norm": 0.7434183359146118,
      "learning_rate": 8.439241917502788e-06,
      "loss": 0.8375,
      "step": 18640
    },
    {
      "epoch": 2.494732969936127,
      "grad_norm": 0.7059828639030457,
      "learning_rate": 8.416945373467113e-06,
      "loss": 0.8503,
      "step": 18650
    },
    {
      "epoch": 2.496070628365047,
      "grad_norm": 0.7314286231994629,
      "learning_rate": 8.394648829431439e-06,
      "loss": 0.8337,
      "step": 18660
    },
    {
      "epoch": 2.4974082867939673,
      "grad_norm": 0.7547459006309509,
      "learning_rate": 8.372352285395764e-06,
      "loss": 0.8521,
      "step": 18670
    },
    {
      "epoch": 2.4987459452228875,
      "grad_norm": 0.7259619832038879,
      "learning_rate": 8.35005574136009e-06,
      "loss": 0.8348,
      "step": 18680
    },
    {
      "epoch": 2.5000836036518077,
      "grad_norm": 0.6560083627700806,
      "learning_rate": 8.327759197324416e-06,
      "loss": 0.8467,
      "step": 18690
    },
    {
      "epoch": 2.5014212620807275,
      "grad_norm": 0.728428065776825,
      "learning_rate": 8.305462653288741e-06,
      "loss": 0.8189,
      "step": 18700
    },
    {
      "epoch": 2.502758920509648,
      "grad_norm": 0.6458755731582642,
      "learning_rate": 8.283166109253067e-06,
      "loss": 0.8474,
      "step": 18710
    },
    {
      "epoch": 2.504096578938568,
      "grad_norm": 0.6845091581344604,
      "learning_rate": 8.26086956521739e-06,
      "loss": 0.8392,
      "step": 18720
    },
    {
      "epoch": 2.505434237367488,
      "grad_norm": 0.6209872364997864,
      "learning_rate": 8.238573021181716e-06,
      "loss": 0.8295,
      "step": 18730
    },
    {
      "epoch": 2.5067718957964082,
      "grad_norm": 0.6548672318458557,
      "learning_rate": 8.216276477146043e-06,
      "loss": 0.8512,
      "step": 18740
    },
    {
      "epoch": 2.5081095542253284,
      "grad_norm": 0.6785295009613037,
      "learning_rate": 8.193979933110369e-06,
      "loss": 0.8351,
      "step": 18750
    },
    {
      "epoch": 2.5094472126542486,
      "grad_norm": 0.6975996494293213,
      "learning_rate": 8.171683389074695e-06,
      "loss": 0.8559,
      "step": 18760
    },
    {
      "epoch": 2.510784871083169,
      "grad_norm": 0.7639795541763306,
      "learning_rate": 8.14938684503902e-06,
      "loss": 0.8481,
      "step": 18770
    },
    {
      "epoch": 2.512122529512089,
      "grad_norm": 0.660563588142395,
      "learning_rate": 8.127090301003346e-06,
      "loss": 0.8322,
      "step": 18780
    },
    {
      "epoch": 2.5134601879410092,
      "grad_norm": 0.717924177646637,
      "learning_rate": 8.10479375696767e-06,
      "loss": 0.8226,
      "step": 18790
    },
    {
      "epoch": 2.5147978463699294,
      "grad_norm": 0.6513605713844299,
      "learning_rate": 8.082497212931995e-06,
      "loss": 0.8425,
      "step": 18800
    },
    {
      "epoch": 2.5161355047988496,
      "grad_norm": 0.708473265171051,
      "learning_rate": 8.06020066889632e-06,
      "loss": 0.822,
      "step": 18810
    },
    {
      "epoch": 2.51747316322777,
      "grad_norm": 0.7306755781173706,
      "learning_rate": 8.037904124860646e-06,
      "loss": 0.8423,
      "step": 18820
    },
    {
      "epoch": 2.51881082165669,
      "grad_norm": 0.7331075668334961,
      "learning_rate": 8.015607580824974e-06,
      "loss": 0.8461,
      "step": 18830
    },
    {
      "epoch": 2.52014848008561,
      "grad_norm": 0.7534354329109192,
      "learning_rate": 7.9933110367893e-06,
      "loss": 0.8378,
      "step": 18840
    },
    {
      "epoch": 2.5214861385145304,
      "grad_norm": 0.7012002468109131,
      "learning_rate": 7.971014492753623e-06,
      "loss": 0.8568,
      "step": 18850
    },
    {
      "epoch": 2.5228237969434506,
      "grad_norm": 0.7214857935905457,
      "learning_rate": 7.948717948717949e-06,
      "loss": 0.8288,
      "step": 18860
    },
    {
      "epoch": 2.524161455372371,
      "grad_norm": 0.6668508052825928,
      "learning_rate": 7.926421404682274e-06,
      "loss": 0.8397,
      "step": 18870
    },
    {
      "epoch": 2.525499113801291,
      "grad_norm": 0.6849004030227661,
      "learning_rate": 7.9041248606466e-06,
      "loss": 0.8515,
      "step": 18880
    },
    {
      "epoch": 2.5268367722302107,
      "grad_norm": 0.7288568615913391,
      "learning_rate": 7.881828316610925e-06,
      "loss": 0.8387,
      "step": 18890
    },
    {
      "epoch": 2.5281744306591314,
      "grad_norm": 0.7023260593414307,
      "learning_rate": 7.859531772575251e-06,
      "loss": 0.8419,
      "step": 18900
    },
    {
      "epoch": 2.529512089088051,
      "grad_norm": 0.684605062007904,
      "learning_rate": 7.837235228539577e-06,
      "loss": 0.855,
      "step": 18910
    },
    {
      "epoch": 2.530849747516972,
      "grad_norm": 0.7534534335136414,
      "learning_rate": 7.814938684503902e-06,
      "loss": 0.8267,
      "step": 18920
    },
    {
      "epoch": 2.5321874059458915,
      "grad_norm": 0.7063460350036621,
      "learning_rate": 7.792642140468228e-06,
      "loss": 0.8535,
      "step": 18930
    },
    {
      "epoch": 2.5335250643748117,
      "grad_norm": 0.7059019207954407,
      "learning_rate": 7.770345596432553e-06,
      "loss": 0.8206,
      "step": 18940
    },
    {
      "epoch": 2.534862722803732,
      "grad_norm": 0.684299647808075,
      "learning_rate": 7.748049052396879e-06,
      "loss": 0.8602,
      "step": 18950
    },
    {
      "epoch": 2.536200381232652,
      "grad_norm": 0.7163268327713013,
      "learning_rate": 7.725752508361204e-06,
      "loss": 0.8543,
      "step": 18960
    },
    {
      "epoch": 2.5375380396615723,
      "grad_norm": 0.705132782459259,
      "learning_rate": 7.70345596432553e-06,
      "loss": 0.8201,
      "step": 18970
    },
    {
      "epoch": 2.5388756980904925,
      "grad_norm": 0.7252764105796814,
      "learning_rate": 7.681159420289856e-06,
      "loss": 0.8295,
      "step": 18980
    },
    {
      "epoch": 2.5402133565194127,
      "grad_norm": 0.7187454104423523,
      "learning_rate": 7.658862876254181e-06,
      "loss": 0.8603,
      "step": 18990
    },
    {
      "epoch": 2.541551014948333,
      "grad_norm": 0.6486397385597229,
      "learning_rate": 7.636566332218507e-06,
      "loss": 0.8018,
      "step": 19000
    },
    {
      "epoch": 2.542888673377253,
      "grad_norm": 0.735770046710968,
      "learning_rate": 7.614269788182831e-06,
      "loss": 0.852,
      "step": 19010
    },
    {
      "epoch": 2.5442263318061733,
      "grad_norm": 0.6808529496192932,
      "learning_rate": 7.591973244147158e-06,
      "loss": 0.8548,
      "step": 19020
    },
    {
      "epoch": 2.5455639902350935,
      "grad_norm": 0.7398472428321838,
      "learning_rate": 7.569676700111483e-06,
      "loss": 0.8291,
      "step": 19030
    },
    {
      "epoch": 2.5469016486640137,
      "grad_norm": 0.698678731918335,
      "learning_rate": 7.547380156075809e-06,
      "loss": 0.8463,
      "step": 19040
    },
    {
      "epoch": 2.548239307092934,
      "grad_norm": 0.7014366984367371,
      "learning_rate": 7.5250836120401346e-06,
      "loss": 0.8275,
      "step": 19050
    },
    {
      "epoch": 2.549576965521854,
      "grad_norm": 0.631370484828949,
      "learning_rate": 7.502787068004459e-06,
      "loss": 0.8134,
      "step": 19060
    },
    {
      "epoch": 2.5509146239507743,
      "grad_norm": 0.656470537185669,
      "learning_rate": 7.480490523968785e-06,
      "loss": 0.8534,
      "step": 19070
    },
    {
      "epoch": 2.5522522823796945,
      "grad_norm": 0.7055798172950745,
      "learning_rate": 7.4581939799331104e-06,
      "loss": 0.8652,
      "step": 19080
    },
    {
      "epoch": 2.5535899408086147,
      "grad_norm": 0.6789248585700989,
      "learning_rate": 7.435897435897436e-06,
      "loss": 0.8372,
      "step": 19090
    },
    {
      "epoch": 2.5549275992375344,
      "grad_norm": 0.7330428957939148,
      "learning_rate": 7.413600891861762e-06,
      "loss": 0.8306,
      "step": 19100
    },
    {
      "epoch": 2.556265257666455,
      "grad_norm": 0.6541728377342224,
      "learning_rate": 7.391304347826088e-06,
      "loss": 0.8437,
      "step": 19110
    },
    {
      "epoch": 2.557602916095375,
      "grad_norm": 0.7068207263946533,
      "learning_rate": 7.369007803790414e-06,
      "loss": 0.8275,
      "step": 19120
    },
    {
      "epoch": 2.5589405745242955,
      "grad_norm": 0.6858194470405579,
      "learning_rate": 7.3467112597547375e-06,
      "loss": 0.8307,
      "step": 19130
    },
    {
      "epoch": 2.560278232953215,
      "grad_norm": 0.7742828726768494,
      "learning_rate": 7.324414715719064e-06,
      "loss": 0.8582,
      "step": 19140
    },
    {
      "epoch": 2.5616158913821354,
      "grad_norm": 0.610388994216919,
      "learning_rate": 7.3021181716833895e-06,
      "loss": 0.8176,
      "step": 19150
    },
    {
      "epoch": 2.5629535498110556,
      "grad_norm": 0.7209102511405945,
      "learning_rate": 7.279821627647715e-06,
      "loss": 0.8337,
      "step": 19160
    },
    {
      "epoch": 2.564291208239976,
      "grad_norm": 0.7109814286231995,
      "learning_rate": 7.257525083612041e-06,
      "loss": 0.8343,
      "step": 19170
    },
    {
      "epoch": 2.565628866668896,
      "grad_norm": 0.7109224796295166,
      "learning_rate": 7.235228539576366e-06,
      "loss": 0.8468,
      "step": 19180
    },
    {
      "epoch": 2.566966525097816,
      "grad_norm": 0.6794715523719788,
      "learning_rate": 7.212931995540692e-06,
      "loss": 0.8361,
      "step": 19190
    },
    {
      "epoch": 2.5683041835267364,
      "grad_norm": 0.7368477582931519,
      "learning_rate": 7.1906354515050165e-06,
      "loss": 0.8444,
      "step": 19200
    },
    {
      "epoch": 2.5696418419556566,
      "grad_norm": 0.6663265228271484,
      "learning_rate": 7.168338907469342e-06,
      "loss": 0.8557,
      "step": 19210
    },
    {
      "epoch": 2.570979500384577,
      "grad_norm": 0.7153304219245911,
      "learning_rate": 7.146042363433668e-06,
      "loss": 0.8439,
      "step": 19220
    },
    {
      "epoch": 2.572317158813497,
      "grad_norm": 0.6920097470283508,
      "learning_rate": 7.123745819397993e-06,
      "loss": 0.8163,
      "step": 19230
    },
    {
      "epoch": 2.573654817242417,
      "grad_norm": 0.7650750875473022,
      "learning_rate": 7.10144927536232e-06,
      "loss": 0.8587,
      "step": 19240
    },
    {
      "epoch": 2.5749924756713374,
      "grad_norm": 0.7056750059127808,
      "learning_rate": 7.079152731326645e-06,
      "loss": 0.8354,
      "step": 19250
    },
    {
      "epoch": 2.5763301341002576,
      "grad_norm": 0.684177041053772,
      "learning_rate": 7.056856187290971e-06,
      "loss": 0.8228,
      "step": 19260
    },
    {
      "epoch": 2.5776677925291778,
      "grad_norm": 0.6979127526283264,
      "learning_rate": 7.0345596432552955e-06,
      "loss": 0.8167,
      "step": 19270
    },
    {
      "epoch": 2.579005450958098,
      "grad_norm": 0.6994264125823975,
      "learning_rate": 7.012263099219621e-06,
      "loss": 0.8483,
      "step": 19280
    },
    {
      "epoch": 2.580343109387018,
      "grad_norm": 0.6701110005378723,
      "learning_rate": 6.989966555183947e-06,
      "loss": 0.8417,
      "step": 19290
    },
    {
      "epoch": 2.5816807678159384,
      "grad_norm": 0.638719379901886,
      "learning_rate": 6.967670011148272e-06,
      "loss": 0.8404,
      "step": 19300
    },
    {
      "epoch": 2.583018426244858,
      "grad_norm": 0.6173098087310791,
      "learning_rate": 6.945373467112598e-06,
      "loss": 0.8259,
      "step": 19310
    },
    {
      "epoch": 2.5843560846737788,
      "grad_norm": 0.6405051350593567,
      "learning_rate": 6.923076923076923e-06,
      "loss": 0.8244,
      "step": 19320
    },
    {
      "epoch": 2.5856937431026985,
      "grad_norm": 0.6570740938186646,
      "learning_rate": 6.90078037904125e-06,
      "loss": 0.8306,
      "step": 19330
    },
    {
      "epoch": 2.587031401531619,
      "grad_norm": 0.6880967020988464,
      "learning_rate": 6.878483835005574e-06,
      "loss": 0.8047,
      "step": 19340
    },
    {
      "epoch": 2.588369059960539,
      "grad_norm": 0.739468514919281,
      "learning_rate": 6.856187290969899e-06,
      "loss": 0.8707,
      "step": 19350
    },
    {
      "epoch": 2.589706718389459,
      "grad_norm": 0.702261209487915,
      "learning_rate": 6.833890746934226e-06,
      "loss": 0.852,
      "step": 19360
    },
    {
      "epoch": 2.5910443768183793,
      "grad_norm": 0.6683720946311951,
      "learning_rate": 6.811594202898551e-06,
      "loss": 0.8383,
      "step": 19370
    },
    {
      "epoch": 2.5923820352472995,
      "grad_norm": 0.7126482129096985,
      "learning_rate": 6.789297658862877e-06,
      "loss": 0.8472,
      "step": 19380
    },
    {
      "epoch": 2.5937196936762197,
      "grad_norm": 0.6832146644592285,
      "learning_rate": 6.767001114827202e-06,
      "loss": 0.8558,
      "step": 19390
    },
    {
      "epoch": 2.59505735210514,
      "grad_norm": 0.6597858667373657,
      "learning_rate": 6.744704570791528e-06,
      "loss": 0.8359,
      "step": 19400
    },
    {
      "epoch": 2.59639501053406,
      "grad_norm": 0.7404734492301941,
      "learning_rate": 6.722408026755853e-06,
      "loss": 0.8103,
      "step": 19410
    },
    {
      "epoch": 2.5977326689629803,
      "grad_norm": 0.6533681750297546,
      "learning_rate": 6.700111482720178e-06,
      "loss": 0.8462,
      "step": 19420
    },
    {
      "epoch": 2.5990703273919005,
      "grad_norm": 0.7469004988670349,
      "learning_rate": 6.677814938684504e-06,
      "loss": 0.8276,
      "step": 19430
    },
    {
      "epoch": 2.6004079858208207,
      "grad_norm": 0.7042726874351501,
      "learning_rate": 6.6555183946488294e-06,
      "loss": 0.8326,
      "step": 19440
    },
    {
      "epoch": 2.601745644249741,
      "grad_norm": 0.7335999011993408,
      "learning_rate": 6.633221850613156e-06,
      "loss": 0.8511,
      "step": 19450
    },
    {
      "epoch": 2.603083302678661,
      "grad_norm": 0.7108445167541504,
      "learning_rate": 6.6109253065774814e-06,
      "loss": 0.8233,
      "step": 19460
    },
    {
      "epoch": 2.6044209611075813,
      "grad_norm": 0.6721820831298828,
      "learning_rate": 6.588628762541807e-06,
      "loss": 0.8288,
      "step": 19470
    },
    {
      "epoch": 2.6057586195365015,
      "grad_norm": 0.6924363970756531,
      "learning_rate": 6.566332218506131e-06,
      "loss": 0.8463,
      "step": 19480
    },
    {
      "epoch": 2.6070962779654216,
      "grad_norm": 0.7363640069961548,
      "learning_rate": 6.544035674470457e-06,
      "loss": 0.8277,
      "step": 19490
    },
    {
      "epoch": 2.608433936394342,
      "grad_norm": 0.6299459934234619,
      "learning_rate": 6.521739130434783e-06,
      "loss": 0.8318,
      "step": 19500
    },
    {
      "epoch": 2.609771594823262,
      "grad_norm": 0.7074363231658936,
      "learning_rate": 6.4994425863991085e-06,
      "loss": 0.8338,
      "step": 19510
    },
    {
      "epoch": 2.611109253252182,
      "grad_norm": 0.6904130578041077,
      "learning_rate": 6.477146042363434e-06,
      "loss": 0.823,
      "step": 19520
    },
    {
      "epoch": 2.6124469116811024,
      "grad_norm": 0.721593976020813,
      "learning_rate": 6.45484949832776e-06,
      "loss": 0.838,
      "step": 19530
    },
    {
      "epoch": 2.613784570110022,
      "grad_norm": 0.6595208048820496,
      "learning_rate": 6.432552954292086e-06,
      "loss": 0.836,
      "step": 19540
    },
    {
      "epoch": 2.615122228538943,
      "grad_norm": 0.6899625658988953,
      "learning_rate": 6.41025641025641e-06,
      "loss": 0.8306,
      "step": 19550
    },
    {
      "epoch": 2.6164598869678626,
      "grad_norm": 0.6503555774688721,
      "learning_rate": 6.3879598662207355e-06,
      "loss": 0.8389,
      "step": 19560
    },
    {
      "epoch": 2.6177975453967828,
      "grad_norm": 0.716280996799469,
      "learning_rate": 6.365663322185061e-06,
      "loss": 0.8327,
      "step": 19570
    },
    {
      "epoch": 2.619135203825703,
      "grad_norm": 0.6883310675621033,
      "learning_rate": 6.3433667781493875e-06,
      "loss": 0.8221,
      "step": 19580
    },
    {
      "epoch": 2.620472862254623,
      "grad_norm": 0.6495444774627686,
      "learning_rate": 6.321070234113713e-06,
      "loss": 0.8324,
      "step": 19590
    },
    {
      "epoch": 2.6218105206835434,
      "grad_norm": 0.6811363101005554,
      "learning_rate": 6.298773690078039e-06,
      "loss": 0.8352,
      "step": 19600
    },
    {
      "epoch": 2.6231481791124636,
      "grad_norm": 0.6536325216293335,
      "learning_rate": 6.276477146042364e-06,
      "loss": 0.8479,
      "step": 19610
    },
    {
      "epoch": 2.6244858375413838,
      "grad_norm": 0.7104148268699646,
      "learning_rate": 6.254180602006689e-06,
      "loss": 0.8355,
      "step": 19620
    },
    {
      "epoch": 2.625823495970304,
      "grad_norm": 0.7150501012802124,
      "learning_rate": 6.231884057971015e-06,
      "loss": 0.8625,
      "step": 19630
    },
    {
      "epoch": 2.627161154399224,
      "grad_norm": 0.722192645072937,
      "learning_rate": 6.20958751393534e-06,
      "loss": 0.8273,
      "step": 19640
    },
    {
      "epoch": 2.6284988128281443,
      "grad_norm": 0.6764651536941528,
      "learning_rate": 6.187290969899666e-06,
      "loss": 0.8261,
      "step": 19650
    },
    {
      "epoch": 2.6298364712570645,
      "grad_norm": 0.6627718210220337,
      "learning_rate": 6.164994425863991e-06,
      "loss": 0.8281,
      "step": 19660
    },
    {
      "epoch": 2.6311741296859847,
      "grad_norm": 0.670842170715332,
      "learning_rate": 6.142697881828317e-06,
      "loss": 0.854,
      "step": 19670
    },
    {
      "epoch": 2.632511788114905,
      "grad_norm": 0.6992771029472351,
      "learning_rate": 6.120401337792642e-06,
      "loss": 0.8184,
      "step": 19680
    },
    {
      "epoch": 2.633849446543825,
      "grad_norm": 0.6735479831695557,
      "learning_rate": 6.098104793756968e-06,
      "loss": 0.8421,
      "step": 19690
    },
    {
      "epoch": 2.6351871049727453,
      "grad_norm": 0.7104118466377258,
      "learning_rate": 6.0758082497212935e-06,
      "loss": 0.8381,
      "step": 19700
    },
    {
      "epoch": 2.6365247634016655,
      "grad_norm": 0.655012309551239,
      "learning_rate": 6.053511705685619e-06,
      "loss": 0.832,
      "step": 19710
    },
    {
      "epoch": 2.6378624218305857,
      "grad_norm": 0.6680837273597717,
      "learning_rate": 6.031215161649945e-06,
      "loss": 0.8457,
      "step": 19720
    },
    {
      "epoch": 2.6392000802595055,
      "grad_norm": 0.7039945721626282,
      "learning_rate": 6.00891861761427e-06,
      "loss": 0.8316,
      "step": 19730
    },
    {
      "epoch": 2.640537738688426,
      "grad_norm": 0.6650377511978149,
      "learning_rate": 5.986622073578595e-06,
      "loss": 0.8175,
      "step": 19740
    },
    {
      "epoch": 2.641875397117346,
      "grad_norm": 0.7468934059143066,
      "learning_rate": 5.964325529542921e-06,
      "loss": 0.8297,
      "step": 19750
    },
    {
      "epoch": 2.6432130555462665,
      "grad_norm": 0.6547611951828003,
      "learning_rate": 5.942028985507247e-06,
      "loss": 0.8423,
      "step": 19760
    },
    {
      "epoch": 2.6445507139751863,
      "grad_norm": 0.6775779724121094,
      "learning_rate": 5.919732441471572e-06,
      "loss": 0.8384,
      "step": 19770
    },
    {
      "epoch": 2.6458883724041065,
      "grad_norm": 0.6941245794296265,
      "learning_rate": 5.897435897435897e-06,
      "loss": 0.8523,
      "step": 19780
    },
    {
      "epoch": 2.6472260308330267,
      "grad_norm": 0.7018985152244568,
      "learning_rate": 5.875139353400224e-06,
      "loss": 0.8175,
      "step": 19790
    },
    {
      "epoch": 2.648563689261947,
      "grad_norm": 0.7155231833457947,
      "learning_rate": 5.852842809364549e-06,
      "loss": 0.8149,
      "step": 19800
    },
    {
      "epoch": 2.649901347690867,
      "grad_norm": 0.7223419547080994,
      "learning_rate": 5.830546265328874e-06,
      "loss": 0.8623,
      "step": 19810
    },
    {
      "epoch": 2.6512390061197872,
      "grad_norm": 0.6397882103919983,
      "learning_rate": 5.8082497212932e-06,
      "loss": 0.8364,
      "step": 19820
    },
    {
      "epoch": 2.6525766645487074,
      "grad_norm": 0.6913180351257324,
      "learning_rate": 5.785953177257525e-06,
      "loss": 0.816,
      "step": 19830
    },
    {
      "epoch": 2.6539143229776276,
      "grad_norm": 0.6784783601760864,
      "learning_rate": 5.763656633221851e-06,
      "loss": 0.8332,
      "step": 19840
    },
    {
      "epoch": 2.655251981406548,
      "grad_norm": 0.6984792947769165,
      "learning_rate": 5.741360089186176e-06,
      "loss": 0.8625,
      "step": 19850
    },
    {
      "epoch": 2.656589639835468,
      "grad_norm": 0.6484155654907227,
      "learning_rate": 5.719063545150502e-06,
      "loss": 0.8126,
      "step": 19860
    },
    {
      "epoch": 2.6579272982643882,
      "grad_norm": 0.7142845988273621,
      "learning_rate": 5.6967670011148275e-06,
      "loss": 0.8563,
      "step": 19870
    },
    {
      "epoch": 2.6592649566933084,
      "grad_norm": 0.6602226495742798,
      "learning_rate": 5.674470457079153e-06,
      "loss": 0.8666,
      "step": 19880
    },
    {
      "epoch": 2.6606026151222286,
      "grad_norm": 0.6451632976531982,
      "learning_rate": 5.652173913043479e-06,
      "loss": 0.8177,
      "step": 19890
    },
    {
      "epoch": 2.661940273551149,
      "grad_norm": 0.6830511093139648,
      "learning_rate": 5.629877369007804e-06,
      "loss": 0.8274,
      "step": 19900
    },
    {
      "epoch": 2.663277931980069,
      "grad_norm": 0.7097250819206238,
      "learning_rate": 5.607580824972129e-06,
      "loss": 0.851,
      "step": 19910
    },
    {
      "epoch": 2.664615590408989,
      "grad_norm": 0.6795774698257446,
      "learning_rate": 5.585284280936455e-06,
      "loss": 0.856,
      "step": 19920
    },
    {
      "epoch": 2.6659532488379094,
      "grad_norm": 0.6688306927680969,
      "learning_rate": 5.562987736900781e-06,
      "loss": 0.8303,
      "step": 19930
    },
    {
      "epoch": 2.667290907266829,
      "grad_norm": 0.9091619253158569,
      "learning_rate": 5.5406911928651065e-06,
      "loss": 0.8369,
      "step": 19940
    },
    {
      "epoch": 2.66862856569575,
      "grad_norm": 0.6309824585914612,
      "learning_rate": 5.518394648829431e-06,
      "loss": 0.8383,
      "step": 19950
    },
    {
      "epoch": 2.6699662241246696,
      "grad_norm": 0.7209615111351013,
      "learning_rate": 5.496098104793758e-06,
      "loss": 0.8416,
      "step": 19960
    },
    {
      "epoch": 2.67130388255359,
      "grad_norm": 0.7381255030632019,
      "learning_rate": 5.473801560758083e-06,
      "loss": 0.8513,
      "step": 19970
    },
    {
      "epoch": 2.67264154098251,
      "grad_norm": 0.6987102031707764,
      "learning_rate": 5.451505016722408e-06,
      "loss": 0.8572,
      "step": 19980
    },
    {
      "epoch": 2.67397919941143,
      "grad_norm": 0.7204867601394653,
      "learning_rate": 5.4292084726867335e-06,
      "loss": 0.8182,
      "step": 19990
    },
    {
      "epoch": 2.6753168578403503,
      "grad_norm": 0.7059452533721924,
      "learning_rate": 5.406911928651059e-06,
      "loss": 0.8446,
      "step": 20000
    },
    {
      "epoch": 2.6766545162692705,
      "grad_norm": 0.769631028175354,
      "learning_rate": 5.3846153846153855e-06,
      "loss": 0.8517,
      "step": 20010
    },
    {
      "epoch": 2.6779921746981907,
      "grad_norm": 0.6893478631973267,
      "learning_rate": 5.36231884057971e-06,
      "loss": 0.8338,
      "step": 20020
    },
    {
      "epoch": 2.679329833127111,
      "grad_norm": 0.7507706880569458,
      "learning_rate": 5.340022296544036e-06,
      "loss": 0.8421,
      "step": 20030
    },
    {
      "epoch": 2.680667491556031,
      "grad_norm": 0.6981502771377563,
      "learning_rate": 5.317725752508361e-06,
      "loss": 0.8275,
      "step": 20040
    },
    {
      "epoch": 2.6820051499849513,
      "grad_norm": 0.6725732088088989,
      "learning_rate": 5.295429208472687e-06,
      "loss": 0.8459,
      "step": 20050
    },
    {
      "epoch": 2.6833428084138715,
      "grad_norm": 0.7439223527908325,
      "learning_rate": 5.2731326644370125e-06,
      "loss": 0.8354,
      "step": 20060
    },
    {
      "epoch": 2.6846804668427917,
      "grad_norm": 0.6594350337982178,
      "learning_rate": 5.250836120401338e-06,
      "loss": 0.8377,
      "step": 20070
    },
    {
      "epoch": 2.686018125271712,
      "grad_norm": 0.6784940361976624,
      "learning_rate": 5.228539576365664e-06,
      "loss": 0.8361,
      "step": 20080
    },
    {
      "epoch": 2.687355783700632,
      "grad_norm": 0.6508601307868958,
      "learning_rate": 5.206243032329989e-06,
      "loss": 0.816,
      "step": 20090
    },
    {
      "epoch": 2.6886934421295523,
      "grad_norm": 0.6716814637184143,
      "learning_rate": 5.183946488294315e-06,
      "loss": 0.8267,
      "step": 20100
    },
    {
      "epoch": 2.6900311005584725,
      "grad_norm": 0.675270140171051,
      "learning_rate": 5.16164994425864e-06,
      "loss": 0.8722,
      "step": 20110
    },
    {
      "epoch": 2.6913687589873927,
      "grad_norm": 0.7085006833076477,
      "learning_rate": 5.139353400222965e-06,
      "loss": 0.8403,
      "step": 20120
    },
    {
      "epoch": 2.692706417416313,
      "grad_norm": 0.6992631554603577,
      "learning_rate": 5.117056856187291e-06,
      "loss": 0.8462,
      "step": 20130
    },
    {
      "epoch": 2.694044075845233,
      "grad_norm": 0.7015963792800903,
      "learning_rate": 5.094760312151617e-06,
      "loss": 0.8379,
      "step": 20140
    },
    {
      "epoch": 2.695381734274153,
      "grad_norm": 0.6964941024780273,
      "learning_rate": 5.072463768115943e-06,
      "loss": 0.8386,
      "step": 20150
    },
    {
      "epoch": 2.6967193927030735,
      "grad_norm": 0.6469664573669434,
      "learning_rate": 5.0501672240802674e-06,
      "loss": 0.8504,
      "step": 20160
    },
    {
      "epoch": 2.6980570511319932,
      "grad_norm": 0.7090360522270203,
      "learning_rate": 5.027870680044593e-06,
      "loss": 0.8479,
      "step": 20170
    },
    {
      "epoch": 2.699394709560914,
      "grad_norm": 0.6742467880249023,
      "learning_rate": 5.0055741360089194e-06,
      "loss": 0.8259,
      "step": 20180
    },
    {
      "epoch": 2.7007323679898336,
      "grad_norm": 0.6691054105758667,
      "learning_rate": 4.983277591973244e-06,
      "loss": 0.8326,
      "step": 20190
    },
    {
      "epoch": 2.702070026418754,
      "grad_norm": 0.6784661412239075,
      "learning_rate": 4.96098104793757e-06,
      "loss": 0.8301,
      "step": 20200
    },
    {
      "epoch": 2.703407684847674,
      "grad_norm": 0.6708368062973022,
      "learning_rate": 4.938684503901895e-06,
      "loss": 0.8213,
      "step": 20210
    },
    {
      "epoch": 2.704745343276594,
      "grad_norm": 0.6716619729995728,
      "learning_rate": 4.916387959866221e-06,
      "loss": 0.8442,
      "step": 20220
    },
    {
      "epoch": 2.7060830017055144,
      "grad_norm": 0.7554976344108582,
      "learning_rate": 4.8940914158305465e-06,
      "loss": 0.8651,
      "step": 20230
    },
    {
      "epoch": 2.7074206601344346,
      "grad_norm": 0.7059569954872131,
      "learning_rate": 4.871794871794872e-06,
      "loss": 0.852,
      "step": 20240
    },
    {
      "epoch": 2.708758318563355,
      "grad_norm": 0.6723120808601379,
      "learning_rate": 4.849498327759198e-06,
      "loss": 0.836,
      "step": 20250
    },
    {
      "epoch": 2.710095976992275,
      "grad_norm": 0.6421247720718384,
      "learning_rate": 4.827201783723523e-06,
      "loss": 0.8439,
      "step": 20260
    },
    {
      "epoch": 2.711433635421195,
      "grad_norm": 0.6618375182151794,
      "learning_rate": 4.804905239687849e-06,
      "loss": 0.8505,
      "step": 20270
    },
    {
      "epoch": 2.7127712938501154,
      "grad_norm": 0.6526923775672913,
      "learning_rate": 4.782608695652174e-06,
      "loss": 0.8334,
      "step": 20280
    },
    {
      "epoch": 2.7141089522790356,
      "grad_norm": 0.6487887501716614,
      "learning_rate": 4.7603121516165e-06,
      "loss": 0.827,
      "step": 20290
    },
    {
      "epoch": 2.715446610707956,
      "grad_norm": 0.7019845843315125,
      "learning_rate": 4.738015607580825e-06,
      "loss": 0.8215,
      "step": 20300
    },
    {
      "epoch": 2.716784269136876,
      "grad_norm": 0.6620405912399292,
      "learning_rate": 4.715719063545151e-06,
      "loss": 0.8517,
      "step": 20310
    },
    {
      "epoch": 2.718121927565796,
      "grad_norm": 0.7597804665565491,
      "learning_rate": 4.693422519509477e-06,
      "loss": 0.8181,
      "step": 20320
    },
    {
      "epoch": 2.7194595859947164,
      "grad_norm": 0.6904538869857788,
      "learning_rate": 4.671125975473801e-06,
      "loss": 0.8356,
      "step": 20330
    },
    {
      "epoch": 2.7207972444236366,
      "grad_norm": 0.7122333645820618,
      "learning_rate": 4.648829431438127e-06,
      "loss": 0.8299,
      "step": 20340
    },
    {
      "epoch": 2.7221349028525568,
      "grad_norm": 0.6863464713096619,
      "learning_rate": 4.626532887402453e-06,
      "loss": 0.8294,
      "step": 20350
    },
    {
      "epoch": 2.7234725612814765,
      "grad_norm": 0.6917828917503357,
      "learning_rate": 4.604236343366778e-06,
      "loss": 0.834,
      "step": 20360
    },
    {
      "epoch": 2.724810219710397,
      "grad_norm": 0.68422532081604,
      "learning_rate": 4.581939799331104e-06,
      "loss": 0.8643,
      "step": 20370
    },
    {
      "epoch": 2.726147878139317,
      "grad_norm": 0.7169888019561768,
      "learning_rate": 4.559643255295429e-06,
      "loss": 0.8325,
      "step": 20380
    },
    {
      "epoch": 2.7274855365682376,
      "grad_norm": 0.6768797636032104,
      "learning_rate": 4.537346711259755e-06,
      "loss": 0.8464,
      "step": 20390
    },
    {
      "epoch": 2.7288231949971573,
      "grad_norm": 0.6722815036773682,
      "learning_rate": 4.51505016722408e-06,
      "loss": 0.8254,
      "step": 20400
    },
    {
      "epoch": 2.7301608534260775,
      "grad_norm": 0.6851032376289368,
      "learning_rate": 4.492753623188406e-06,
      "loss": 0.8314,
      "step": 20410
    },
    {
      "epoch": 2.7314985118549977,
      "grad_norm": 0.6798931360244751,
      "learning_rate": 4.4704570791527315e-06,
      "loss": 0.8455,
      "step": 20420
    },
    {
      "epoch": 2.732836170283918,
      "grad_norm": 0.6532048583030701,
      "learning_rate": 4.448160535117057e-06,
      "loss": 0.8135,
      "step": 20430
    },
    {
      "epoch": 2.734173828712838,
      "grad_norm": 0.6749452352523804,
      "learning_rate": 4.425863991081383e-06,
      "loss": 0.8116,
      "step": 20440
    },
    {
      "epoch": 2.7355114871417583,
      "grad_norm": 0.7026869058609009,
      "learning_rate": 4.403567447045708e-06,
      "loss": 0.859,
      "step": 20450
    },
    {
      "epoch": 2.7368491455706785,
      "grad_norm": 0.7093023657798767,
      "learning_rate": 4.381270903010034e-06,
      "loss": 0.8209,
      "step": 20460
    },
    {
      "epoch": 2.7381868039995987,
      "grad_norm": 0.6685881614685059,
      "learning_rate": 4.3589743589743586e-06,
      "loss": 0.837,
      "step": 20470
    },
    {
      "epoch": 2.739524462428519,
      "grad_norm": 0.6416194438934326,
      "learning_rate": 4.336677814938685e-06,
      "loss": 0.8482,
      "step": 20480
    },
    {
      "epoch": 2.740862120857439,
      "grad_norm": 0.6806294322013855,
      "learning_rate": 4.3143812709030106e-06,
      "loss": 0.8348,
      "step": 20490
    },
    {
      "epoch": 2.7421997792863593,
      "grad_norm": 0.7502905130386353,
      "learning_rate": 4.292084726867335e-06,
      "loss": 0.8193,
      "step": 20500
    },
    {
      "epoch": 2.7435374377152795,
      "grad_norm": 0.6682144403457642,
      "learning_rate": 4.269788182831661e-06,
      "loss": 0.8148,
      "step": 20510
    },
    {
      "epoch": 2.7448750961441997,
      "grad_norm": 0.6763497591018677,
      "learning_rate": 4.247491638795987e-06,
      "loss": 0.8276,
      "step": 20520
    },
    {
      "epoch": 2.74621275457312,
      "grad_norm": 0.6492921710014343,
      "learning_rate": 4.225195094760313e-06,
      "loss": 0.8078,
      "step": 20530
    },
    {
      "epoch": 2.74755041300204,
      "grad_norm": 0.6942647099494934,
      "learning_rate": 4.202898550724638e-06,
      "loss": 0.8538,
      "step": 20540
    },
    {
      "epoch": 2.7488880714309603,
      "grad_norm": 0.7074572443962097,
      "learning_rate": 4.180602006688963e-06,
      "loss": 0.8626,
      "step": 20550
    },
    {
      "epoch": 2.7502257298598805,
      "grad_norm": 0.7658349871635437,
      "learning_rate": 4.158305462653289e-06,
      "loss": 0.8373,
      "step": 20560
    },
    {
      "epoch": 2.7515633882888,
      "grad_norm": 0.7540216445922852,
      "learning_rate": 4.136008918617614e-06,
      "loss": 0.8551,
      "step": 20570
    },
    {
      "epoch": 2.752901046717721,
      "grad_norm": 0.7068281769752502,
      "learning_rate": 4.11371237458194e-06,
      "loss": 0.8631,
      "step": 20580
    },
    {
      "epoch": 2.7542387051466406,
      "grad_norm": 0.656069815158844,
      "learning_rate": 4.0914158305462655e-06,
      "loss": 0.8274,
      "step": 20590
    },
    {
      "epoch": 2.7555763635755612,
      "grad_norm": 0.6739773750305176,
      "learning_rate": 4.069119286510591e-06,
      "loss": 0.8133,
      "step": 20600
    },
    {
      "epoch": 2.756914022004481,
      "grad_norm": 0.6657664179801941,
      "learning_rate": 4.046822742474917e-06,
      "loss": 0.8225,
      "step": 20610
    },
    {
      "epoch": 2.758251680433401,
      "grad_norm": 0.660586416721344,
      "learning_rate": 4.024526198439242e-06,
      "loss": 0.8193,
      "step": 20620
    },
    {
      "epoch": 2.7595893388623214,
      "grad_norm": 0.7553042769432068,
      "learning_rate": 4.002229654403568e-06,
      "loss": 0.8319,
      "step": 20630
    },
    {
      "epoch": 2.7609269972912416,
      "grad_norm": 0.6664790511131287,
      "learning_rate": 3.9799331103678925e-06,
      "loss": 0.8312,
      "step": 20640
    },
    {
      "epoch": 2.7622646557201618,
      "grad_norm": 0.7064679265022278,
      "learning_rate": 3.957636566332219e-06,
      "loss": 0.8266,
      "step": 20650
    },
    {
      "epoch": 2.763602314149082,
      "grad_norm": 0.6884164810180664,
      "learning_rate": 3.9353400222965445e-06,
      "loss": 0.8401,
      "step": 20660
    },
    {
      "epoch": 2.764939972578002,
      "grad_norm": 0.7102681398391724,
      "learning_rate": 3.91304347826087e-06,
      "loss": 0.8616,
      "step": 20670
    },
    {
      "epoch": 2.7662776310069224,
      "grad_norm": 0.6515930891036987,
      "learning_rate": 3.890746934225195e-06,
      "loss": 0.8396,
      "step": 20680
    },
    {
      "epoch": 2.7676152894358426,
      "grad_norm": 0.671553373336792,
      "learning_rate": 3.868450390189521e-06,
      "loss": 0.8255,
      "step": 20690
    },
    {
      "epoch": 2.7689529478647628,
      "grad_norm": 0.6683688759803772,
      "learning_rate": 3.846153846153847e-06,
      "loss": 0.8217,
      "step": 20700
    },
    {
      "epoch": 2.770290606293683,
      "grad_norm": 0.6945423483848572,
      "learning_rate": 3.8238573021181715e-06,
      "loss": 0.832,
      "step": 20710
    },
    {
      "epoch": 2.771628264722603,
      "grad_norm": 0.6516674757003784,
      "learning_rate": 3.8015607580824975e-06,
      "loss": 0.8354,
      "step": 20720
    },
    {
      "epoch": 2.7729659231515233,
      "grad_norm": 0.6623625159263611,
      "learning_rate": 3.779264214046823e-06,
      "loss": 0.8503,
      "step": 20730
    },
    {
      "epoch": 2.7743035815804435,
      "grad_norm": 0.6830084919929504,
      "learning_rate": 3.7569676700111487e-06,
      "loss": 0.8452,
      "step": 20740
    },
    {
      "epoch": 2.7756412400093637,
      "grad_norm": 0.616486668586731,
      "learning_rate": 3.734671125975474e-06,
      "loss": 0.8079,
      "step": 20750
    },
    {
      "epoch": 2.776978898438284,
      "grad_norm": 0.6851694583892822,
      "learning_rate": 3.7123745819397994e-06,
      "loss": 0.8289,
      "step": 20760
    },
    {
      "epoch": 2.778316556867204,
      "grad_norm": 0.7349138855934143,
      "learning_rate": 3.6900780379041254e-06,
      "loss": 0.8395,
      "step": 20770
    },
    {
      "epoch": 2.779654215296124,
      "grad_norm": 0.6993539333343506,
      "learning_rate": 3.66778149386845e-06,
      "loss": 0.8408,
      "step": 20780
    },
    {
      "epoch": 2.7809918737250445,
      "grad_norm": 0.6920658946037292,
      "learning_rate": 3.645484949832776e-06,
      "loss": 0.8371,
      "step": 20790
    },
    {
      "epoch": 2.7823295321539643,
      "grad_norm": 0.7608557939529419,
      "learning_rate": 3.6231884057971017e-06,
      "loss": 0.8326,
      "step": 20800
    },
    {
      "epoch": 2.783667190582885,
      "grad_norm": 0.6885130405426025,
      "learning_rate": 3.6008918617614273e-06,
      "loss": 0.835,
      "step": 20810
    },
    {
      "epoch": 2.7850048490118047,
      "grad_norm": 0.6668437123298645,
      "learning_rate": 3.5785953177257524e-06,
      "loss": 0.8209,
      "step": 20820
    },
    {
      "epoch": 2.786342507440725,
      "grad_norm": 0.6765488982200623,
      "learning_rate": 3.5562987736900784e-06,
      "loss": 0.8522,
      "step": 20830
    },
    {
      "epoch": 2.787680165869645,
      "grad_norm": 0.6533100008964539,
      "learning_rate": 3.534002229654404e-06,
      "loss": 0.8628,
      "step": 20840
    },
    {
      "epoch": 2.7890178242985653,
      "grad_norm": 0.7027162909507751,
      "learning_rate": 3.511705685618729e-06,
      "loss": 0.8172,
      "step": 20850
    },
    {
      "epoch": 2.7903554827274855,
      "grad_norm": 0.7411330938339233,
      "learning_rate": 3.4894091415830547e-06,
      "loss": 0.8302,
      "step": 20860
    },
    {
      "epoch": 2.7916931411564057,
      "grad_norm": 0.6505343914031982,
      "learning_rate": 3.4671125975473803e-06,
      "loss": 0.8163,
      "step": 20870
    },
    {
      "epoch": 2.793030799585326,
      "grad_norm": 0.7035006284713745,
      "learning_rate": 3.4448160535117063e-06,
      "loss": 0.8374,
      "step": 20880
    },
    {
      "epoch": 2.794368458014246,
      "grad_norm": 0.7221987247467041,
      "learning_rate": 3.422519509476031e-06,
      "loss": 0.8537,
      "step": 20890
    },
    {
      "epoch": 2.7957061164431662,
      "grad_norm": 0.7408884167671204,
      "learning_rate": 3.400222965440357e-06,
      "loss": 0.8508,
      "step": 20900
    },
    {
      "epoch": 2.7970437748720864,
      "grad_norm": 0.66534024477005,
      "learning_rate": 3.3779264214046826e-06,
      "loss": 0.8496,
      "step": 20910
    },
    {
      "epoch": 2.7983814333010066,
      "grad_norm": 0.7154926061630249,
      "learning_rate": 3.3556298773690077e-06,
      "loss": 0.8683,
      "step": 20920
    },
    {
      "epoch": 2.799719091729927,
      "grad_norm": 0.7060149312019348,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.8365,
      "step": 20930
    },
    {
      "epoch": 2.801056750158847,
      "grad_norm": 0.6909601092338562,
      "learning_rate": 3.3110367892976593e-06,
      "loss": 0.8365,
      "step": 20940
    },
    {
      "epoch": 2.8023944085877672,
      "grad_norm": 0.6737433075904846,
      "learning_rate": 3.288740245261984e-06,
      "loss": 0.8361,
      "step": 20950
    },
    {
      "epoch": 2.8037320670166874,
      "grad_norm": 0.707091212272644,
      "learning_rate": 3.26644370122631e-06,
      "loss": 0.8405,
      "step": 20960
    },
    {
      "epoch": 2.8050697254456076,
      "grad_norm": 0.692913293838501,
      "learning_rate": 3.2441471571906356e-06,
      "loss": 0.8143,
      "step": 20970
    },
    {
      "epoch": 2.806407383874528,
      "grad_norm": 0.6820939183235168,
      "learning_rate": 3.221850613154961e-06,
      "loss": 0.8359,
      "step": 20980
    },
    {
      "epoch": 2.8077450423034476,
      "grad_norm": 0.6809031367301941,
      "learning_rate": 3.1995540691192863e-06,
      "loss": 0.845,
      "step": 20990
    },
    {
      "epoch": 2.809082700732368,
      "grad_norm": 0.6661695837974548,
      "learning_rate": 3.1772575250836123e-06,
      "loss": 0.8414,
      "step": 21000
    },
    {
      "epoch": 2.810420359161288,
      "grad_norm": 0.7460951209068298,
      "learning_rate": 3.154960981047938e-06,
      "loss": 0.8407,
      "step": 21010
    },
    {
      "epoch": 2.8117580175902086,
      "grad_norm": 0.6586597561836243,
      "learning_rate": 3.132664437012263e-06,
      "loss": 0.8402,
      "step": 21020
    },
    {
      "epoch": 2.8130956760191284,
      "grad_norm": 0.9338603615760803,
      "learning_rate": 3.1103678929765886e-06,
      "loss": 0.8452,
      "step": 21030
    },
    {
      "epoch": 2.8144333344480486,
      "grad_norm": 0.674795389175415,
      "learning_rate": 3.0880713489409142e-06,
      "loss": 0.8346,
      "step": 21040
    },
    {
      "epoch": 2.8157709928769687,
      "grad_norm": 0.6684666275978088,
      "learning_rate": 3.06577480490524e-06,
      "loss": 0.8236,
      "step": 21050
    },
    {
      "epoch": 2.817108651305889,
      "grad_norm": 0.64073246717453,
      "learning_rate": 3.0434782608695654e-06,
      "loss": 0.8287,
      "step": 21060
    },
    {
      "epoch": 2.818446309734809,
      "grad_norm": 0.726971447467804,
      "learning_rate": 3.021181716833891e-06,
      "loss": 0.8079,
      "step": 21070
    },
    {
      "epoch": 2.8197839681637293,
      "grad_norm": 0.6940332651138306,
      "learning_rate": 2.998885172798216e-06,
      "loss": 0.8188,
      "step": 21080
    },
    {
      "epoch": 2.8211216265926495,
      "grad_norm": 0.6835690140724182,
      "learning_rate": 2.976588628762542e-06,
      "loss": 0.8077,
      "step": 21090
    },
    {
      "epoch": 2.8224592850215697,
      "grad_norm": 0.6592556238174438,
      "learning_rate": 2.9542920847268673e-06,
      "loss": 0.8428,
      "step": 21100
    },
    {
      "epoch": 2.82379694345049,
      "grad_norm": 0.7217952013015747,
      "learning_rate": 2.9319955406911932e-06,
      "loss": 0.842,
      "step": 21110
    },
    {
      "epoch": 2.82513460187941,
      "grad_norm": 0.7724506855010986,
      "learning_rate": 2.9096989966555184e-06,
      "loss": 0.85,
      "step": 21120
    },
    {
      "epoch": 2.8264722603083303,
      "grad_norm": 0.6738249659538269,
      "learning_rate": 2.8874024526198444e-06,
      "loss": 0.8185,
      "step": 21130
    },
    {
      "epoch": 2.8278099187372505,
      "grad_norm": 0.7342796921730042,
      "learning_rate": 2.8651059085841696e-06,
      "loss": 0.8212,
      "step": 21140
    },
    {
      "epoch": 2.8291475771661707,
      "grad_norm": 0.6816047430038452,
      "learning_rate": 2.842809364548495e-06,
      "loss": 0.8076,
      "step": 21150
    },
    {
      "epoch": 2.830485235595091,
      "grad_norm": 0.652374267578125,
      "learning_rate": 2.8205128205128207e-06,
      "loss": 0.8569,
      "step": 21160
    },
    {
      "epoch": 2.831822894024011,
      "grad_norm": 0.6592250466346741,
      "learning_rate": 2.7982162764771463e-06,
      "loss": 0.8318,
      "step": 21170
    },
    {
      "epoch": 2.8331605524529313,
      "grad_norm": 0.6319701075553894,
      "learning_rate": 2.775919732441472e-06,
      "loss": 0.8646,
      "step": 21180
    },
    {
      "epoch": 2.8344982108818515,
      "grad_norm": 0.7000303864479065,
      "learning_rate": 2.753623188405797e-06,
      "loss": 0.8327,
      "step": 21190
    },
    {
      "epoch": 2.8358358693107713,
      "grad_norm": 0.6484702825546265,
      "learning_rate": 2.731326644370123e-06,
      "loss": 0.8269,
      "step": 21200
    },
    {
      "epoch": 2.837173527739692,
      "grad_norm": 0.6646072864532471,
      "learning_rate": 2.709030100334448e-06,
      "loss": 0.8641,
      "step": 21210
    },
    {
      "epoch": 2.8385111861686116,
      "grad_norm": 0.7081214785575867,
      "learning_rate": 2.6867335562987737e-06,
      "loss": 0.84,
      "step": 21220
    },
    {
      "epoch": 2.8398488445975323,
      "grad_norm": 0.7101341485977173,
      "learning_rate": 2.6644370122630993e-06,
      "loss": 0.8178,
      "step": 21230
    },
    {
      "epoch": 2.841186503026452,
      "grad_norm": 0.7739062905311584,
      "learning_rate": 2.642140468227425e-06,
      "loss": 0.8472,
      "step": 21240
    },
    {
      "epoch": 2.8425241614553722,
      "grad_norm": 0.691157877445221,
      "learning_rate": 2.6198439241917505e-06,
      "loss": 0.8562,
      "step": 21250
    },
    {
      "epoch": 2.8438618198842924,
      "grad_norm": 0.6718946695327759,
      "learning_rate": 2.597547380156076e-06,
      "loss": 0.8603,
      "step": 21260
    },
    {
      "epoch": 2.8451994783132126,
      "grad_norm": 0.646195650100708,
      "learning_rate": 2.5752508361204016e-06,
      "loss": 0.8401,
      "step": 21270
    },
    {
      "epoch": 2.846537136742133,
      "grad_norm": 0.7124553322792053,
      "learning_rate": 2.552954292084727e-06,
      "loss": 0.8163,
      "step": 21280
    },
    {
      "epoch": 2.847874795171053,
      "grad_norm": 0.6526380181312561,
      "learning_rate": 2.5306577480490523e-06,
      "loss": 0.8352,
      "step": 21290
    },
    {
      "epoch": 2.849212453599973,
      "grad_norm": 0.6655855774879456,
      "learning_rate": 2.508361204013378e-06,
      "loss": 0.8392,
      "step": 21300
    },
    {
      "epoch": 2.8505501120288934,
      "grad_norm": 0.8588141798973083,
      "learning_rate": 2.4860646599777035e-06,
      "loss": 0.8627,
      "step": 21310
    },
    {
      "epoch": 2.8518877704578136,
      "grad_norm": 0.7504366636276245,
      "learning_rate": 2.463768115942029e-06,
      "loss": 0.8285,
      "step": 21320
    },
    {
      "epoch": 2.853225428886734,
      "grad_norm": 0.667129397392273,
      "learning_rate": 2.4414715719063546e-06,
      "loss": 0.8511,
      "step": 21330
    },
    {
      "epoch": 2.854563087315654,
      "grad_norm": 0.7353605031967163,
      "learning_rate": 2.41917502787068e-06,
      "loss": 0.8098,
      "step": 21340
    },
    {
      "epoch": 2.855900745744574,
      "grad_norm": 2.8549654483795166,
      "learning_rate": 2.3968784838350058e-06,
      "loss": 0.8403,
      "step": 21350
    },
    {
      "epoch": 2.8572384041734944,
      "grad_norm": 0.6845992803573608,
      "learning_rate": 2.374581939799331e-06,
      "loss": 0.8425,
      "step": 21360
    },
    {
      "epoch": 2.8585760626024146,
      "grad_norm": 0.6904535293579102,
      "learning_rate": 2.352285395763657e-06,
      "loss": 0.8239,
      "step": 21370
    },
    {
      "epoch": 2.859913721031335,
      "grad_norm": 0.7063260674476624,
      "learning_rate": 2.329988851727982e-06,
      "loss": 0.8495,
      "step": 21380
    },
    {
      "epoch": 2.861251379460255,
      "grad_norm": 0.715046226978302,
      "learning_rate": 2.307692307692308e-06,
      "loss": 0.8511,
      "step": 21390
    },
    {
      "epoch": 2.862589037889175,
      "grad_norm": 0.7136759757995605,
      "learning_rate": 2.2853957636566332e-06,
      "loss": 0.8345,
      "step": 21400
    },
    {
      "epoch": 2.863926696318095,
      "grad_norm": 0.6676014065742493,
      "learning_rate": 2.263099219620959e-06,
      "loss": 0.8421,
      "step": 21410
    },
    {
      "epoch": 2.8652643547470156,
      "grad_norm": 0.689643919467926,
      "learning_rate": 2.2408026755852844e-06,
      "loss": 0.823,
      "step": 21420
    },
    {
      "epoch": 2.8666020131759353,
      "grad_norm": 0.6493193507194519,
      "learning_rate": 2.21850613154961e-06,
      "loss": 0.8502,
      "step": 21430
    },
    {
      "epoch": 2.867939671604856,
      "grad_norm": 0.624612033367157,
      "learning_rate": 2.1962095875139355e-06,
      "loss": 0.8283,
      "step": 21440
    },
    {
      "epoch": 2.8692773300337757,
      "grad_norm": 0.6793367862701416,
      "learning_rate": 2.173913043478261e-06,
      "loss": 0.8772,
      "step": 21450
    },
    {
      "epoch": 2.870614988462696,
      "grad_norm": 0.7093678712844849,
      "learning_rate": 2.1516164994425867e-06,
      "loss": 0.8029,
      "step": 21460
    },
    {
      "epoch": 2.871952646891616,
      "grad_norm": 0.6804120540618896,
      "learning_rate": 2.129319955406912e-06,
      "loss": 0.8284,
      "step": 21470
    },
    {
      "epoch": 2.8732903053205363,
      "grad_norm": 0.6667081117630005,
      "learning_rate": 2.1070234113712374e-06,
      "loss": 0.791,
      "step": 21480
    },
    {
      "epoch": 2.8746279637494565,
      "grad_norm": 0.6766059398651123,
      "learning_rate": 2.084726867335563e-06,
      "loss": 0.8358,
      "step": 21490
    },
    {
      "epoch": 2.8759656221783767,
      "grad_norm": 0.7095627784729004,
      "learning_rate": 2.0624303232998886e-06,
      "loss": 0.8251,
      "step": 21500
    },
    {
      "epoch": 2.877303280607297,
      "grad_norm": 0.8769101500511169,
      "learning_rate": 2.040133779264214e-06,
      "loss": 0.8287,
      "step": 21510
    },
    {
      "epoch": 2.878640939036217,
      "grad_norm": 0.6979470252990723,
      "learning_rate": 2.0178372352285397e-06,
      "loss": 0.8401,
      "step": 21520
    },
    {
      "epoch": 2.8799785974651373,
      "grad_norm": 0.7051964402198792,
      "learning_rate": 1.9955406911928653e-06,
      "loss": 0.8587,
      "step": 21530
    },
    {
      "epoch": 2.8813162558940575,
      "grad_norm": 0.6546303033828735,
      "learning_rate": 1.973244147157191e-06,
      "loss": 0.834,
      "step": 21540
    },
    {
      "epoch": 2.8826539143229777,
      "grad_norm": 0.7317185997962952,
      "learning_rate": 1.950947603121516e-06,
      "loss": 0.8483,
      "step": 21550
    },
    {
      "epoch": 2.883991572751898,
      "grad_norm": 0.6588382720947266,
      "learning_rate": 1.928651059085842e-06,
      "loss": 0.8,
      "step": 21560
    },
    {
      "epoch": 2.885329231180818,
      "grad_norm": 0.6493967175483704,
      "learning_rate": 1.9063545150501672e-06,
      "loss": 0.8318,
      "step": 21570
    },
    {
      "epoch": 2.8866668896097383,
      "grad_norm": 0.6270602941513062,
      "learning_rate": 1.884057971014493e-06,
      "loss": 0.828,
      "step": 21580
    },
    {
      "epoch": 2.8880045480386585,
      "grad_norm": 0.709042489528656,
      "learning_rate": 1.8617614269788183e-06,
      "loss": 0.8308,
      "step": 21590
    },
    {
      "epoch": 2.8893422064675787,
      "grad_norm": 0.6622918248176575,
      "learning_rate": 1.839464882943144e-06,
      "loss": 0.8557,
      "step": 21600
    },
    {
      "epoch": 2.890679864896499,
      "grad_norm": 0.7159324884414673,
      "learning_rate": 1.8171683389074695e-06,
      "loss": 0.8106,
      "step": 21610
    },
    {
      "epoch": 2.8920175233254186,
      "grad_norm": 0.6485892534255981,
      "learning_rate": 1.7948717948717948e-06,
      "loss": 0.82,
      "step": 21620
    },
    {
      "epoch": 2.8933551817543393,
      "grad_norm": 0.6524958610534668,
      "learning_rate": 1.7725752508361206e-06,
      "loss": 0.8138,
      "step": 21630
    },
    {
      "epoch": 2.894692840183259,
      "grad_norm": 0.6912874579429626,
      "learning_rate": 1.750278706800446e-06,
      "loss": 0.8189,
      "step": 21640
    },
    {
      "epoch": 2.8960304986121796,
      "grad_norm": 0.7001848220825195,
      "learning_rate": 1.7279821627647718e-06,
      "loss": 0.8526,
      "step": 21650
    },
    {
      "epoch": 2.8973681570410994,
      "grad_norm": 0.6888259649276733,
      "learning_rate": 1.7056856187290971e-06,
      "loss": 0.8621,
      "step": 21660
    },
    {
      "epoch": 2.8987058154700196,
      "grad_norm": 0.7374334335327148,
      "learning_rate": 1.6833890746934225e-06,
      "loss": 0.8372,
      "step": 21670
    },
    {
      "epoch": 2.90004347389894,
      "grad_norm": 0.6273810267448425,
      "learning_rate": 1.661092530657748e-06,
      "loss": 0.8182,
      "step": 21680
    },
    {
      "epoch": 2.90138113232786,
      "grad_norm": 0.6709342002868652,
      "learning_rate": 1.6387959866220736e-06,
      "loss": 0.8374,
      "step": 21690
    },
    {
      "epoch": 2.90271879075678,
      "grad_norm": 0.6785558462142944,
      "learning_rate": 1.6164994425863992e-06,
      "loss": 0.8421,
      "step": 21700
    },
    {
      "epoch": 2.9040564491857004,
      "grad_norm": 0.9842515587806702,
      "learning_rate": 1.5942028985507246e-06,
      "loss": 0.8419,
      "step": 21710
    },
    {
      "epoch": 2.9053941076146206,
      "grad_norm": 0.6850267648696899,
      "learning_rate": 1.5719063545150504e-06,
      "loss": 0.8415,
      "step": 21720
    },
    {
      "epoch": 2.9067317660435408,
      "grad_norm": 0.6710712313652039,
      "learning_rate": 1.5496098104793757e-06,
      "loss": 0.8349,
      "step": 21730
    },
    {
      "epoch": 2.908069424472461,
      "grad_norm": 0.6434586048126221,
      "learning_rate": 1.5273132664437013e-06,
      "loss": 0.8582,
      "step": 21740
    },
    {
      "epoch": 2.909407082901381,
      "grad_norm": 0.7429812550544739,
      "learning_rate": 1.5050167224080269e-06,
      "loss": 0.8402,
      "step": 21750
    },
    {
      "epoch": 2.9107447413303014,
      "grad_norm": 0.6942541003227234,
      "learning_rate": 1.4827201783723524e-06,
      "loss": 0.8341,
      "step": 21760
    },
    {
      "epoch": 2.9120823997592216,
      "grad_norm": 0.707480251789093,
      "learning_rate": 1.4604236343366778e-06,
      "loss": 0.8219,
      "step": 21770
    },
    {
      "epoch": 2.9134200581881418,
      "grad_norm": 0.7347409725189209,
      "learning_rate": 1.4381270903010034e-06,
      "loss": 0.8603,
      "step": 21780
    },
    {
      "epoch": 2.914757716617062,
      "grad_norm": 0.6996123194694519,
      "learning_rate": 1.415830546265329e-06,
      "loss": 0.8298,
      "step": 21790
    },
    {
      "epoch": 2.916095375045982,
      "grad_norm": 0.7169934511184692,
      "learning_rate": 1.3935340022296545e-06,
      "loss": 0.8386,
      "step": 21800
    },
    {
      "epoch": 2.9174330334749023,
      "grad_norm": 0.7158328294754028,
      "learning_rate": 1.3712374581939801e-06,
      "loss": 0.8286,
      "step": 21810
    },
    {
      "epoch": 2.9187706919038225,
      "grad_norm": 0.799015998840332,
      "learning_rate": 1.3489409141583057e-06,
      "loss": 0.8263,
      "step": 21820
    },
    {
      "epoch": 2.9201083503327423,
      "grad_norm": 0.6795902848243713,
      "learning_rate": 1.326644370122631e-06,
      "loss": 0.819,
      "step": 21830
    },
    {
      "epoch": 2.921446008761663,
      "grad_norm": 0.6653278470039368,
      "learning_rate": 1.3043478260869564e-06,
      "loss": 0.8473,
      "step": 21840
    },
    {
      "epoch": 2.9227836671905827,
      "grad_norm": 0.6744517087936401,
      "learning_rate": 1.282051282051282e-06,
      "loss": 0.8216,
      "step": 21850
    },
    {
      "epoch": 2.9241213256195033,
      "grad_norm": 0.7228891253471375,
      "learning_rate": 1.2597547380156076e-06,
      "loss": 0.8386,
      "step": 21860
    },
    {
      "epoch": 2.925458984048423,
      "grad_norm": 0.7047227621078491,
      "learning_rate": 1.2374581939799331e-06,
      "loss": 0.825,
      "step": 21870
    },
    {
      "epoch": 2.9267966424773433,
      "grad_norm": 0.7098694443702698,
      "learning_rate": 1.2151616499442587e-06,
      "loss": 0.8566,
      "step": 21880
    },
    {
      "epoch": 2.9281343009062635,
      "grad_norm": 0.6937557458877563,
      "learning_rate": 1.1928651059085843e-06,
      "loss": 0.8059,
      "step": 21890
    },
    {
      "epoch": 2.9294719593351837,
      "grad_norm": 0.6663541197776794,
      "learning_rate": 1.1705685618729096e-06,
      "loss": 0.8502,
      "step": 21900
    },
    {
      "epoch": 2.930809617764104,
      "grad_norm": 0.6734803318977356,
      "learning_rate": 1.1482720178372352e-06,
      "loss": 0.8287,
      "step": 21910
    },
    {
      "epoch": 2.932147276193024,
      "grad_norm": 0.6627956628799438,
      "learning_rate": 1.1259754738015608e-06,
      "loss": 0.827,
      "step": 21920
    },
    {
      "epoch": 2.9334849346219443,
      "grad_norm": 0.7070600390434265,
      "learning_rate": 1.1036789297658864e-06,
      "loss": 0.8434,
      "step": 21930
    },
    {
      "epoch": 2.9348225930508645,
      "grad_norm": 0.6914901733398438,
      "learning_rate": 1.081382385730212e-06,
      "loss": 0.8147,
      "step": 21940
    },
    {
      "epoch": 2.9361602514797847,
      "grad_norm": 0.725729763507843,
      "learning_rate": 1.0590858416945375e-06,
      "loss": 0.8395,
      "step": 21950
    },
    {
      "epoch": 2.937497909908705,
      "grad_norm": 0.7228373289108276,
      "learning_rate": 1.036789297658863e-06,
      "loss": 0.8611,
      "step": 21960
    },
    {
      "epoch": 2.938835568337625,
      "grad_norm": 0.6943525075912476,
      "learning_rate": 1.0144927536231885e-06,
      "loss": 0.8384,
      "step": 21970
    },
    {
      "epoch": 2.9401732267665452,
      "grad_norm": 0.7707187533378601,
      "learning_rate": 9.92196209587514e-07,
      "loss": 0.8532,
      "step": 21980
    },
    {
      "epoch": 2.9415108851954654,
      "grad_norm": 0.658035933971405,
      "learning_rate": 9.698996655518394e-07,
      "loss": 0.8278,
      "step": 21990
    },
    {
      "epoch": 2.9428485436243856,
      "grad_norm": 0.7586178183555603,
      "learning_rate": 9.476031215161651e-07,
      "loss": 0.8306,
      "step": 22000
    }
  ],
  "logging_steps": 10,
  "max_steps": 22425,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2993698816e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
